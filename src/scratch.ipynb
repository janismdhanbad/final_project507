{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/papers_arxiv_v1.json\", 'r') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'abs/2110.08861v2': {'id': None,\n",
       "  'title': '3D-RETR: End-to-End Single and Multi-View 3D Reconstruction with\\n  Transformers',\n",
       "  'authors': ['Zai Shi',\n",
       "   'Zhao Meng',\n",
       "   'Yiran Xing',\n",
       "   'Yunpu Ma',\n",
       "   'Roger Wattenhofer'],\n",
       "  'abstract': '3D reconstruction aims to reconstruct 3D objects from 2D views. Previous works for 3D reconstruction mainly focus on feature matching between views or using CNNs as backbones. Recently, Transformers have been shown effective in multiple applications of computer vision. However, whether or not Transformers can be used for 3D reconstruction is still unclear. In this paper, we fill this gap by proposing 3D-RETR, which is able to perform end-to-end 3D REconstruction with TRansformers. 3D-RETR first uses a pretrained Transformer to extract visual features from 2D input images. 3D-RETR then uses another Transformer Decoder to obtain the voxel features. A CNN Decoder then takes as input the voxel features to obtain the reconstructed objects. 3D-RETR is capable of 3D reconstruction from a single view or multiple views. Experimental results on two datasets show that 3DRETR reaches state-of-the-art performance on 3D reconstruction. Additional ablation study also demonstrates that 3D-DETR benefits from using Transformers.',\n",
       "  'arxiv_id': 'abs/2110.08861v2',\n",
       "  'category': None},\n",
       " 'abs/1912.00439v3': {'id': None,\n",
       "  'title': 'DeepC-MVS: Deep Confidence Prediction for Multi-View Stereo\\n  Reconstruction',\n",
       "  'authors': ['Andreas Kuhn',\n",
       "   'Christian Sormann',\n",
       "   'Mattia Rossi',\n",
       "   'Oliver Erdler',\n",
       "   'Friedrich Fraundorfer'],\n",
       "  'abstract': 'Deep Neural Networks (DNNs) have the potential to improve the quality of image-based 3D reconstructions. However, the use of DNNs in the context of 3D reconstruction from large and high-resolution image datasets is still an open challenge, due to memory and computational constraints. We propose a pipeline which takes advantage of DNNs to improve the quality of 3D reconstructions while being able to handle large and high-resolution datasets. In particular, we propose a confidence prediction network explicitly tailored for Multi-View Stereo (MVS) and we use it for both depth map outlier filtering and depth map refinement within our pipeline, in order to improve the quality of the final 3D reconstructions. We train our confidence prediction network on (semi-)dense ground truth depth maps from publicly available real world MVS datasets. With extensive experiments on popular benchmarks, we show that our overall pipeline can produce state-of-the-art 3D reconstructions, both qualitatively and quantitatively.',\n",
       "  'arxiv_id': 'abs/1912.00439v3',\n",
       "  'category': None},\n",
       " 'abs/2003.12618v2': {'id': None,\n",
       "  'title': 'Image compression optimized for 3D reconstruction by utilizing deep\\n  neural networks',\n",
       "  'authors': ['Alex Golts', 'Yoav Y. Schechner'],\n",
       "  'abstract': 'Computer vision tasks are often expected to be executed on compressed images. Classical image compression standards like JPEG 2000 are widely used. However, they do not account for the specific end-task at hand. Motivated by works on recurrent neural network (RNN)-based image compression and three-dimensional (3D) reconstruction, we propose unified network architectures to solve both tasks jointly. These joint models provide image compression tailored for the specific task of 3D reconstruction. Images compressed by our proposed models, yield 3D reconstruction performance superior as compared to using JPEG 2000 compression. Our models significantly extend the range of compression rates for which 3D reconstruction is possible. We also show that this can be done highly efficiently at almost no additional cost to obtain compression on top of the computation already required for performing the 3D reconstruction task.',\n",
       "  'arxiv_id': 'abs/2003.12618v2',\n",
       "  'category': None},\n",
       " 'abs/2112.00557v1': {'id': None,\n",
       "  'title': '3D Reconstruction Using a Linear Laser Scanner and a Camera',\n",
       "  'authors': ['Rui Wang'],\n",
       "  'abstract': 'With the rapid development of computer graphics and vision, several three-dimensional (3D) reconstruction techniques have been proposed and used to obtain the 3D representation of objects in the form of point cloud models, mesh models, and geometric models. The cost of 3D reconstruction is declining due to the maturing of this technology, however, the inexpensive 3D reconstruction scanners on the market may not be able to generate a clear point cloud model as expected. This study systematically reviews some basic types of 3D reconstruction technology and introduces an easy implementation using a linear laser scanner, a camera, and a turntable. The implementation is based on the monovision with laser and has tested several objects like wiki and mug. The accuracy and resolution of the point cloud result are quite satisfying. It turns out everyone can build such a 3D reconstruction system with appropriate procedures.',\n",
       "  'arxiv_id': 'abs/2112.00557v1',\n",
       "  'category': None},\n",
       " 'abs/1201.3172v2': {'id': None,\n",
       "  'title': 'Assessing the Value of 3D Reconstruction in Building Construction',\n",
       "  'authors': ['Uma Murthy', 'David Boardman', 'Chirag Garg'],\n",
       "  'abstract': '3-dimensional (3D) reconstruction is an emerging field in image processing and computer vision that aims to create 3D visualizations/ models of objects/ scenes from image sets. However, its commercial applications and benefits are yet to be fully explored. In this paper, we describe ongoing work towards assessing the value of 3D reconstruction in the building construction domain. We present preliminary results from a user study, where our objective is to understand the use of visual information in building construction in order to determine problems with the use of visual information and identify potential benefits and scenarios for the use of 3D reconstruction.',\n",
       "  'arxiv_id': 'abs/1201.3172v2',\n",
       "  'category': None},\n",
       " 'abs/2009.00887v2': {'id': None,\n",
       "  'title': 'Inspection of histological 3D reconstructions in virtual reality',\n",
       "  'authors': ['Oleg Lobachev',\n",
       "   'Moritz Berthold',\n",
       "   'Henriette Pfeffer',\n",
       "   'Michael Guthe',\n",
       "   'Birte S. Steiniger'],\n",
       "  'abstract': '3D reconstruction is a challenging current topic in medical research. We perform 3D reconstructions from serial sections stained by immunohistological methods. This paper presents an immersive visualisation solution to quality control (QC), inspect, and analyse such reconstructions. QC is essential to establish correct digital processing methodologies. Visual analytics, such as annotation placement, mesh painting, and classification utility, facilitates medical research insights. We propose a visualisation in virtual reality (VR) for these purposes. In this manner, we advance the microanatomical research of human bone marrow and spleen. Both 3D reconstructions and original data are available in VR. Data inspection is streamlined by subtle implementation details and general immersion in VR.',\n",
       "  'arxiv_id': 'abs/2009.00887v2',\n",
       "  'category': None},\n",
       " 'abs/2106.15328v2': {'id': None,\n",
       "  'title': 'Deep Learning for Multi-View Stereo via Plane Sweep: A Survey',\n",
       "  'authors': ['Qingtian Zhu',\n",
       "   'Chen Min',\n",
       "   'Zizhuang Wei',\n",
       "   'Yisong Chen',\n",
       "   'Guoping Wang'],\n",
       "  'abstract': '3D reconstruction has lately attracted increasing attention due to its wide application in many areas, such as autonomous driving, robotics and virtual reality. As a dominant technique in artificial intelligence, deep learning has been successfully adopted to solve various computer vision problems. However, deep learning for 3D reconstruction is still at its infancy due to its unique challenges and varying pipelines. To stimulate future research, this paper presents a review of recent progress in deep learning methods for Multi-view Stereo (MVS), which is considered as a crucial task of image-based 3D reconstruction. It also presents comparative results on several publicly available datasets, with insightful observations and inspiring future research directions.',\n",
       "  'arxiv_id': 'abs/2106.15328v2',\n",
       "  'category': None},\n",
       " 'abs/1506.06876v1': {'id': None,\n",
       "  'title': 'Autonomous 3D Reconstruction Using a MAV',\n",
       "  'authors': ['Alexander Popov',\n",
       "   'Dimitrios Zermas',\n",
       "   'Nikolaos Papanikolopoulos'],\n",
       "  'abstract': 'An approach is proposed for high resolution 3D reconstruction of an object using a Micro Air Vehicle (MAV). A system is described which autonomously captures images and performs a dense 3D reconstruction via structure from motion with no prior knowledge of the environment. Only the MAVs own sensors, the front facing camera and the Inertial Measurement Unit (IMU) are utilized. Precision agriculture is considered as an example application for the system.',\n",
       "  'arxiv_id': 'abs/1506.06876v1',\n",
       "  'category': None},\n",
       " 'abs/1708.04672v1': {'id': None,\n",
       "  'title': 'DeformNet: Free-Form Deformation Network for 3D Shape Reconstruction\\n  from a Single Image',\n",
       "  'authors': ['Andrey Kurenkov',\n",
       "   'Jingwei Ji',\n",
       "   'Animesh Garg',\n",
       "   'Viraj Mehta',\n",
       "   'JunYoung Gwak',\n",
       "   'Christopher Choy',\n",
       "   'Silvio Savarese'],\n",
       "  'abstract': '3D reconstruction from a single image is a key problem in multiple applications ranging from robotic manipulation to augmented reality. Prior methods have tackled this problem through generative models which predict 3D reconstructions as voxels or point clouds. However, these methods can be computationally expensive and miss fine details. We introduce a new differentiable layer for 3D data deformation and use it in DeformNet to learn a model for 3D reconstruction-through-deformation. DeformNet takes an image input, searches the nearest shape template from a database, and deforms the template to match the query image. We evaluate our approach on the ShapeNet dataset and show that - (a) the Free-Form Deformation layer is a powerful new building block for Deep Learning models that manipulate 3D data (b) DeformNet uses this FFD layer combined with shape retrieval for smooth and detail-preserving 3D reconstruction of qualitatively plausible point clouds with respect to a single query image (c) compared to other state-of-the-art 3D reconstruction methods, DeformNet quantitatively matches or outperforms their benchmarks by significant margins. For more information, visit: https://deformnet-site.github.io/DeformNet-website/ .',\n",
       "  'arxiv_id': 'abs/1708.04672v1',\n",
       "  'category': None},\n",
       " 'abs/1705.10904v2': {'id': None,\n",
       "  'title': 'Weakly supervised 3D Reconstruction with Adversarial Constraint',\n",
       "  'authors': ['JunYoung Gwak',\n",
       "   'Christopher B. Choy',\n",
       "   'Animesh Garg',\n",
       "   'Manmohan Chandraker',\n",
       "   'Silvio Savarese'],\n",
       "  'abstract': 'Supervised 3D reconstruction has witnessed a significant progress through the use of deep neural networks. However, this increase in performance requires large scale annotations of 2D/3D data. In this paper, we explore inexpensive 2D supervision as an alternative for expensive 3D CAD annotation. Specifically, we use foreground masks as weak supervision through a raytrace pooling layer that enables perspective projection and backpropagation. Additionally, since the 3D reconstruction from masks is an ill posed problem, we propose to constrain the 3D reconstruction to the manifold of unlabeled realistic 3D shapes that match mask observations. We demonstrate that learning a log-barrier solution to this constrained optimization problem resembles the GAN objective, enabling the use of existing tools for training GANs. We evaluate and analyze the manifold constrained reconstruction on various datasets for single and multi-view reconstruction of both synthetic and real images.',\n",
       "  'arxiv_id': 'abs/1705.10904v2',\n",
       "  'category': None},\n",
       " 'abs/1803.02257v1': {'id': None,\n",
       "  'title': 'Methodology to analyze the accuracy of 3D objects reconstructed with\\n  collaborative robot based monocular LSD-SLAM',\n",
       "  'authors': ['Sergey Triputen',\n",
       "   'Atmaraaj Gopal',\n",
       "   'Thomas Weber',\n",
       "   'Christian Hofert',\n",
       "   'Kristiaan Schreve',\n",
       "   'Matthias Ratsch'],\n",
       "  'abstract': 'SLAM systems are mainly applied for robot navigation while research on feasibility for motion planning with SLAM for tasks like bin-picking, is scarce. Accurate 3D reconstruction of objects and environments is important for planning motion and computing optimal gripper pose to grasp objects. In this work, we propose the methods to analyze the accuracy of a 3D environment reconstructed using a LSD-SLAM system with a monocular camera mounted onto the gripper of a collaborative robot. We discuss and propose a solution to the pose space conversion problem. Finally, we present several criteria to analyze the 3D reconstruction accuracy. These could be used as guidelines to improve the accuracy of 3D reconstructions with monocular LSD-SLAM and other SLAM based solutions.',\n",
       "  'arxiv_id': 'abs/1803.02257v1',\n",
       "  'category': None},\n",
       " 'abs/1810.03774v1': {'id': None,\n",
       "  'title': 'Skeleton Driven Non-rigid Motion Tracking and 3D Reconstruction',\n",
       "  'authors': ['Shafeeq Elanattil',\n",
       "   'Peyman Moghadam',\n",
       "   'Simon Denman',\n",
       "   'Sridha Sridharan',\n",
       "   'Clinton Fookes'],\n",
       "  'abstract': 'This paper presents a method which can track and 3D reconstruct the non-rigid surface motion of human performance using a moving RGB-D camera. 3D reconstruction of marker-less human performance is a challenging problem due to the large range of articulated motions and considerable non-rigid deformations. Current approaches use local optimization for tracking. These methods need many iterations to converge and may get stuck in local minima during sudden articulated movements. We propose a puppet model-based tracking approach using skeleton prior, which provides a better initialization for tracking articulated movements. The proposed approach uses an aligned puppet model to estimate correct correspondences for human performance capture. We also contribute a synthetic dataset which provides ground truth locations for frame-by-frame geometry and skeleton joints of human subjects. Experimental results show that our approach is more robust when faced with sudden articulated motions, and provides better 3D reconstruction compared to the existing state-of-the-art approaches.',\n",
       "  'arxiv_id': 'abs/1810.03774v1',\n",
       "  'category': None},\n",
       " 'abs/2006.15427v1': {'id': None,\n",
       "  'title': 'On the generalization of learning-based 3D reconstruction',\n",
       "  'authors': ['Miguel Angel Bautista',\n",
       "   'Walter Talbott',\n",
       "   'Shuangfei Zhai',\n",
       "   'Nitish Srivastava',\n",
       "   'Joshua M Susskind'],\n",
       "  'abstract': 'State-of-the-art learning-based monocular 3D reconstruction methods learn priors over object categories on the training set, and as a result struggle to achieve reasonable generalization to object categories unseen during training. In this paper we study the inductive biases encoded in the model architecture that impact the generalization of learning-based 3D reconstruction methods. We find that 3 inductive biases impact performance: the spatial extent of the encoder, the use of the underlying geometry of the scene to describe point features, and the mechanism to aggregate information from multiple views. Additionally, we propose mechanisms to enforce those inductive biases: a point representation that is aware of camera position, and a variance cost to aggregate information across views. Our model achieves state-of-the-art results on the standard ShapeNet 3D reconstruction benchmark in various settings.',\n",
       "  'arxiv_id': 'abs/2006.15427v1',\n",
       "  'category': None},\n",
       " 'abs/2012.05551v2': {'id': None,\n",
       "  'title': 'DI-Fusion: Online Implicit 3D Reconstruction with Deep Priors',\n",
       "  'authors': ['Jiahui Huang', 'Shi-Sheng Huang', 'Haoxuan Song', 'Shi-Min Hu'],\n",
       "  'abstract': 'Previous online 3D dense reconstruction methods struggle to achieve the balance between memory storage and surface quality, largely due to the usage of stagnant underlying geometry representation, such as TSDF (truncated signed distance functions) or surfels, without any knowledge of the scene priors. In this paper, we present DI-Fusion (Deep Implicit Fusion), based on a novel 3D representation, i.e. Probabilistic Local Implicit Voxels (PLIVoxs), for online 3D reconstruction with a commodity RGB-D camera. Our PLIVox encodes scene priors considering both the local geometry and uncertainty parameterized by a deep neural network. With such deep priors, we are able to perform online implicit 3D reconstruction achieving state-of-the-art camera trajectory estimation accuracy and mapping quality, while achieving better storage efficiency compared with previous online 3D reconstruction approaches. Our implementation is available at https://www.github.com/huangjh-pub/di-fusion.',\n",
       "  'arxiv_id': 'abs/2012.05551v2',\n",
       "  'category': None},\n",
       " 'abs/2104.09259v1': {'id': None,\n",
       "  'title': 'Temporal Consistency Loss for High Resolution Textured and Clothed\\n  3DHuman Reconstruction from Monocular Video',\n",
       "  'authors': ['Akin Caliskan', 'Armin Mustafa', 'Adrian Hilton'],\n",
       "  'abstract': 'We present a novel method to learn temporally consistent 3D reconstruction of clothed people from a monocular video. Recent methods for 3D human reconstruction from monocular video using volumetric, implicit or parametric human shape models, produce per frame reconstructions giving temporally inconsistent output and limited performance when applied to video. In this paper, we introduce an approach to learn temporally consistent features for textured reconstruction of clothed 3D human sequences from monocular video by proposing two advances: a novel temporal consistency loss function; and hybrid representation learning for implicit 3D reconstruction from 2D images and coarse 3D geometry. The proposed advances improve the temporal consistency and accuracy of both the 3D reconstruction and texture prediction from a monocular video. Comprehensive comparative performance evaluation on images of people demonstrates that the proposed method significantly outperforms the state-of-the-art learning-based single image 3D human shape estimation approaches achieving significant improvement of reconstruction accuracy, completeness, quality and temporal consistency.',\n",
       "  'arxiv_id': 'abs/2104.09259v1',\n",
       "  'category': None},\n",
       " 'abs/1504.08308v3': {'id': None,\n",
       "  'title': 'Efficient Image-Space Extraction and Representation of 3D Surface\\n  Topography',\n",
       "  'authors': ['Matthias Zeppelzauer', 'Markus Seidl'],\n",
       "  'abstract': 'Surface topography refers to the geometric micro-structure of a surface and defines its tactile characteristics (typically in the sub-millimeter range). High-resolution 3D scanning techniques developed recently enable the 3D reconstruction of surfaces including their surface topography. In his paper, we present an efficient image-space technique for the extraction of surface topography from high-resolution 3D reconstructions. Additionally, we filter noise and enhance topographic attributes to obtain an improved representation for subsequent topography classification. Comprehensive experiments show that the our representation captures well topographic attributes and significantly improves classification performance compared to alternative 2D and 3D representations.',\n",
       "  'arxiv_id': 'abs/1504.08308v3',\n",
       "  'category': None},\n",
       " 'abs/1510.01070v1': {'id': None,\n",
       "  'title': 'Error control in the set-up of stereo camera systems for 3d animal\\n  tracking',\n",
       "  'authors': ['Andrea Cavagna',\n",
       "   'Chiara Creato',\n",
       "   'Lorenzo Del Castello',\n",
       "   'Irene Giardina',\n",
       "   'Stefania Melillo',\n",
       "   'Leonardo Parisi',\n",
       "   'Massimiliano Viale'],\n",
       "  'abstract': 'Three-dimensional tracking of animal systems is the key to the comprehension of collective behavior. Experimental data collected via a stereo camera system allow the reconstruction of the 3d trajectories of each individual in the group. Trajectories can then be used to compute some quantities of interest to better understand collective motion, such as velocities, distances between individuals and correlation functions. The reliability of the retrieved trajectories is strictly related to the accuracy of the 3d reconstruction. In this paper, we perform a careful analysis of the most significant errors affecting 3d reconstruction, showing how the accuracy depends on the camera system set-up and on the precision of the calibration parameters.',\n",
       "  'arxiv_id': 'abs/1510.01070v1',\n",
       "  'category': None},\n",
       " 'abs/1711.04118v1': {'id': None,\n",
       "  'title': 'Software for full-color 3D reconstruction of the biological tissues\\n  internal structure',\n",
       "  'authors': ['A. V. Khoperskov',\n",
       "   'M. E. Kovalev',\n",
       "   'A. S. Astakhov',\n",
       "   'V. V. Novochadov',\n",
       "   'A. A. Terpilovskiy',\n",
       "   'K. P. Tiras',\n",
       "   'D. A. Malanin'],\n",
       "  'abstract': 'A software for processing sets of full-color images of biological tissue histological sections is developed. We used histological sections obtained by the method of high-precision layer-by-layer grinding of frozen biological tissues. The software allows restoring the image of the tissue for an arbitrary cross-section of the tissue sample. Thus, our method is designed to create a full-color 3D reconstruction of the biological tissue structure. The resolution of 3D reconstruction is determined by the quality of the initial histological sections. The newly developed technology available to us provides a resolution of up to 5 - 10 {\\\\mu}m in three dimensions.',\n",
       "  'arxiv_id': 'abs/1711.04118v1',\n",
       "  'category': None},\n",
       " 'abs/1909.03101v1': {'id': None,\n",
       "  'title': 'Self-supervised Dense 3D Reconstruction from Monocular Endoscopic Video',\n",
       "  'authors': ['Xingtong Liu',\n",
       "   'Ayushi Sinha',\n",
       "   'Masaru Ishii',\n",
       "   'Gregory D. Hager',\n",
       "   'Russell H. Taylor',\n",
       "   'Mathias Unberath'],\n",
       "  'abstract': 'We present a self-supervised learning-based pipeline for dense 3D reconstruction from full-length monocular endoscopic videos without a priori modeling of anatomy or shading. Our method only relies on unlabeled monocular endoscopic videos and conventional multi-view stereo algorithms, and requires neither manual interaction nor patient CT in both training and application phases. In a cross-patient study using CT scans as groundtruth, we show that our method is able to produce photo-realistic dense 3D reconstructions with submillimeter mean residual errors from endoscopic videos from unseen patients and scopes.',\n",
       "  'arxiv_id': 'abs/1909.03101v1',\n",
       "  'category': None},\n",
       " 'abs/1912.03858v1': {'id': None,\n",
       "  'title': 'Bundle Adjustment Revisited',\n",
       "  'authors': ['Yu Chen', 'Yisong Chen', 'Guoping Wang'],\n",
       "  'abstract': \"3D reconstruction has been developing all these two decades, from moderate to medium size and to large scale. It's well known that bundle adjustment plays an important role in 3D reconstruction, mainly in Structure from Motion(SfM) and Simultaneously Localization and Mapping(SLAM). While bundle adjustment optimizes camera parameters and 3D points as a non-negligible final step, it suffers from memory and efficiency requirements in very large scale reconstruction. In this paper, we study the development of bundle adjustment elaborately in both conventional and distributed approaches. The detailed derivation and pseudo code are also given in this paper.\",\n",
       "  'arxiv_id': 'abs/1912.03858v1',\n",
       "  'category': None},\n",
       " 'abs/2011.13649v1': {'id': None,\n",
       "  'title': 'Descriptor-Free Multi-View Region Matching for Instance-Wise 3D\\n  Reconstruction',\n",
       "  'authors': ['Takuma Doi',\n",
       "   'Fumio Okura',\n",
       "   'Toshiki Nagahara',\n",
       "   'Yasuyuki Matsushita',\n",
       "   'Yasushi Yagi'],\n",
       "  'abstract': 'This paper proposes a multi-view extension of instance segmentation without relying on texture or shape descriptor matching. Multi-view instance segmentation becomes challenging for scenes with repetitive textures and shapes, e.g., plant leaves, due to the difficulty of multi-view matching using texture or shape descriptors. To this end, we propose a multi-view region matching method based on epipolar geometry, which does not rely on any feature descriptors. We further show that the epipolar region matching can be easily integrated into instance segmentation and effective for instance-wise 3D reconstruction. Experiments demonstrate the improved accuracy of multi-view instance matching and the 3D reconstruction compared to the baseline methods.',\n",
       "  'arxiv_id': 'abs/2011.13649v1',\n",
       "  'category': None},\n",
       " 'abs/2110.11599v1': {'id': None,\n",
       "  'title': 'High Fidelity 3D Reconstructions with Limited Physical Views',\n",
       "  'authors': ['Mosam Dabhi',\n",
       "   'Chaoyang Wang',\n",
       "   'Kunal Saluja',\n",
       "   'Laszlo Jeni',\n",
       "   'Ian Fasel',\n",
       "   'Simon Lucey'],\n",
       "  'abstract': 'Multi-view triangulation is the gold standard for 3D reconstruction from 2D correspondences given known calibration and sufficient views. However in practice, expensive multi-view setups -- involving tens sometimes hundreds of cameras -- are required in order to obtain the high fidelity 3D reconstructions necessary for many modern applications. In this paper we present a novel approach that leverages recent advances in 2D-3D lifting using neural shape priors while also enforcing multi-view equivariance. We show how our method can achieve comparable fidelity to expensive calibrated multi-view rigs using a limited (2-3) number of uncalibrated camera views.',\n",
       "  'arxiv_id': 'abs/2110.11599v1',\n",
       "  'category': None},\n",
       " 'abs/0805.2864v1': {'id': None,\n",
       "  'title': \"Fusion d'images: application au contrôle de la distribution des\\n  biopsies prostatiques\",\n",
       "  'authors': ['Pierre Mozer',\n",
       "   'Michael Baumann',\n",
       "   'G. Chevreau',\n",
       "   'Jocelyne Troccaz'],\n",
       "  'abstract': 'This paper is about the application of a 3D ultrasound data fusion technique to the 3D reconstruction of prostate biopies in a reference volume. The method is introduced and its evaluation on a series of data coming from 15 patients is described.',\n",
       "  'arxiv_id': 'abs/0805.2864v1',\n",
       "  'category': None},\n",
       " 'abs/1701.04752v1': {'id': None,\n",
       "  'title': '3D Reconstruction of Simple Objects from A Single View Silhouette Image',\n",
       "  'authors': ['Xinhan Di', 'Pengqian Yu'],\n",
       "  'abstract': 'While recent deep neural networks have achieved promising results for 3D reconstruction from a single-view image, these rely on the availability of RGB textures in images and extra information as supervision. In this work, we propose novel stacked hierarchical networks and an end to end training strategy to tackle a more challenging task for the first time, 3D reconstruction from a single-view 2D silhouette image. We demonstrate that our model is able to conduct 3D reconstruction from a single-view silhouette image both qualitatively and quantitatively. Evaluation is performed using Shapenet for the single-view reconstruction and results are presented in comparison with a single network, to highlight the improvements obtained with the proposed stacked networks and the end to end training strategy. Furthermore, 3D re- construction in forms of IoU is compared with the state of art 3D reconstruction from a single-view RGB image, and the proposed model achieves higher IoU than the state of art of reconstruction from a single view RGB image.',\n",
       "  'arxiv_id': 'abs/1701.04752v1',\n",
       "  'category': None},\n",
       " 'abs/1703.04699v1': {'id': None,\n",
       "  'title': 'A fully end-to-end deep learning approach for real-time simultaneous 3D\\n  reconstruction and material recognition',\n",
       "  'authors': ['Cheng Zhao', 'Li Sun', 'Rustam Stolkin'],\n",
       "  'abstract': 'This paper addresses the problem of simultaneous 3D reconstruction and material recognition and segmentation. Enabling robots to recognise different materials (concrete, metal etc.) in a scene is important for many tasks, e.g. robotic interventions in nuclear decommissioning. Previous work on 3D semantic reconstruction has predominantly focused on recognition of everyday domestic objects (tables, chairs etc.), whereas previous work on material recognition has largely been confined to single 2D images without any 3D reconstruction. Meanwhile, most 3D semantic reconstruction methods rely on computationally expensive post-processing, using Fully-Connected Conditional Random Fields (CRFs), to achieve consistent segmentations. In contrast, we propose a deep learning method which performs 3D reconstruction while simultaneously recognising different types of materials and labelling them at the pixel level. Unlike previous methods, we propose a fully end-to-end approach, which does not require hand-crafted features or CRF post-processing. Instead, we use only learned features, and the CRF segmentation constraints are incorporated inside the fully end-to-end learned system. We present the results of experiments, in which we trained our system to perform real-time 3D semantic reconstruction for 23 different materials in a real-world application. The run-time performance of the system can be boosted to around 10Hz, using a conventional GPU, which is enough to achieve real-time semantic reconstruction using a 30fps RGB-D camera. To the best of our knowledge, this work is the first real-time end-to-end system for simultaneous 3D reconstruction and material recognition.',\n",
       "  'arxiv_id': 'abs/1703.04699v1',\n",
       "  'category': None},\n",
       " 'abs/1709.05665v2': {'id': None,\n",
       "  'title': 'Automatic Tool Landmark Detection for Stereo Vision in Robot-Assisted\\n  Retinal Surgery',\n",
       "  'authors': ['Thomas Probst',\n",
       "   'Kevis-Kokitsi Maninis',\n",
       "   'Ajad Chhatkuli',\n",
       "   'Mouloud Ourak',\n",
       "   'Emmanuel Vander Poorten',\n",
       "   'Luc Van Gool'],\n",
       "  'abstract': 'Computer vision and robotics are being increasingly applied in medical interventions. Especially in interventions where extreme precision is required they could make a difference. One such application is robot-assisted retinal microsurgery. In recent works, such interventions are conducted under a stereo-microscope, and with a robot-controlled surgical tool. The complementarity of computer vision and robotics has however not yet been fully exploited. In order to improve the robot control we are interested in 3D reconstruction of the anatomy and in automatic tool localization using a stereo microscope. In this paper, we solve this problem for the first time using a single pipeline, starting from uncalibrated cameras to reach metric 3D reconstruction and registration, in retinal microsurgery. The key ingredients of our method are: (a) surgical tool landmark detection, and (b) 3D reconstruction with the stereo microscope, using the detected landmarks. To address the former, we propose a novel deep learning method that detects and recognizes keypoints in high definition images at higher than real-time speed. We use the detected 2D keypoints along with their corresponding 3D coordinates obtained from the robot sensors to calibrate the stereo microscope using an affine projection model. We design an online 3D reconstruction pipeline that makes use of smoothness constraints and performs robot-to-camera registration. The entire pipeline is extensively validated on open-sky porcine eye sequences. Quantitative and qualitative results are presented for all steps.',\n",
       "  'arxiv_id': 'abs/1709.05665v2',\n",
       "  'category': None},\n",
       " 'abs/1803.06267v1': {'id': None,\n",
       "  'title': 'Consistent sets of lines with no colorful incidence',\n",
       "  'authors': ['Boris Bukh',\n",
       "   'Xavier Goaoc',\n",
       "   'Alfredo Hubard',\n",
       "   'Matthew Trager'],\n",
       "  'abstract': 'We consider incidences among colored sets of lines in $\\\\mathbb{R}^d$ and examine whether the existence of certain concurrences between lines of $k$ colors force the existence of at least one concurrence between lines of $k+1$ colors. This question is relevant for problems in 3D reconstruction in computer vision.',\n",
       "  'arxiv_id': 'abs/1803.06267v1',\n",
       "  'category': None},\n",
       " 'abs/1812.03828v2': {'id': None,\n",
       "  'title': 'Occupancy Networks: Learning 3D Reconstruction in Function Space',\n",
       "  'authors': ['Lars Mescheder',\n",
       "   'Michael Oechsle',\n",
       "   'Michael Niemeyer',\n",
       "   'Sebastian Nowozin',\n",
       "   'Andreas Geiger'],\n",
       "  'abstract': 'With the advent of deep neural networks, learning-based approaches for 3D reconstruction have gained popularity. However, unlike for images, in 3D there is no canonical representation which is both computationally and memory efficient yet allows for representing high-resolution geometry of arbitrary topology. Many of the state-of-the-art learning-based 3D reconstruction approaches can hence only represent very coarse 3D geometry or are limited to a restricted domain. In this paper, we propose Occupancy Networks, a new representation for learning-based 3D reconstruction methods. Occupancy networks implicitly represent the 3D surface as the continuous decision boundary of a deep neural network classifier. In contrast to existing approaches, our representation encodes a description of the 3D output at infinite resolution without excessive memory footprint. We validate that our representation can efficiently encode 3D structure and can be inferred from various kinds of input. Our experiments demonstrate competitive results, both qualitatively and quantitatively, for the challenging tasks of 3D reconstruction from single images, noisy point clouds and coarse discrete voxel grids. We believe that occupancy networks will become a useful tool in a wide variety of learning-based 3D tasks.',\n",
       "  'arxiv_id': 'abs/1812.03828v2',\n",
       "  'category': None},\n",
       " 'abs/1812.05806v2': {'id': None,\n",
       "  'title': 'A Self-Supervised Bootstrap Method for Single-Image 3D Face\\n  Reconstruction',\n",
       "  'authors': ['Yifan Xing', 'Rahul Tewari', 'Paulo R. S. Mendonca'],\n",
       "  'abstract': 'State-of-the-art methods for 3D reconstruction of faces from a single image require 2D-3D pairs of ground-truth data for supervision. Such data is costly to acquire, and most datasets available in the literature are restricted to pairs for which the input 2D images depict faces in a near fronto-parallel pose. Therefore, many data-driven methods for single-image 3D facial reconstruction perform poorly on profile and near-profile faces. We propose a method to improve the performance of single-image 3D facial reconstruction networks by utilizing the network to synthesize its own training data for fine-tuning, comprising: (i) single-image 3D reconstruction of faces in near-frontal images without ground-truth 3D shape; (ii) application of a rigid-body transformation to the reconstructed face model; (iii) rendering of the face model from new viewpoints; and (iv) use of the rendered image and corresponding 3D reconstruction as additional data for supervised fine-tuning. The new 2D-3D pairs thus produced have the same high-quality observed for near fronto-parallel reconstructions, thereby nudging the network towards more uniform performance as a function of the viewing angle of input faces. Application of the proposed technique to the fine-tuning of a state-of-the-art single-image 3D-reconstruction network for faces demonstrates the usefulness of the method, with particularly significant gains for profile or near-profile views.',\n",
       "  'arxiv_id': 'abs/1812.05806v2',\n",
       "  'category': None},\n",
       " 'abs/2006.10042v1': {'id': None,\n",
       "  'title': 'Learning to Detect 3D Reflection Symmetry for Single-View Reconstruction',\n",
       "  'authors': ['Yichao Zhou', 'Shichen Liu', 'Yi Ma'],\n",
       "  'abstract': '3D reconstruction from a single RGB image is a challenging problem in computer vision. Previous methods are usually solely data-driven, which lead to inaccurate 3D shape recovery and limited generalization capability. In this work, we focus on object-level 3D reconstruction and present a geometry-based end-to-end deep learning framework that first detects the mirror plane of reflection symmetry that commonly exists in man-made objects and then predicts depth maps by finding the intra-image pixel-wise correspondence of the symmetry. Our method fully utilizes the geometric cues from symmetry during the test time by building plane-sweep cost volumes, a powerful tool that has been used in multi-view stereopsis. To our knowledge, this is the first work that uses the concept of cost volumes in the setting of single-image 3D reconstruction. We conduct extensive experiments on the ShapeNet dataset and find that our reconstruction method significantly outperforms the previous state-of-the-art single-view 3D reconstruction networks in term of the accuracy of camera poses and depth maps, without requiring objects being completely symmetric. Code is available at https://github.com/zhou13/symmetrynet.',\n",
       "  'arxiv_id': 'abs/2006.10042v1',\n",
       "  'category': None},\n",
       " 'abs/2007.01636v1': {'id': None,\n",
       "  'title': 'Noise2Filter: fast, self-supervised learning and real-time\\n  reconstruction for 3D Computed Tomography',\n",
       "  'authors': ['Marinus J. Lagerwerf',\n",
       "   'Allard A. Hendriksen',\n",
       "   'Jan-Willem Buurlage',\n",
       "   'K. Joost Batenburg'],\n",
       "  'abstract': 'At X-ray beamlines of synchrotron light sources, the achievable time-resolution for 3D tomographic imaging of the interior of an object has been reduced to a fraction of a second, enabling rapidly changing structures to be examined. The associated data acquisition rates require sizable computational resources for reconstruction. Therefore, full 3D reconstruction of the object is usually performed after the scan has completed. Quasi-3D reconstruction -- where several interactive 2D slices are computed instead of a 3D volume -- has been shown to be significantly more efficient, and can enable the real-time reconstruction and visualization of the interior. However, quasi-3D reconstruction relies on filtered backprojection type algorithms, which are typically sensitive to measurement noise. To overcome this issue, we propose Noise2Filter, a learned filter method that can be trained using only the measured data, and does not require any additional training data. This method combines quasi-3D reconstruction, learned filters, and self-supervised learning to derive a tomographic reconstruction method that can be trained in under a minute and evaluated in real-time. We show limited loss of accuracy compared to training with additional training data, and improved accuracy compared to standard filter-based methods.',\n",
       "  'arxiv_id': 'abs/2007.01636v1',\n",
       "  'category': None},\n",
       " 'abs/2112.00879v2': {'id': None,\n",
       "  'title': 'Generating Diverse 3D Reconstructions from a Single Occluded Face Image',\n",
       "  'authors': ['Rahul Dey', 'Vishnu Naresh Boddeti'],\n",
       "  'abstract': 'Occlusions are a common occurrence in unconstrained face images. Single image 3D reconstruction from such face images often suffers from corruption due to the presence of occlusions. Furthermore, while a plurality of 3D reconstructions is plausible in the occluded regions, existing approaches are limited to generating only a single solution. To address both of these challenges, we present Diverse3DFace, which is specifically designed to simultaneously generate a diverse and realistic set of 3D reconstructions from a single occluded face image. It consists of three components: a global+local shape fitting process, a graph neural network-based mesh VAE, and a Determinantal Point Process based diversity promoting iterative optimization procedure. Quantitative and qualitative comparisons of 3D reconstruction on occluded faces show that Diverse3DFace can estimate 3D shapes that are consistent with the visible regions in the target image while exhibiting high, yet realistic, levels of diversity on the occluded regions. On face images occluded by masks, glasses, and other random objects, Diverse3DFace generates a distribution of 3D shapes having ~50% higher diversity on the occluded regions compared to the baselines. Moreover, our closest sample to the ground truth has ~40% lower MSE than the singular reconstructions by existing approaches.',\n",
       "  'arxiv_id': 'abs/2112.00879v2',\n",
       "  'category': None},\n",
       " 'abs/2208.00183v1': {'id': None,\n",
       "  'title': 'Few-shot Single-view 3D Reconstruction with Memory Prior Contrastive\\n  Network',\n",
       "  'authors': ['Zhen Xing',\n",
       "   'Yijiang Chen',\n",
       "   'Zhixin Ling',\n",
       "   'Xiangdong Zhou',\n",
       "   'Yu Xiang'],\n",
       "  'abstract': '3D reconstruction of novel categories based on few-shot learning is appealing in real-world applications and attracts increasing research interests. Previous approaches mainly focus on how to design shape prior models for different categories. Their performance on unseen categories is not very competitive. In this paper, we present a Memory Prior Contrastive Network (MPCN) that can store shape prior knowledge in a few-shot learning based 3D reconstruction framework. With the shape memory, a multi-head attention module is proposed to capture different parts of a candidate shape prior and fuse these parts together to guide 3D reconstruction of novel categories. Besides, we introduce a 3D-aware contrastive learning method, which can not only complement the retrieval accuracy of memory network, but also better organize image features for downstream tasks. Compared with previous few-shot 3D reconstruction methods, MPCN can handle the inter-class variability without category annotations. Experimental results on a benchmark synthetic dataset and the Pascal3D+ real-world dataset show that our model outperforms the current state-of-the-art methods significantly.',\n",
       "  'arxiv_id': 'abs/2208.00183v1',\n",
       "  'category': None},\n",
       " 'abs/2211.11836v1': {'id': None,\n",
       "  'title': 'Towards Live 3D Reconstruction from Wearable Video: An Evaluation of\\n  V-SLAM, NeRF, and Videogrammetry Techniques',\n",
       "  'authors': ['David Ramirez', 'Suren Jayasuriya', 'Andreas Spanias'],\n",
       "  'abstract': 'Mixed reality (MR) is a key technology which promises to change the future of warfare. An MR hybrid of physical outdoor environments and virtual military training will enable engagements with long distance enemies, both real and simulated. To enable this technology, a large-scale 3D model of a physical environment must be maintained based on live sensor observations. 3D reconstruction algorithms should utilize the low cost and pervasiveness of video camera sensors, from both overhead and soldier-level perspectives. Mapping speed and 3D quality can be balanced to enable live MR training in dynamic environments. Given these requirements, we survey several 3D reconstruction algorithms for large-scale mapping for military applications given only live video. We measure 3D reconstruction performance from common structure from motion, visual-SLAM, and photogrammetry techniques. This includes the open source algorithms COLMAP, ORB-SLAM3, and NeRF using Instant-NGP. We utilize the autonomous driving academic benchmark KITTI, which includes both dashboard camera video and lidar produced 3D ground truth. With the KITTI data, our primary contribution is a quantitative evaluation of 3D reconstruction computational speed when considering live video.',\n",
       "  'arxiv_id': 'abs/2211.11836v1',\n",
       "  'category': None},\n",
       " 'abs/0907.3215v1': {'id': None,\n",
       "  'title': 'Fully Automatic 3D Reconstruction of Histological Images',\n",
       "  'authors': ['Ulas Bagci', 'Li Bai'],\n",
       "  'abstract': 'In this paper, we propose a computational framework for 3D volume reconstruction from 2D histological slices using registration algorithms in feature space. To improve the quality of reconstructed 3D volume, first, intensity variations in images are corrected by an intensity standardization process which maps image intensity scale to a standard scale where similar intensities correspond to similar tissues. Second, a subvolume approach is proposed for 3D reconstruction by dividing standardized slices into groups. Third, in order to improve the quality of the reconstruction process, an automatic best reference slice selection algorithm is developed based on an iterative assessment of image entropy and mean square error of the registration process. Finally, we demonstrate that the choice of the reference slice has a significant impact on registration quality and subsequent 3D reconstruction.',\n",
       "  'arxiv_id': 'abs/0907.3215v1',\n",
       "  'category': None},\n",
       " 'abs/1210.3188v1': {'id': None,\n",
       "  'title': 'Holographic microscopy for the three-dimensional exploration of light\\n  scattering from gold nanomarkers in biological media',\n",
       "  'authors': ['Fadwa Joud',\n",
       "   'Frédéric Verpillat',\n",
       "   'Pierre Desbiolles',\n",
       "   'Marie Abboud',\n",
       "   'Michel Gross'],\n",
       "  'abstract': 'The 3D structure of light scattering from dark-field illuminated live 3T3 cells marked with 40 nm gold nanomarkers is explored. For this purpose, we use a high resolution holographic microscope combining the off-axis heterodyne geometry and the phase-shifting acquisition of the digital holograms. Images are obtained using a novel 3D reconstruction method providing longitudinally undistorted 3D images. A comparative study of the 3D reconstructions of the scattered fields allows us to locate the gold markers which yield, contrarily to the cellular structures, well defined bright scattering patterns that are not angularly titled and clearly located along the optical axis. This characterization is an unambiguous signature of the presence of the gold biological nanomarkers, and validates the capability of digital holographic microscopy to discriminate them from background signals in live cells.',\n",
       "  'arxiv_id': 'abs/1210.3188v1',\n",
       "  'category': None},\n",
       " 'abs/1301.5491v1': {'id': None,\n",
       "  'title': 'ChESS - Quick and Robust Detection of Chess-board Features',\n",
       "  'authors': ['Stuart Bennett', 'Joan Lasenby'],\n",
       "  'abstract': \"Localization of chess-board vertices is a common task in computer vision, underpinning many applications, but relatively little work focusses on designing a specific feature detector that is fast, accurate and robust. In this paper the `Chess-board Extraction by Subtraction and Summation' (ChESS) feature detector, designed to exclusively respond to chess-board vertices, is presented. The method proposed is robust against noise, poor lighting and poor contrast, requires no prior knowledge of the extent of the chess-board pattern, is computationally very efficient, and provides a strength measure of detected features. Such a detector has significant application both in the key field of camera calibration, as well as in Structured Light 3D reconstruction. Evidence is presented showing its robustness, accuracy, and efficiency in comparison to other commonly used detectors both under simulation and in experimental 3D reconstruction of flat plate and cylindrical objects\",\n",
       "  'arxiv_id': 'abs/1301.5491v1',\n",
       "  'category': None},\n",
       " 'abs/1602.04502v1': {'id': None,\n",
       "  'title': 'Do We Need Binary Features for 3D Reconstruction?',\n",
       "  'authors': ['Bin Fan',\n",
       "   'Qingqun Kong',\n",
       "   'Wei Sui',\n",
       "   'Zhiheng Wang',\n",
       "   'Xinchao Wang',\n",
       "   'Shiming Xiang',\n",
       "   'Chunhong Pan',\n",
       "   'Pascal Fua'],\n",
       "  'abstract': 'Binary features have been incrementally popular in the past few years due to their low memory footprints and the efficient computation of Hamming distance between binary descriptors. They have been shown with promising results on some real time applications, e.g., SLAM, where the matching operations are relative few. However, in computer vision, there are many applications such as 3D reconstruction requiring lots of matching operations between local features. Therefore, a natural question is that is the binary feature still a promising solution to this kind of applications? To get the answer, this paper conducts a comparative study of binary features and their matching methods on the context of 3D reconstruction in a recently proposed large scale mutliview stereo dataset. Our evaluations reveal that not all binary features are capable of this task. Most of them are inferior to the classical SIFT based method in terms of reconstruction accuracy and completeness with a not significant better computational performance.',\n",
       "  'arxiv_id': 'abs/1602.04502v1',\n",
       "  'category': None},\n",
       " 'abs/1604.00449v1': {'id': None,\n",
       "  'title': '3D-R2N2: A Unified Approach for Single and Multi-view 3D Object\\n  Reconstruction',\n",
       "  'authors': ['Christopher B. Choy',\n",
       "   'Danfei Xu',\n",
       "   'JunYoung Gwak',\n",
       "   'Kevin Chen',\n",
       "   'Silvio Savarese'],\n",
       "  'abstract': 'Inspired by the recent success of methods that employ shape priors to achieve robust 3D reconstructions, we propose a novel recurrent neural network architecture that we call the 3D Recurrent Reconstruction Neural Network (3D-R2N2). The network learns a mapping from images of objects to their underlying 3D shapes from a large collection of synthetic data. Our network takes in one or more images of an object instance from arbitrary viewpoints and outputs a reconstruction of the object in the form of a 3D occupancy grid. Unlike most of the previous works, our network does not require any image annotations or object class labels for training or testing. Our extensive experimental analysis shows that our reconstruction framework i) outperforms the state-of-the-art methods for single view reconstruction, and ii) enables the 3D reconstruction of objects in situations when traditional SFM/SLAM methods fail (because of lack of texture and/or wide baseline).',\n",
       "  'arxiv_id': 'abs/1604.00449v1',\n",
       "  'category': None},\n",
       " 'abs/1604.02885v3': {'id': None,\n",
       "  'title': 'Semantic 3D Reconstruction with Continuous Regularization and Ray\\n  Potentials Using a Visibility Consistency Constraint',\n",
       "  'authors': ['Nikolay Savinov',\n",
       "   'Christian Haene',\n",
       "   'Lubor Ladicky',\n",
       "   'Marc Pollefeys'],\n",
       "  'abstract': 'We propose an approach for dense semantic 3D reconstruction which uses a data term that is defined as potentials over viewing rays, combined with continuous surface area penalization. Our formulation is a convex relaxation which we augment with a crucial non-convex constraint that ensures exact handling of visibility. To tackle the non-convex minimization problem, we propose a majorize-minimize type strategy which converges to a critical point. We demonstrate the benefits of using the non-convex constraint experimentally. For the geometry-only case, we set a new state of the art on two datasets of the commonly used Middlebury multi-view stereo benchmark. Moreover, our general-purpose formulation directly reconstructs thin objects, which are usually treated with specialized algorithms. A qualitative evaluation on the dense semantic 3D reconstruction task shows that we improve significantly over previous methods.',\n",
       "  'arxiv_id': 'abs/1604.02885v3',\n",
       "  'category': None},\n",
       " 'abs/1606.05002v2': {'id': None,\n",
       "  'title': '3DFS: Deformable Dense Depth Fusion and Segmentation for Object\\n  Reconstruction from a Handheld Camera',\n",
       "  'authors': ['Tanmay Gupta',\n",
       "   'Daeyun Shin',\n",
       "   'Naren Sivagnanadasan',\n",
       "   'Derek Hoiem'],\n",
       "  'abstract': 'We propose an approach for 3D reconstruction and segmentation of a single object placed on a flat surface from an input video. Our approach is to perform dense depth map estimation for multiple views using a proposed objective function that preserves detail. The resulting depth maps are then fused using a proposed implicit surface function that is robust to estimation error, producing a smooth surface reconstruction of the entire scene. Finally, the object is segmented from the remaining scene using a proposed 2D-3D segmentation that incorporates image and depth cues with priors and regularization over the 3D volume and 2D segmentations. We evaluate 3D reconstructions qualitatively on our Object-Videos dataset, comparing to fusion, multiview stereo, and segmentation baselines. We also quantitatively evaluate the dense depth estimation using the RGBD Scenes V2 dataset [Henry et al. 2013] and the segmentation using keyframe annotations of the Object-Videos dataset.',\n",
       "  'arxiv_id': 'abs/1606.05002v2',\n",
       "  'category': None},\n",
       " 'abs/1609.03986v2': {'id': None,\n",
       "  'title': 'The CUDA LATCH Binary Descriptor: Because Sometimes Faster Means Better',\n",
       "  'authors': ['Christopher Parker',\n",
       "   'Matthew Daiter',\n",
       "   'Kareem Omar',\n",
       "   'Gil Levi',\n",
       "   'Tal Hassner'],\n",
       "  'abstract': 'Accuracy, descriptor size, and the time required for extraction and matching are all important factors when selecting local image descriptors. To optimize over all these requirements, this paper presents a CUDA port for the recent Learned Arrangement of Three Patches (LATCH) binary descriptors to the GPU platform. The design of LATCH makes it well suited for GPU processing. Owing to its small size and binary nature, the GPU can further be used to efficiently match LATCH features. Taken together, this leads to breakneck descriptor extraction and matching speeds. We evaluate the trade off between these speeds and the quality of results in a feature matching intensive application. To this end, we use our proposed CUDA LATCH (CLATCH) to recover structure from motion (SfM), comparing 3D reconstructions and speed using different representations. Our results show that CLATCH provides high quality 3D reconstructions at fractions of the time required by other representations, with little, if any, loss of reconstruction quality.',\n",
       "  'arxiv_id': 'abs/1609.03986v2',\n",
       "  'category': None},\n",
       " 'abs/1707.07360v1': {'id': None,\n",
       "  'title': 'Compact Model Representation for 3D Reconstruction',\n",
       "  'authors': ['Jhony K. Pontes',\n",
       "   'Chen Kong',\n",
       "   'Anders Eriksson',\n",
       "   'Clinton Fookes',\n",
       "   'Sridha Sridharan',\n",
       "   'Simon Lucey'],\n",
       "  'abstract': '3D reconstruction from 2D images is a central problem in computer vision. Recent works have been focusing on reconstruction directly from a single image. It is well known however that only one image cannot provide enough information for such a reconstruction. A prior knowledge that has been entertained are 3D CAD models due to its online ubiquity. A fundamental question is how to compactly represent millions of CAD models while allowing generalization to new unseen objects with fine-scaled geometry. We introduce an approach to compactly represent a 3D mesh. Our method first selects a 3D model from a graph structure by using a novel free-form deformation FFD 3D-2D registration, and then the selected 3D model is refined to best fit the image silhouette. We perform a comprehensive quantitative and qualitative analysis that demonstrates impressive dense and realistic 3D reconstruction from single images.',\n",
       "  'arxiv_id': 'abs/1707.07360v1',\n",
       "  'category': None},\n",
       " 'abs/1804.02836v2': {'id': None,\n",
       "  'title': 'Photometric Stereo in Participating Media Considering Shape-Dependent\\n  Forward Scatter',\n",
       "  'authors': ['Yuki Fujimura',\n",
       "   'Masaaki Iiyama',\n",
       "   'Atsushi Hashimoto',\n",
       "   'Michihiko Minoh'],\n",
       "  'abstract': 'Images captured in participating media such as murky water, fog, or smoke are degraded by scattered light. Thus, the use of traditional three-dimensional (3D) reconstruction techniques in such environments is difficult. In this paper, we propose a photometric stereo method for participating media. The proposed method differs from previous studies with respect to modeling shape-dependent forward scatter. In the proposed model, forward scatter is described as an analytical form using lookup tables and is represented by spatially-variant kernels. We also propose an approximation of a large-scale dense matrix as a sparse matrix, which enables the removal of forward scatter. Experiments with real and synthesized data demonstrate that the proposed method improves 3D reconstruction in participating media.',\n",
       "  'arxiv_id': 'abs/1804.02836v2',\n",
       "  'category': None},\n",
       " 'abs/1807.02294v1': {'id': None,\n",
       "  'title': 'Combining SLAM with muti-spectral photometric stereo for real-time dense\\n  3D reconstruction',\n",
       "  'authors': ['Yuanhong Xu', 'Pei Dong', 'Junyu Dong', 'Lin Qi'],\n",
       "  'abstract': 'Obtaining dense 3D reconstrution with low computational cost is one of the important goals in the field of SLAM. In this paper we propose a dense 3D reconstruction framework from monocular multispectral video sequences using jointly semi-dense SLAM and Multispectral Photometric Stereo approaches. Starting from multispectral video, SALM (a) reconstructs a semi-dense 3D shape that will be densified;(b) recovers relative sparse depth map that is then fed as prioris into optimization-based multispectral photometric stereo for a more accurate dense surface normal recovery;(c)obtains camera pose that is subsequently used for conversion of view in the process of fusion where we combine the relative sparse point cloud with the dense surface normal using the automated cross-scale fusion method proposed in this paper to get a dense point cloud with subtle texture information. Experiments show that our method can effectively obtain denser 3D reconstructions.',\n",
       "  'arxiv_id': 'abs/1807.02294v1',\n",
       "  'category': None},\n",
       " 'abs/1807.06294v2': {'id': None,\n",
       "  'title': 'GeoDesc: Learning Local Descriptors by Integrating Geometry Constraints',\n",
       "  'authors': ['Zixin Luo',\n",
       "   'Tianwei Shen',\n",
       "   'Lei Zhou',\n",
       "   'Siyu Zhu',\n",
       "   'Runze Zhang',\n",
       "   'Yao Yao',\n",
       "   'Tian Fang',\n",
       "   'Long Quan'],\n",
       "  'abstract': 'Learned local descriptors based on Convolutional Neural Networks (CNNs) have achieved significant improvements on patch-based benchmarks, whereas not having demonstrated strong generalization ability on recent benchmarks of image-based 3D reconstruction. In this paper, we mitigate this limitation by proposing a novel local descriptor learning approach that integrates geometry constraints from multi-view reconstructions, which benefits the learning process in terms of data generation, data sampling and loss computation. We refer to the proposed descriptor as GeoDesc, and demonstrate its superior performance on various large-scale benchmarks, and in particular show its great success on challenging reconstruction tasks. Moreover, we provide guidelines towards practical integration of learned descriptors in Structure-from-Motion (SfM) pipelines, showing the good trade-off that GeoDesc delivers to 3D reconstruction tasks between accuracy and efficiency.',\n",
       "  'arxiv_id': 'abs/1807.06294v2',\n",
       "  'category': None},\n",
       " 'abs/1807.11080v1': {'id': None,\n",
       "  'title': 'Towards ultra-high resolution 3D reconstruction of a whole rat brain\\n  from 3D-PLI data',\n",
       "  'authors': ['Sharib Ali',\n",
       "   'Martin Schober',\n",
       "   'Philipp Schlöme',\n",
       "   'Katrin Amunts',\n",
       "   'Markus Axer',\n",
       "   'Karl Rohr'],\n",
       "  'abstract': '3D reconstruction of the fiber connectivity of the rat brain at microscopic scale enables gaining detailed insight about the complex structural organization of the brain. We introduce a new method for registration and 3D reconstruction of high- and ultra-high resolution (64 $\\\\mu$m and 1.3 $\\\\mu$m pixel size) histological images of a Wistar rat brain acquired by 3D polarized light imaging (3D-PLI). Our method exploits multi-scale and multi-modal 3D-PLI data up to cellular resolution. We propose a new feature transform-based similarity measure and a weighted regularization scheme for accurate and robust non-rigid registration. To transform the 1.3 $\\\\mu$m ultra-high resolution data to the reference blockface images a feature-based registration method followed by a non-rigid registration is proposed. Our approach has been successfully applied to 278 histological sections of a rat brain and the performance has been quantitatively evaluated using manually placed landmarks by an expert.',\n",
       "  'arxiv_id': 'abs/1807.11080v1',\n",
       "  'category': None},\n",
       " 'abs/1808.02244v1': {'id': None,\n",
       "  'title': 'A Generic Multi-Projection-Center Model and Calibration Method for Light\\n  Field Cameras',\n",
       "  'authors': ['Qi Zhang',\n",
       "   'Chunping Zhang',\n",
       "   'Jinbo Ling',\n",
       "   'Qing Wang',\n",
       "   'Jingyi Yu'],\n",
       "  'abstract': 'Light field cameras can capture both spatial and angular information of light rays, enabling 3D reconstruction by a single exposure. The geometry of 3D reconstruction is affected by intrinsic parameters of a light field camera significantly. In the paper, we propose a multi-projection-center (MPC) model with 6 intrinsic parameters to characterize light field cameras based on traditional two-parallel-plane (TPP) representation. The MPC model can generally parameterize light field in different imaging formations, including conventional and focused light field cameras. By the constraints of 4D ray and 3D geometry, a 3D projective transformation is deduced to describe the relationship between geometric structure and the MPC coordinates. Based on the MPC model and projective transformation, we propose a calibration algorithm to verify our light field camera model. Our calibration method includes a close-form solution and a non-linear optimization by minimizing re-projection errors. Experimental results on both simulated and real scene data have verified the performance of our algorithm.',\n",
       "  'arxiv_id': 'abs/1808.02244v1',\n",
       "  'category': None},\n",
       " 'abs/1808.08544v1': {'id': None,\n",
       "  'title': 'Scale Drift Correction of Camera Geo-Localization using Geo-Tagged\\n  Images',\n",
       "  'authors': ['Kazuya Iwami', 'Satoshi Ikehata', 'Kiyoharu Aizawa'],\n",
       "  'abstract': 'Camera geo-localization from a monocular video is a fundamental task for video analysis and autonomous navigation. Although 3D reconstruction is a key technique to obtain camera poses, monocular 3D reconstruction in a large environment tends to result in the accumulation of errors in rotation, translation, and especially in scale: a problem known as scale drift. To overcome these errors, we propose a novel framework that integrates incremental structure from motion (SfM) and a scale drift correction method utilizing geo-tagged images, such as those provided by Google Street View. Our correction method begins by obtaining sparse 6-DoF correspondences between the reconstructed 3D map coordinate system and the world coordinate system, by using geo-tagged images. Then, it corrects scale drift by applying pose graph optimization over Sim(3) constraints and bundle adjustment. Experimental evaluations on large-scale datasets show that the proposed framework not only sufficiently corrects scale drift, but also achieves accurate geo-localization in a kilometer-scale environment.',\n",
       "  'arxiv_id': 'abs/1808.08544v1',\n",
       "  'category': None},\n",
       " 'abs/1811.10343v2': {'id': None,\n",
       "  'title': 'Matchable Image Retrieval by Learning from Surface Reconstruction',\n",
       "  'authors': ['Tianwei Shen',\n",
       "   'Zixin Luo',\n",
       "   'Lei Zhou',\n",
       "   'Runze Zhang',\n",
       "   'Siyu Zhu',\n",
       "   'Tian Fang',\n",
       "   'Long Quan'],\n",
       "  'abstract': 'Convolutional Neural Networks (CNNs) have achieved superior performance on object image retrieval, while Bag-of-Words (BoW) models with handcrafted local features still dominate the retrieval of overlapping images in 3D reconstruction. In this paper, we narrow down this gap by presenting an efficient CNN-based method to retrieve images with overlaps, which we refer to as the matchable image retrieval problem. Different from previous methods that generates training data based on sparse reconstruction, we create a large-scale image database with rich 3D geometrics and exploit information from surface reconstruction to obtain fine-grained training data. We propose a batched triplet-based loss function combined with mesh re-projection to effectively learn the CNN representation. The proposed method significantly accelerates the image retrieval process in 3D reconstruction and outperforms the state-of-the-art CNN-based and BoW methods for matchable image retrieval. The code and data are available at https://github.com/hlzz/mirror.',\n",
       "  'arxiv_id': 'abs/1811.10343v2',\n",
       "  'category': None},\n",
       " 'abs/1812.03015v1': {'id': None,\n",
       "  'title': 'Real-time Indoor Scene Reconstruction with RGBD and Inertia Input',\n",
       "  'authors': ['Zunjie Zhu', 'Feng Xu'],\n",
       "  'abstract': 'Camera motion estimation is a key technique for 3D scene reconstruction and Simultaneous localization and mapping (SLAM). To make it be feasibly achieved, previous works usually assume slow camera motions, which limits its usage in many real cases. We propose an end-to-end 3D reconstruction system which combines color, depth and inertial measurements to achieve robust reconstruction with fast sensor motions. Our framework extends Kalman filter to fuse the three kinds of information and involve an iterative method to jointly optimize feature correspondences, camera poses and scene geometry. We also propose a novel geometry-aware patch deformation technique to adapt the feature appearance in image domain, leading to a more accurate feature matching under fast camera motions. Experiments show that our patch deformation method improves the accuracy of feature tracking, and our 3D reconstruction outperforms the state-of-the-art solutions under fast camera motions.',\n",
       "  'arxiv_id': 'abs/1812.03015v1',\n",
       "  'category': None},\n",
       " 'abs/1909.01205v1': {'id': None,\n",
       "  'title': 'Few-Shot Generalization for Single-Image 3D Reconstruction via Priors',\n",
       "  'authors': ['Bram Wallace', 'Bharath Hariharan'],\n",
       "  'abstract': 'Recent work on single-view 3D reconstruction shows impressive results, but has been restricted to a few fixed categories where extensive training data is available. The problem of generalizing these models to new classes with limited training data is largely open. To address this problem, we present a new model architecture that reframes single-view 3D reconstruction as learnt, category agnostic refinement of a provided, category-specific prior. The provided prior shape for a novel class can be obtained from as few as one 3D shape from this class. Our model can start reconstructing objects from the novel class using this prior without seeing any training image for this class and without any retraining. Our model outperforms category-agnostic baselines and remains competitive with more sophisticated baselines that finetune on the novel categories. Additionally, our network is capable of improving the reconstruction given multiple views despite not being trained on task of multi-view reconstruction.',\n",
       "  'arxiv_id': 'abs/1909.01205v1',\n",
       "  'category': None},\n",
       " 'abs/1704.00529v1': {'id': None,\n",
       "  'title': '3D Object Reconstruction from Hand-Object Interactions',\n",
       "  'authors': ['Dimitrios Tzionas', 'Juergen Gall'],\n",
       "  'abstract': 'Recent advances have enabled 3d object reconstruction approaches using a single off-the-shelf RGB-D camera. Although these approaches are successful for a wide range of object classes, they rely on stable and distinctive geometric or texture features. Many objects like mechanical parts, toys, household or decorative articles, however, are textureless and characterized by minimalistic shapes that are simple and symmetric. Existing in-hand scanning systems and 3d reconstruction techniques fail for such symmetric objects in the absence of highly distinctive features. In this work, we show that extracting 3d hand motion for in-hand scanning effectively facilitates the reconstruction of even featureless and highly symmetric objects and we present an approach that fuses the rich additional information of hands into a 3d reconstruction pipeline, significantly contributing to the state-of-the-art of in-hand scanning.',\n",
       "  'arxiv_id': 'abs/1704.00529v1',\n",
       "  'category': None},\n",
       " 'abs/1708.05375v1': {'id': None,\n",
       "  'title': 'Learning a Multi-View Stereo Machine',\n",
       "  'authors': ['Abhishek Kar', 'Christian Häne', 'Jitendra Malik'],\n",
       "  'abstract': 'We present a learnt system for multi-view stereopsis. In contrast to recent learning based methods for 3D reconstruction, we leverage the underlying 3D geometry of the problem through feature projection and unprojection along viewing rays. By formulating these operations in a differentiable manner, we are able to learn the system end-to-end for the task of metric 3D reconstruction. End-to-end learning allows us to jointly reason about shape priors while conforming geometric constraints, enabling reconstruction from much fewer images (even a single image) than required by classical approaches as well as completion of unseen surfaces. We thoroughly evaluate our approach on the ShapeNet dataset and demonstrate the benefits over classical approaches as well as recent learning based methods.',\n",
       "  'arxiv_id': 'abs/1708.05375v1',\n",
       "  'category': None},\n",
       " 'abs/1802.08824v1': {'id': None,\n",
       "  'title': 'The AdobeIndoorNav Dataset: Towards Deep Reinforcement Learning based\\n  Real-world Indoor Robot Visual Navigation',\n",
       "  'authors': ['Kaichun Mo', 'Haoxiang Li', 'Zhe Lin', 'Joon-Young Lee'],\n",
       "  'abstract': 'Deep reinforcement learning (DRL) demonstrates its potential in learning a model-free navigation policy for robot visual navigation. However, the data-demanding algorithm relies on a large number of navigation trajectories in training. Existing datasets supporting training such robot navigation algorithms consist of either 3D synthetic scenes or reconstructed scenes. Synthetic data suffers from domain gap to the real-world scenes while visual inputs rendered from 3D reconstructed scenes have undesired holes and artifacts. In this paper, we present a new dataset collected in real-world to facilitate the research in DRL based visual navigation. Our dataset includes 3D reconstruction for real-world scenes as well as densely captured real 2D images from the scenes. It provides high-quality visual inputs with real-world scene complexity to the robot at dense grid locations. We further study and benchmark one recent DRL based navigation algorithm and present our attempts and thoughts on improving its generalizability to unseen test targets in the scenes.',\n",
       "  'arxiv_id': 'abs/1802.08824v1',\n",
       "  'category': None},\n",
       " 'abs/1910.08338v1': {'id': None,\n",
       "  'title': 'A Gigapixel Computational Light-Field Camera',\n",
       "  'authors': ['Thomas Gregory',\n",
       "   'Matthew P. Edgar',\n",
       "   'Graham M. Gibson',\n",
       "   'Paul-Antoine Moreau'],\n",
       "  'abstract': 'Light-field cameras allow the acquisition of both the spatial and angular components of the light. This has a wide range of applications from image refocusing to 3D reconstruction of a scene. The conventional way to perform such acquisitions leads to a strong spatio-angular resolution limit. Here we propose a computational version of the light-field camera. We perform a one gigapixel photo-realistic diffraction limited light-field acquisition, that would require the use of a one gigapixel sensor were the acquisition to be performed with a conventional light-field camera. This result is mostly limited by the total acquisition time, as our system could in principle allow $\\\\sim$Terapixel reconstructions to be achieved. The reported result presents many potential advantages, such as the possibility to perform large depth of field light-field acquisitions, realistic refocusing along a very wide range of depths, very high dimensional super-resolved image acquisitions, and large depth of field 3D reconstructions.',\n",
       "  'arxiv_id': 'abs/1910.08338v1',\n",
       "  'category': None},\n",
       " 'abs/1910.13740v1': {'id': None,\n",
       "  'title': 'Probabilistic Inference for Camera Calibration in Light Microscopy under\\n  Circular Motion',\n",
       "  'authors': ['Yuanhao Guo', 'Fons J. Verbeek', 'Ge Yang'],\n",
       "  'abstract': 'Robust and accurate camera calibration is essential for 3D reconstruction in light microscopy under circular motion. Conventional methods require either accurate key point matching or precise segmentation of the axial-view images. Both remain challenging because specimens often exhibit transparency/translucency in a light microscope. To address those issues, we propose a probabilistic inference based method for the camera calibration that does not require sophisticated image pre-processing. Based on 3D projective geometry, our method assigns a probability on each of a range of voxels that cover the whole object. The probability indicates the likelihood of a voxel belonging to the object to be reconstructed. Our method maximizes a joint probability that distinguishes the object from the background. Experimental results show that the proposed method can accurately recover camera configurations in both light microscopy and natural scene imaging. Furthermore, the method can be used to produce high-fidelity 3D reconstructions and accurate 3D measurements.',\n",
       "  'arxiv_id': 'abs/1910.13740v1',\n",
       "  'category': None},\n",
       " 'abs/1911.01082v1': {'id': None,\n",
       "  'title': 'Technical Report: Co-learning of geometry and semantics for online 3D\\n  mapping',\n",
       "  'authors': ['Marcela Carvalho',\n",
       "   'Maxime Ferrera',\n",
       "   'Alexandre Boulch',\n",
       "   'Julien Moras',\n",
       "   'Bertrand Le Saux',\n",
       "   'Pauline Trouvé-Peloux'],\n",
       "  'abstract': 'This paper is a technical report about our submission for the ECCV 2018 3DRMS Workshop Challenge on Semantic 3D Reconstruction \\\\cite{Tylecek2018rms}. In this paper, we address 3D semantic reconstruction for autonomous navigation using co-learning of depth map and semantic segmentation. The core of our pipeline is a deep multi-task neural network which tightly refines depth and also produces accurate semantic segmentation maps. Its inputs are an image and a raw depth map produced from a pair of images by standard stereo vision. The resulting semantic 3D point clouds are then merged in order to create a consistent 3D mesh, in turn used to produce dense semantic 3D reconstruction maps. The performances of each step of the proposed method are evaluated on the dataset and multiple tasks of the 3DRMS Challenge, and repeatedly surpass state-of-the-art approaches.',\n",
       "  'arxiv_id': 'abs/1911.01082v1',\n",
       "  'category': None},\n",
       " 'abs/1911.09204v4': {'id': None,\n",
       "  'title': 'DR-KFS: A Differentiable Visual Similarity Metric for 3D Shape\\n  Reconstruction',\n",
       "  'authors': ['Jiongchao Jin',\n",
       "   'Akshay Gadi Patil',\n",
       "   'Zhang Xiong',\n",
       "   'Hao Zhang'],\n",
       "  'abstract': 'We introduce a differential visual similarity metric to train deep neural networks for 3D reconstruction, aimed at improving reconstruction quality. The metric compares two 3D shapes by measuring distances between multi-view images differentiably rendered from the shapes. Importantly, the image-space distance is also differentiable and measures visual similarity, rather than pixel-wise distortion. Specifically, the similarity is defined by mean-squared errors over HardNet features computed from probabilistic keypoint maps of the compared images. Our differential visual shape similarity metric can be easily plugged into various 3D reconstruction networks, replacing their distortion-based losses, such as Chamfer or Earth Mover distances, so as to optimize the network weights to produce reconstructions with better structural fidelity and visual quality. We demonstrate this both objectively, using well-known shape metrics for retrieval and classification tasks that are independent from our new metric, and subjectively through a perceptual study.',\n",
       "  'arxiv_id': 'abs/1911.09204v4',\n",
       "  'category': None},\n",
       " 'abs/1912.07109v2': {'id': None,\n",
       "  'title': 'SDFDiff: Differentiable Rendering of Signed Distance Fields for 3D Shape\\n  Optimization',\n",
       "  'authors': ['Yue Jiang', 'Dantong Ji', 'Zhizhong Han', 'Matthias Zwicker'],\n",
       "  'abstract': 'We propose SDFDiff, a novel approach for image-based shape optimization using differentiable rendering of 3D shapes represented by signed distance functions (SDFs). Compared to other representations, SDFs have the advantage that they can represent shapes with arbitrary topology, and that they guarantee watertight surfaces. We apply our approach to the problem of multi-view 3D reconstruction, where we achieve high reconstruction quality and can capture complex topology of 3D objects. In addition, we employ a multi-resolution strategy to obtain a robust optimization algorithm. We further demonstrate that our SDF-based differentiable renderer can be integrated with deep learning models, which opens up options for learning approaches on 3D objects without 3D supervision. In particular, we apply our method to single-view 3D reconstruction and achieve state-of-the-art results.',\n",
       "  'arxiv_id': 'abs/1912.07109v2',\n",
       "  'category': None},\n",
       " 'abs/1912.07372v2': {'id': None,\n",
       "  'title': 'Differentiable Volumetric Rendering: Learning Implicit 3D\\n  Representations without 3D Supervision',\n",
       "  'authors': ['Michael Niemeyer',\n",
       "   'Lars Mescheder',\n",
       "   'Michael Oechsle',\n",
       "   'Andreas Geiger'],\n",
       "  'abstract': 'Learning-based 3D reconstruction methods have shown impressive results. However, most methods require 3D supervision which is often hard to obtain for real-world datasets. Recently, several works have proposed differentiable rendering techniques to train reconstruction models from RGB images. Unfortunately, these approaches are currently restricted to voxel- and mesh-based representations, suffering from discretization or low resolution. In this work, we propose a differentiable rendering formulation for implicit shape and texture representations. Implicit representations have recently gained popularity as they represent shape and texture continuously. Our key insight is that depth gradients can be derived analytically using the concept of implicit differentiation. This allows us to learn implicit shape and texture representations directly from RGB images. We experimentally show that our single-view reconstructions rival those learned with full 3D supervision. Moreover, we find that our method can be used for multi-view 3D reconstruction, directly resulting in watertight meshes.',\n",
       "  'arxiv_id': 'abs/1912.07372v2',\n",
       "  'category': None},\n",
       " 'abs/2003.04618v2': {'id': None,\n",
       "  'title': 'Convolutional Occupancy Networks',\n",
       "  'authors': ['Songyou Peng',\n",
       "   'Michael Niemeyer',\n",
       "   'Lars Mescheder',\n",
       "   'Marc Pollefeys',\n",
       "   'Andreas Geiger'],\n",
       "  'abstract': 'Recently, implicit neural representations have gained popularity for learning-based 3D reconstruction. While demonstrating promising results, most implicit approaches are limited to comparably simple geometry of single objects and do not scale to more complicated or large-scale scenes. The key limiting factor of implicit methods is their simple fully-connected network architecture which does not allow for integrating local information in the observations or incorporating inductive biases such as translational equivariance. In this paper, we propose Convolutional Occupancy Networks, a more flexible implicit representation for detailed reconstruction of objects and 3D scenes. By combining convolutional encoders with implicit occupancy decoders, our model incorporates inductive biases, enabling structured reasoning in 3D space. We investigate the effectiveness of the proposed representation by reconstructing complex geometry from noisy point clouds and low-resolution voxel representations. We empirically find that our method enables the fine-grained implicit 3D reconstruction of single objects, scales to large indoor scenes, and generalizes well from synthetic to real data.',\n",
       "  'arxiv_id': 'abs/2003.04618v2',\n",
       "  'category': None},\n",
       " 'abs/2005.04623v1': {'id': None,\n",
       "  'title': 'A Simple and Scalable Shape Representation for 3D Reconstruction',\n",
       "  'authors': ['Mateusz Michalkiewicz',\n",
       "   'Eugene Belilovsky',\n",
       "   'Mahsa Baktashmotlagh',\n",
       "   'Anders Eriksson'],\n",
       "  'abstract': 'Deep learning applied to the reconstruction of 3D shapes has seen growing interest. A popular approach to 3D reconstruction and generation in recent years has been the CNN encoder-decoder model usually applied in voxel space. However, this often scales very poorly with the resolution limiting the effectiveness of these models. Several sophisticated alternatives for decoding to 3D shapes have been proposed typically relying on complex deep learning architectures for the decoder model. In this work, we show that this additional complexity is not necessary, and that we can actually obtain high quality 3D reconstruction using a linear decoder, obtained from principal component analysis on the signed distance function (SDF) of the surface. This approach allows easily scaling to larger resolutions. We show in multiple experiments that our approach is competitive with state-of-the-art methods. It also allows the decoder to be fine-tuned on the target task using a loss designed specifically for SDF transforms, obtaining further gains.',\n",
       "  'arxiv_id': 'abs/2005.04623v1',\n",
       "  'category': None},\n",
       " 'abs/2006.14660v2': {'id': None,\n",
       "  'title': 'SPSG: Self-Supervised Photometric Scene Generation from RGB-D Scans',\n",
       "  'authors': ['Angela Dai',\n",
       "   'Yawar Siddiqui',\n",
       "   'Justus Thies',\n",
       "   'Julien Valentin',\n",
       "   'Matthias Nießner'],\n",
       "  'abstract': 'We present SPSG, a novel approach to generate high-quality, colored 3D models of scenes from RGB-D scan observations by learning to infer unobserved scene geometry and color in a self-supervised fashion. Our self-supervised approach learns to jointly inpaint geometry and color by correlating an incomplete RGB-D scan with a more complete version of that scan. Notably, rather than relying on 3D reconstruction losses to inform our 3D geometry and color reconstruction, we propose adversarial and perceptual losses operating on 2D renderings in order to achieve high-resolution, high-quality colored reconstructions of scenes. This exploits the high-resolution, self-consistent signal from individual raw RGB-D frames, in contrast to fused 3D reconstructions of the frames which exhibit inconsistencies from view-dependent effects, such as color balancing or pose inconsistencies. Thus, by informing our 3D scene generation directly through 2D signal, we produce high-quality colored reconstructions of 3D scenes, outperforming state of the art on both synthetic and real data.',\n",
       "  'arxiv_id': 'abs/2006.14660v2',\n",
       "  'category': None},\n",
       " 'abs/2008.12709v2': {'id': None,\n",
       "  'title': 'Canonical 3D Deformer Maps: Unifying parametric and non-parametric\\n  methods for dense weakly-supervised category reconstruction',\n",
       "  'authors': ['David Novotny', 'Roman Shapovalov', 'Andrea Vedaldi'],\n",
       "  'abstract': 'We propose the Canonical 3D Deformer Map, a new representation of the 3D shape of common object categories that can be learned from a collection of 2D images of independent objects. Our method builds in a novel way on concepts from parametric deformation models, non-parametric 3D reconstruction, and canonical embeddings, combining their individual advantages. In particular, it learns to associate each image pixel with a deformation model of the corresponding 3D object point which is canonical, i.e. intrinsic to the identity of the point and shared across objects of the category. The result is a method that, given only sparse 2D supervision at training time, can, at test time, reconstruct the 3D shape and texture of objects from single views, while establishing meaningful dense correspondences between object instances. It also achieves state-of-the-art results in dense 3D reconstruction on public in-the-wild datasets of faces, cars, and birds.',\n",
       "  'arxiv_id': 'abs/2008.12709v2',\n",
       "  'category': None},\n",
       " 'abs/2009.13146v1': {'id': None,\n",
       "  'title': 'Amodal 3D Reconstruction for Robotic Manipulation via Stability and\\n  Connectivity',\n",
       "  'authors': ['William Agnew',\n",
       "   'Christopher Xie',\n",
       "   'Aaron Walsman',\n",
       "   'Octavian Murad',\n",
       "   'Caelen Wang',\n",
       "   'Pedro Domingos',\n",
       "   'Siddhartha Srinivasa'],\n",
       "  'abstract': 'Learning-based 3D object reconstruction enables single- or few-shot estimation of 3D object models. For robotics, this holds the potential to allow model-based methods to rapidly adapt to novel objects and scenes. Existing 3D reconstruction techniques optimize for visual reconstruction fidelity, typically measured by chamfer distance or voxel IOU. We find that when applied to realistic, cluttered robotics environments, these systems produce reconstructions with low physical realism, resulting in poor task performance when used for model-based control. We propose ARM, an amodal 3D reconstruction system that introduces (1) a stability prior over object shapes, (2) a connectivity prior, and (3) a multi-channel input representation that allows for reasoning over relationships between groups of objects. By using these priors over the physical properties of objects, our system improves reconstruction quality not just by standard visual metrics, but also performance of model-based control on a variety of robotics manipulation tasks in challenging, cluttered environments. Code is available at github.com/wagnew3/ARM.',\n",
       "  'arxiv_id': 'abs/2009.13146v1',\n",
       "  'category': None},\n",
       " 'abs/2011.00980v1': {'id': None,\n",
       "  'title': '3D Multi-bodies: Fitting Sets of Plausible 3D Human Models to Ambiguous\\n  Image Data',\n",
       "  'authors': ['Benjamin Biggs',\n",
       "   'Sébastien Ehrhadt',\n",
       "   'Hanbyul Joo',\n",
       "   'Benjamin Graham',\n",
       "   'Andrea Vedaldi',\n",
       "   'David Novotny'],\n",
       "  'abstract': 'We consider the problem of obtaining dense 3D reconstructions of humans from single and partially occluded views. In such cases, the visual evidence is usually insufficient to identify a 3D reconstruction uniquely, so we aim at recovering several plausible reconstructions compatible with the input data. We suggest that ambiguities can be modelled more effectively by parametrizing the possible body shapes and poses via a suitable 3D model, such as SMPL for humans. We propose to learn a multi-hypothesis neural network regressor using a best-of-M loss, where each of the M hypotheses is constrained to lie on a manifold of plausible human poses by means of a generative model. We show that our method outperforms alternative approaches in ambiguous pose recovery on standard benchmarks for 3D humans, and in heavily occluded versions of these benchmarks.',\n",
       "  'arxiv_id': 'abs/2011.00980v1',\n",
       "  'category': None},\n",
       " 'abs/2012.01743v1': {'id': None,\n",
       "  'title': '3D-NVS: A 3D Supervision Approach for Next View Selection',\n",
       "  'authors': ['Kumar Ashutosh', 'Saurabh Kumar', 'Subhasis Chaudhuri'],\n",
       "  'abstract': \"We present a classification based approach for the next best view selection and show how we can plausibly obtain a supervisory signal for this task. The proposed approach is end-to-end trainable and aims to get the best possible 3D reconstruction quality with a pair of passively acquired 2D views. The proposed model consists of two stages: a classifier and a reconstructor network trained jointly via the indirect 3D supervision from ground truth voxels. While testing, the proposed method assumes no prior knowledge of the underlying 3D shape for selecting the next best view. We demonstrate the proposed method's effectiveness via detailed experiments on synthetic and real images and show how it provides improved reconstruction quality than the existing state of the art 3D reconstruction and the next best view prediction techniques.\",\n",
       "  'arxiv_id': 'abs/2012.01743v1',\n",
       "  'category': None},\n",
       " 'abs/2012.06650v2': {'id': None,\n",
       "  'title': 'D$^2$IM-Net: Learning Detail Disentangled Implicit Fields from Single\\n  Images',\n",
       "  'authors': ['Manyi Li', 'Hao Zhang'],\n",
       "  'abstract': 'We present the first single-view 3D reconstruction network aimed at recovering geometric details from an input image which encompass both topological shape structures and surface features. Our key idea is to train the network to learn a detail disentangled reconstruction consisting of two functions, one implicit field representing the coarse 3D shape and the other capturing the details. Given an input image, our network, coined D$^2$IM-Net, encodes it into global and local features which are respectively fed into two decoders. The base decoder uses the global features to reconstruct a coarse implicit field, while the detail decoder reconstructs, from the local features, two displacement maps, defined over the front and back sides of the captured object. The final 3D reconstruction is a fusion between the base shape and the displacement maps, with three losses enforcing the recovery of coarse shape, overall structure, and surface details via a novel Laplacian term.',\n",
       "  'arxiv_id': 'abs/2012.06650v2',\n",
       "  'category': None},\n",
       " 'abs/2103.14794v1': {'id': None,\n",
       "  'title': 'Learning Efficient Photometric Feature Transform for Multi-view Stereo',\n",
       "  'authors': ['Kaizhang Kang',\n",
       "   'Cihui Xie',\n",
       "   'Ruisheng Zhu',\n",
       "   'Xiaohe Ma',\n",
       "   'Ping Tan',\n",
       "   'Hongzhi Wu',\n",
       "   'Kun Zhou'],\n",
       "  'abstract': 'We present a novel framework to learn to convert the perpixel photometric information at each view into spatially distinctive and view-invariant low-level features, which can be plugged into existing multi-view stereo pipeline for enhanced 3D reconstruction. Both the illumination conditions during acquisition and the subsequent per-pixel feature transform can be jointly optimized in a differentiable fashion. Our framework automatically adapts to and makes efficient use of the geometric information available in different forms of input data. High-quality 3D reconstructions of a variety of challenging objects are demonstrated on the data captured with an illumination multiplexing device, as well as a point light. Our results compare favorably with state-of-the-art techniques.',\n",
       "  'arxiv_id': 'abs/2103.14794v1',\n",
       "  'category': None},\n",
       " 'abs/2104.13854v1': {'id': None,\n",
       "  'title': 'D-OccNet: Detailed 3D Reconstruction Using Cross-Domain Learning',\n",
       "  'authors': ['Minhaj Uddin Ansari', 'Talha Bilal', 'Naeem Akhter'],\n",
       "  'abstract': 'Deep learning based 3D reconstruction of single view 2D image is becoming increasingly popular due to their wide range of real-world applications, but this task is inherently challenging because of the partial observability of an object from a single perspective. Recently, state of the art probability based Occupancy Networks reconstructed 3D surfaces from three different types of input domains: single view 2D image, point cloud and voxel. In this study, we extend the work on Occupancy Networks by exploiting cross-domain learning of image and point cloud domains. Specifically, we first convert the single view 2D image into a simpler point cloud representation, and then reconstruct a 3D surface from it. Our network, the Double Occupancy Network (D-OccNet) outperforms Occupancy Networks in terms of visual quality and details captured in the 3D reconstruction.',\n",
       "  'arxiv_id': 'abs/2104.13854v1',\n",
       "  'category': None},\n",
       " 'abs/2105.11352v1': {'id': None,\n",
       "  'title': 'Reconstructing Small 3D Objects in front of a Textured Background',\n",
       "  'authors': ['Petr Hruby', 'Tomas Pajdla'],\n",
       "  'abstract': \"We present a technique for a complete 3D reconstruction of small objects moving in front of a textured background. It is a particular variation of multibody structure from motion, which specializes to two objects only. The scene is captured in several static configurations between which the relative pose of the two objects may change. We reconstruct every static configuration individually and segment the points locally by finding multiple poses of cameras that capture the scene's other configurations. Then, the local segmentation results are combined, and the reconstructions are merged into the resulting model of the scene. In experiments with real artifacts, we show that our approach has practical advantages when reconstructing 3D objects from all sides. In this setting, our method outperforms the state-of-the-art. We integrate our method into the state of the art 3D reconstruction pipeline COLMAP.\",\n",
       "  'arxiv_id': 'abs/2105.11352v1',\n",
       "  'category': None},\n",
       " 'abs/2106.08851v1': {'id': None,\n",
       "  'title': 'GelSight Wedge: Measuring High-Resolution 3D Contact Geometry with a\\n  Compact Robot Finger',\n",
       "  'authors': ['Shaoxiong Wang', 'Yu She', 'Branden Romero', 'Edward Adelson'],\n",
       "  'abstract': 'Vision-based tactile sensors have the potential to provide important contact geometry to localize the objective with visual occlusion. However, it is challenging to measure high-resolution 3D contact geometry for a compact robot finger, to simultaneously meet optical and mechanical constraints. In this work, we present the GelSight Wedge sensor, which is optimized to have a compact shape for robot fingers, while achieving high-resolution 3D reconstruction. We evaluate the 3D reconstruction under different lighting configurations, and extend the method from 3 lights to 1 or 2 lights. We demonstrate the flexibility of the design by shrinking the sensor to the size of a human finger for fine manipulation tasks. We also show the effectiveness and potential of the reconstructed 3D geometry for pose tracking in the 3D space.',\n",
       "  'arxiv_id': 'abs/2106.08851v1',\n",
       "  'category': None},\n",
       " 'abs/2106.12102v2': {'id': None,\n",
       "  'title': 'LegoFormer: Transformers for Block-by-Block Multi-view 3D Reconstruction',\n",
       "  'authors': ['Farid Yagubbayli',\n",
       "   'Yida Wang',\n",
       "   'Alessio Tonioni',\n",
       "   'Federico Tombari'],\n",
       "  'abstract': 'Most modern deep learning-based multi-view 3D reconstruction techniques use RNNs or fusion modules to combine information from multiple images after independently encoding them. These two separate steps have loose connections and do not allow easy information sharing among views. We propose LegoFormer, a transformer model for voxel-based 3D reconstruction that uses the attention layers to share information among views during all computational stages. Moreover, instead of predicting each voxel independently, we propose to parametrize the output with a series of low-rank decomposition factors. This reformulation allows the prediction of an object as a set of independent regular structures then aggregated to obtain the final reconstruction. Experiments conducted on ShapeNet demonstrate the competitive performance of our model with respect to the state of the art while having increased interpretability thanks to the self-attention layers. We also show promising generalization results to real data.',\n",
       "  'arxiv_id': 'abs/2106.12102v2',\n",
       "  'category': None},\n",
       " 'abs/2109.02288v2': {'id': None,\n",
       "  'title': 'Toward Realistic Single-View 3D Object Reconstruction with Unsupervised\\n  Learning from Multiple Images',\n",
       "  'authors': ['Long-Nhat Ho', 'Anh Tuan Tran', 'Quynh Phung', 'Minh Hoai'],\n",
       "  'abstract': 'Recovering the 3D structure of an object from a single image is a challenging task due to its ill-posed nature. One approach is to utilize the plentiful photos of the same object category to learn a strong 3D shape prior for the object. This approach has successfully been demonstrated by a recent work of Wu et al. (2020), which obtained impressive 3D reconstruction networks with unsupervised learning. However, their algorithm is only applicable to symmetric objects. In this paper, we eliminate the symmetry requirement with a novel unsupervised algorithm that can learn a 3D reconstruction network from a multi-image dataset. Our algorithm is more general and covers the symmetry-required scenario as a special case. Besides, we employ a novel albedo loss that improves the reconstructed details and realisticity. Our method surpasses the previous work in both quality and robustness, as shown in experiments on datasets of various structures, including single-view, multi-view, image-collection, and video sets.',\n",
       "  'arxiv_id': 'abs/2109.02288v2',\n",
       "  'category': None},\n",
       " 'abs/2109.11798v1': {'id': None,\n",
       "  'title': 'Adversarial Domain Feature Adaptation for Bronchoscopic Depth Estimation',\n",
       "  'authors': ['Mert Asim Karaoglu',\n",
       "   'Nikolas Brasch',\n",
       "   'Marijn Stollenga',\n",
       "   'Wolfgang Wein',\n",
       "   'Nassir Navab',\n",
       "   'Federico Tombari',\n",
       "   'Alexander Ladikos'],\n",
       "  'abstract': \"Depth estimation from monocular images is an important task in localization and 3D reconstruction pipelines for bronchoscopic navigation. Various supervised and self-supervised deep learning-based approaches have proven themselves on this task for natural images. However, the lack of labeled data and the bronchial tissue's feature-scarce texture make the utilization of these methods ineffective on bronchoscopic scenes. In this work, we propose an alternative domain-adaptive approach. Our novel two-step structure first trains a depth estimation network with labeled synthetic images in a supervised manner; then adopts an unsupervised adversarial domain feature adaptation scheme to improve the performance on real images. The results of our experiments show that the proposed method improves the network's performance on real images by a considerable margin and can be employed in 3D reconstruction pipelines.\",\n",
       "  'arxiv_id': 'abs/2109.11798v1',\n",
       "  'category': None},\n",
       " 'abs/2110.02404v1': {'id': None,\n",
       "  'title': '3D-MOV: Audio-Visual LSTM Autoencoder for 3D Reconstruction of Multiple\\n  Objects from Video',\n",
       "  'authors': ['Justin Wilson', 'Ming C. Lin'],\n",
       "  'abstract': '3D object reconstructions of transparent and concave structured objects, with inferred material properties, remains an open research problem for robot navigation in unstructured environments. In this paper, we propose a multimodal single- and multi-frame neural network for 3D reconstructions using audio-visual inputs. Our trained reconstruction LSTM autoencoder 3D-MOV accepts multiple inputs to account for a variety of surface types and views. Our neural network produces high-quality 3D reconstructions using voxel representation. Based on Intersection-over-Union (IoU), we evaluate against other baseline methods using synthetic audio-visual datasets ShapeNet and Sound20K with impact sounds and bounding box annotations. To the best of our knowledge, our single- and multi-frame model is the first audio-visual reconstruction neural network for 3D geometry and material representation.',\n",
       "  'arxiv_id': 'abs/2110.02404v1',\n",
       "  'category': None},\n",
       " 'abs/2203.15065v2': {'id': None,\n",
       "  'title': 'DeepShadow: Neural Shape from Shadow',\n",
       "  'authors': ['Asaf Karnieli', 'Ohad Fried', 'Yacov Hel-Or'],\n",
       "  'abstract': 'This paper presents DeepShadow, a one-shot method for recovering the depth map and surface normals from photometric stereo shadow maps. Previous works that try to recover the surface normals from photometric stereo images treat cast shadows as a disturbance. We show that the self and cast shadows not only do not disturb 3D reconstruction, but can be used alone, as a strong learning signal, to recover the depth map and surface normals. We demonstrate that 3D reconstruction from shadows can even outperform shape-from-shading in certain cases. To the best of our knowledge, our method is the first to reconstruct 3D shape-from-shadows using neural networks. The method does not require any pre-training or expensive labeled data, and is optimized during inference time.',\n",
       "  'arxiv_id': 'abs/2203.15065v2',\n",
       "  'category': None},\n",
       " 'abs/2204.01139v1': {'id': None,\n",
       "  'title': 'BNV-Fusion: Dense 3D Reconstruction using Bi-level Neural Volume Fusion',\n",
       "  'authors': ['Kejie Li',\n",
       "   'Yansong Tang',\n",
       "   'Victor Adrian Prisacariu',\n",
       "   'Philip H. S. Torr'],\n",
       "  'abstract': 'Dense 3D reconstruction from a stream of depth images is the key to many mixed reality and robotic applications. Although methods based on Truncated Signed Distance Function (TSDF) Fusion have advanced the field over the years, the TSDF volume representation is confronted with striking a balance between the robustness to noisy measurements and maintaining the level of detail. We present Bi-level Neural Volume Fusion (BNV-Fusion), which leverages recent advances in neural implicit representations and neural rendering for dense 3D reconstruction. In order to incrementally integrate new depth maps into a global neural implicit representation, we propose a novel bi-level fusion strategy that considers both efficiency and reconstruction quality by design. We evaluate the proposed method on multiple datasets quantitatively and qualitatively, demonstrating a significant improvement over existing methods.',\n",
       "  'arxiv_id': 'abs/2204.01139v1',\n",
       "  'category': None},\n",
       " 'abs/2204.03642v1': {'id': None,\n",
       "  'title': 'Pre-train, Self-train, Distill: A simple recipe for Supersizing 3D\\n  Reconstruction',\n",
       "  'authors': ['Kalyan Vasudev Alwala', 'Abhinav Gupta', 'Shubham Tulsiani'],\n",
       "  'abstract': 'Our work learns a unified model for single-view 3D reconstruction of objects from hundreds of semantic categories. As a scalable alternative to direct 3D supervision, our work relies on segmented image collections for learning 3D of generic categories. Unlike prior works that use similar supervision but learn independent category-specific models from scratch, our approach of learning a unified model simplifies the training process while also allowing the model to benefit from the common structure across categories. Using image collections from standard recognition datasets, we show that our approach allows learning 3D inference for over 150 object categories. We evaluate using two datasets and qualitatively and quantitatively show that our unified reconstruction approach improves over prior category-specific reconstruction baselines. Our final 3D reconstruction model is also capable of zero-shot inference on images from unseen object categories and we empirically show that increasing the number of training categories improves the reconstruction quality.',\n",
       "  'arxiv_id': 'abs/2204.03642v1',\n",
       "  'category': None},\n",
       " 'abs/2207.10494v2': {'id': None,\n",
       "  'title': 'Multi-Event-Camera Depth Estimation and Outlier Rejection by Refocused\\n  Events Fusion',\n",
       "  'authors': ['Suman Ghosh', 'Guillermo Gallego'],\n",
       "  'abstract': \"Event cameras are bio-inspired sensors that offer advantages over traditional cameras. They operate asynchronously, sampling the scene at microsecond resolution and producing a stream of brightness changes. This unconventional output has sparked novel computer vision methods to unlock the camera's potential. Here, the problem of event-based stereo 3D reconstruction for SLAM is considered. Most event-based stereo methods attempt to exploit the high temporal resolution of the camera and the simultaneity of events across cameras to establish matches and estimate depth. By contrast, this work investigates how to estimate depth without explicit data association by fusing Disparity Space Images (DSIs) originated in efficient monocular methods. Fusion theory is developed and applied to design multi-camera 3D reconstruction algorithms that produce state-of-the-art results, as confirmed by comparisons with four baseline methods and tests on a variety of available datasets.\",\n",
       "  'arxiv_id': 'abs/2207.10494v2',\n",
       "  'category': None},\n",
       " 'abs/2209.13433v1': {'id': None,\n",
       "  'title': 'OmniNeRF: Hybriding Omnidirectional Distance and Radiance fields for\\n  Neural Surface Reconstruction',\n",
       "  'authors': ['Jiaming Shen', 'Bolin Song', 'Zirui Wu', 'Yi Xu'],\n",
       "  'abstract': '3D reconstruction from images has wide applications in Virtual Reality and Automatic Driving, where the precision requirement is very high. Ground-breaking research in the neural radiance field (NeRF) by utilizing Multi-Layer Perceptions has dramatically improved the representation quality of 3D objects. Some later studies improved NeRF by building truncated signed distance fields (TSDFs) but still suffer from the problem of blurred surfaces in 3D reconstruction. In this work, this surface ambiguity is addressed by proposing a novel way of 3D shape representation, OmniNeRF. It is based on training a hybrid implicit field of Omni-directional Distance Field (ODF) and neural radiance field, replacing the apparent density in NeRF with omnidirectional information. Moreover, we introduce additional supervision on the depth map to further improve reconstruction quality. The proposed method has been proven to effectively deal with NeRF defects at the edges of the surface reconstruction, providing higher quality 3D scene reconstruction results.',\n",
       "  'arxiv_id': 'abs/2209.13433v1',\n",
       "  'category': None},\n",
       " 'abs/2210.02299v1': {'id': None,\n",
       "  'title': 'SHINE-Mapping: Large-Scale 3D Mapping Using Sparse Hierarchical Implicit\\n  Neural Representations',\n",
       "  'authors': ['Xingguang Zhong', 'Yue Pan', 'Jens Behley', 'Cyrill Stachniss'],\n",
       "  'abstract': 'Accurate mapping of large-scale environments is an essential building block of most outdoor autonomous systems. Challenges of traditional mapping methods include the balance between memory consumption and mapping accuracy. This paper addresses the problems of achieving large-scale 3D reconstructions with implicit representations using 3D LiDAR measurements. We learn and store implicit features through an octree-based hierarchical structure, which is sparse and extensible. The features can be turned into signed distance values through a shallow neural network. We leverage binary cross entropy loss to optimize the local features with the 3D measurements as supervision. Based on our implicit representation, we design an incremental mapping system with regularization to tackle the issue of catastrophic forgetting in continual learning. Our experiments show that our 3D reconstructions are more accurate, complete, and memory-efficient than current state-of-the-art 3D mapping methods.',\n",
       "  'arxiv_id': 'abs/2210.02299v1',\n",
       "  'category': None},\n",
       " 'abs/2211.16991v1': {'id': None,\n",
       "  'title': 'SparsePose: Sparse-View Camera Pose Regression and Refinement',\n",
       "  'authors': ['Samarth Sinha',\n",
       "   'Jason Y. Zhang',\n",
       "   'Andrea Tagliasacchi',\n",
       "   'Igor Gilitschenski',\n",
       "   'David B. Lindell'],\n",
       "  'abstract': 'Camera pose estimation is a key step in standard 3D reconstruction pipelines that operate on a dense set of images of a single object or scene. However, methods for pose estimation often fail when only a few images are available because they rely on the ability to robustly identify and match visual features between image pairs. While these methods can work robustly with dense camera views, capturing a large set of images can be time-consuming or impractical. We propose SparsePose for recovering accurate camera poses given a sparse set of wide-baseline images (fewer than 10). The method learns to regress initial camera poses and then iteratively refine them after training on a large-scale dataset of objects (Co3D: Common Objects in 3D). SparsePose significantly outperforms conventional and learning-based baselines in recovering accurate camera rotations and translations. We also demonstrate our pipeline for high-fidelity 3D reconstruction using only 5-9 images of an object.',\n",
       "  'arxiv_id': 'abs/2211.16991v1',\n",
       "  'category': None},\n",
       " 'abs/2212.04386v1': {'id': None,\n",
       "  'title': 'Multi-View Mesh Reconstruction with Neural Deferred Shading',\n",
       "  'authors': ['Markus Worchel',\n",
       "   'Rodrigo Diaz',\n",
       "   'Weiwen Hu',\n",
       "   'Oliver Schreer',\n",
       "   'Ingo Feldmann',\n",
       "   'Peter Eisert'],\n",
       "  'abstract': 'We propose an analysis-by-synthesis method for fast multi-view 3D reconstruction of opaque objects with arbitrary materials and illumination. State-of-the-art methods use both neural surface representations and neural rendering. While flexible, neural surface representations are a significant bottleneck in optimization runtime. Instead, we represent surfaces as triangle meshes and build a differentiable rendering pipeline around triangle rasterization and neural shading. The renderer is used in a gradient descent optimization where both a triangle mesh and a neural shader are jointly optimized to reproduce the multi-view images. We evaluate our method on a public 3D reconstruction dataset and show that it can match the reconstruction accuracy of traditional baselines and neural approaches while surpassing them in optimization runtime. Additionally, we investigate the shader and find that it learns an interpretable representation of appearance, enabling applications such as 3D material editing.',\n",
       "  'arxiv_id': 'abs/2212.04386v1',\n",
       "  'category': None},\n",
       " 'abs/cs/0201019v1': {'id': None,\n",
       "  'title': 'Structure from Motion: Theoretical Foundations of a Novel Approach Using\\n  Custom Built Invariants',\n",
       "  'authors': ['Pierre-Louis Bazin', 'Mireille Boutin'],\n",
       "  'abstract': 'We rephrase the problem of 3D reconstruction from images in terms of intersections of projections of orbits of custom built Lie groups actions. We then use an algorithmic method based on moving frames \"a la Fels-Olver\" to obtain a fundamental set of invariants of these groups actions. The invariants are used to define a set of equations to be solved by the points of the 3D object, providing a new technique for recovering 3D structure from motion.',\n",
       "  'arxiv_id': 'abs/cs/0201019v1',\n",
       "  'category': None},\n",
       " 'abs/0812.5089v1': {'id': None,\n",
       "  'title': 'Interlocking of convex polyhedra: towards a geometric theory of\\n  fragmented solids',\n",
       "  'authors': ['A. J. Kanel-Belov',\n",
       "   'A. V. Dyskin',\n",
       "   'Y. Estrin',\n",
       "   'E. Pasternak',\n",
       "   'I. A. Ivanov-Pogodaev'],\n",
       "  'abstract': 'We present structures comprised of identical convex polyhedra which are interlocked geometrically. These sets cannot be disassembled by removing individual polyhedra by translations and/or rotations. The shapes that permit interlocking arrangements include all five platonic solids. Criteria for interlocking based on transformations of the cross-sections of the elements in a 3D reconstruction of a layer from its middle cross-section are formulated. A generalization to higher dimensions is also given. In particular, an interlocking layer of four-dimensional cubes is described.',\n",
       "  'arxiv_id': 'abs/0812.5089v1',\n",
       "  'category': None},\n",
       " 'abs/1006.1335v1': {'id': None,\n",
       "  'title': 'Data acquisition electronics and reconstruction software for directional\\n  detection of Dark Matter with MIMAC',\n",
       "  'authors': ['O. Bourrion',\n",
       "   'G. Bosson',\n",
       "   'C. Grignon',\n",
       "   'J. L. Bouly',\n",
       "   'J. P. Richer',\n",
       "   'O. Guillaudin',\n",
       "   'F. Mayet',\n",
       "   'D. Santos'],\n",
       "  'abstract': 'Directional detection of galactic Dark Matter requires 3D reconstruction of low energy nuclear recoils tracks. A dedicated acquisition electronics with auto triggering feature and a real time track reconstruction software have been developed within the framework of the MIMAC project of detector. This auto-triggered acquisition electronic uses embedded processing to reduce data transfer to its useful part only, i.e. decoded coordinates of hit tracks and corresponding energy measurements. An acquisition software with on-line monitoring and 3D track reconstruction is also presented.',\n",
       "  'arxiv_id': 'abs/1006.1335v1',\n",
       "  'category': None},\n",
       " 'abs/1705.05548v2': {'id': None,\n",
       "  'title': 'Intel RealSense Stereoscopic Depth Cameras',\n",
       "  'authors': ['Leonid Keselman',\n",
       "   'John Iselin Woodfill',\n",
       "   'Anders Grunnet-Jepsen',\n",
       "   'Achintya Bhowmik'],\n",
       "  'abstract': \"We present a comprehensive overview of the stereoscopic Intel RealSense RGBD imaging systems. We discuss these systems' mode-of-operation, functional behavior and include models of their expected performance, shortcomings, and limitations. We provide information about the systems' optical characteristics, their correlation algorithms, and how these properties can affect different applications, including 3D reconstruction and gesture recognition. Our discussion covers the Intel RealSense R200 and the Intel RealSense D400 (formally RS400).\",\n",
       "  'arxiv_id': 'abs/1705.05548v2',\n",
       "  'category': None},\n",
       " 'abs/2009.04301v1': {'id': None,\n",
       "  'title': 'High-resolution three-dimensional crystalline microscopy',\n",
       "  'authors': ['Marc Allain', 'Virginie Chamard', 'Stephan O. Hruszkewycz'],\n",
       "  'abstract': 'In this communication, we discuss how 3D information about the structure of a crystalline sample is encoded in Bragg 3DXCDI measurements. Our analysis brings to light the role of the experimental parameters in the quality of the final reconstruction. One of our salient conclusions is that these parameters can be set prior to the ptychographic 3DXCDI experiment and that the spatial resolution limit of the 3D reconstruction can be evaluated accordingly.',\n",
       "  'arxiv_id': 'abs/2009.04301v1',\n",
       "  'category': None},\n",
       " 'abs/2207.03041v1': {'id': None,\n",
       "  'title': 'Vision Transformers: State of the Art and Research Challenges',\n",
       "  'authors': ['Bo-Kai Ruan', 'Hong-Han Shuai', 'Wen-Huang Cheng'],\n",
       "  'abstract': 'Transformers have achieved great success in natural language processing. Due to the powerful capability of self-attention mechanism in transformers, researchers develop the vision transformers for a variety of computer vision tasks, such as image recognition, object detection, image segmentation, pose estimation, and 3D reconstruction. This paper presents a comprehensive overview of the literature on different architecture designs and training tricks (including self-supervised learning) for vision transformers. Our goal is to provide a systematic review with the open research opportunities.',\n",
       "  'arxiv_id': 'abs/2207.03041v1',\n",
       "  'category': None},\n",
       " 'abs/1503.06465v2': {'id': None,\n",
       "  'title': 'Lifting Object Detection Datasets into 3D',\n",
       "  'authors': ['Joao Carreira',\n",
       "   'Sara Vicente',\n",
       "   'Lourdes Agapito',\n",
       "   'Jorge Batista'],\n",
       "  'abstract': \"While data has certainly taken the center stage in computer vision in recent years, it can still be difficult to obtain in certain scenarios. In particular, acquiring ground truth 3D shapes of objects pictured in 2D images remains a challenging feat and this has hampered progress in recognition-based object reconstruction from a single image. Here we propose to bypass previous solutions such as 3D scanning or manual design, that scale poorly, and instead populate object category detection datasets semi-automatically with dense, per-object 3D reconstructions, bootstrapped from:(i) class labels, (ii) ground truth figure-ground segmentations and (iii) a small set of keypoint annotations. Our proposed algorithm first estimates camera viewpoint using rigid structure-from-motion and then reconstructs object shapes by optimizing over visual hull proposals guided by loose within-class shape similarity assumptions. The visual hull sampling process attempts to intersect an object's projection cone with the cones of minimal subsets of other similar objects among those pictured from certain vantage points. We show that our method is able to produce convincing per-object 3D reconstructions and to accurately estimate cameras viewpoints on one of the most challenging existing object-category detection datasets, PASCAL VOC. We hope that our results will re-stimulate interest on joint object recognition and 3D reconstruction from a single image.\",\n",
       "  'arxiv_id': 'abs/1503.06465v2',\n",
       "  'category': None},\n",
       " 'abs/1712.07122v1': {'id': None,\n",
       "  'title': 'Real-time 3D Reconstruction on Construction Site using Visual SLAM and\\n  UAV',\n",
       "  'authors': ['Zhexiong Shang', 'Zhigang Shen'],\n",
       "  'abstract': '3D reconstruction can be used as a platform to monitor the performance of activities on construction site, such as construction progress monitoring, structure inspection and post-disaster rescue. Comparing to other sensors, RGB image has the advantages of low-cost, texture rich and easy to implement that has been used as the primary method for 3D reconstruction in construction industry. However, the image-based 3D reconstruction always requires extended time to acquire and/or to process the image data, which limits its application on time critical projects. Recent progress in Visual Simultaneous Localization and Mapping (SLAM) make it possible to reconstruct a 3D map of construction site in real-time. Integrated with Unmanned Aerial Vehicle (UAV), the obstacles areas that are inaccessible for the ground equipment can also be sensed. Despite these advantages of visual SLAM and UAV, until now, such technique has not been fully investigated on construction site. Therefore, the objective of this research is to present a pilot study of using visual SLAM and UAV for real-time construction site reconstruction. The system architecture and the experimental setup are introduced, and the preliminary results and the potential applications using Visual SLAM and UAV on construction site are discussed.',\n",
       "  'arxiv_id': 'abs/1712.07122v1',\n",
       "  'category': None},\n",
       " 'abs/1807.07796v2': {'id': None,\n",
       "  'title': '3D-LMNet: Latent Embedding Matching for Accurate and Diverse 3D Point\\n  Cloud Reconstruction from a Single Image',\n",
       "  'authors': ['Priyanka Mandikal',\n",
       "   'K L Navaneet',\n",
       "   'Mayank Agarwal',\n",
       "   'R. Venkatesh Babu'],\n",
       "  'abstract': '3D reconstruction from single view images is an ill-posed problem. Inferring the hidden regions from self-occluded images is both challenging and ambiguous. We propose a two-pronged approach to address these issues. To better incorporate the data prior and generate meaningful reconstructions, we propose 3D-LMNet, a latent embedding matching approach for 3D reconstruction. We first train a 3D point cloud auto-encoder and then learn a mapping from the 2D image to the corresponding learnt embedding. To tackle the issue of uncertainty in the reconstruction, we predict multiple reconstructions that are consistent with the input view. This is achieved by learning a probablistic latent space with a novel view-specific diversity loss. Thorough quantitative and qualitative analysis is performed to highlight the significance of the proposed approach. We outperform state-of-the-art approaches on the task of single-view 3D reconstruction on both real and synthetic datasets while generating multiple plausible reconstructions, demonstrating the generalizability and utility of our approach.',\n",
       "  'arxiv_id': 'abs/1807.07796v2',\n",
       "  'category': None},\n",
       " 'abs/1810.05863v1': {'id': None,\n",
       "  'title': 'Equivalent Constraints for Two-View Geometry: Pose Solution/Pure\\n  Rotation Identification and 3D Reconstruction',\n",
       "  'authors': ['Qi Cai', 'Yuanxin Wu', 'Lilian Zhang', 'Peike Zhang'],\n",
       "  'abstract': 'Two-view relative pose estimation and structure reconstruction is a classical problem in computer vision. The typical methods usually employ the singular value decomposition of the essential matrix to get multiple solutions of the relative pose, from which the right solution is picked out by reconstructing the three-dimension (3D) feature points and imposing the constraint of positive depth. This paper revisits the two-view geometry problem and discovers that the two-view imaging geometry is equivalently governed by a Pair of new Pose-Only (PPO) constraints: the same-side constraint and the intersection constraint. From the perspective of solving equation, the complete pose solutions of the essential matrix are explicitly derived and we rigorously prove that the orientation part of the pose can still be recovered in the case of pure rotation. The PPO constraints are simplified and formulated in the form of inequalities to directly identify the right pose solution with no need of 3D reconstruction and the 3D reconstruction can be analytically achieved from the identified right pose. Furthermore, the intersection inequality also enables a robust criterion for pure rotation identification. Experiment results validate the correctness of analyses and the robustness of the derived pose solution/pure rotation identification and analytical 3D reconstruction.',\n",
       "  'arxiv_id': 'abs/1810.05863v1',\n",
       "  'category': None},\n",
       " 'abs/1906.10288v2': {'id': None,\n",
       "  'title': '3DBGrowth: volumetric vertebrae segmentation and reconstruction in\\n  magnetic resonance imaging',\n",
       "  'authors': ['Jonathan S. Ramos',\n",
       "   'Mirela T. Cazzolato',\n",
       "   'Bruno S. Faiçal',\n",
       "   'Marcello H. Nogueira-Barbosa',\n",
       "   'Caetano Traina Jr.',\n",
       "   'Agma J. M. Traina'],\n",
       "  'abstract': 'Segmentation of medical images is critical for making several processes of analysis and classification more reliable. With the growing number of people presenting back pain and related problems, the semi-automatic segmentation and 3D reconstruction of vertebral bodies became even more important to support decision making. A 3D reconstruction allows a fast and objective analysis of each vertebrae condition, which may play a major role in surgical planning and evaluation of suitable treatments. In this paper, we propose 3DBGrowth, which develops a 3D reconstruction over the efficient Balanced Growth method for 2D images. We also take advantage of the slope coefficient from the annotation time to reduce the total number of annotated slices, reducing the time spent on manual annotation. We show experimental results on a representative dataset with 17 MRI exams demonstrating that our approach significantly outperforms the competitors and, on average, only 37% of the total slices with vertebral body content must be annotated without losing performance/accuracy. Compared to the state-of-the-art methods, we have achieved a Dice Score gain of over 5% with comparable processing time. Moreover, 3DBGrowth works well with imprecise seed points, which reduces the time spent on manual annotation by the specialist.',\n",
       "  'arxiv_id': 'abs/1906.10288v2',\n",
       "  'category': None},\n",
       " 'abs/1907.03387v2': {'id': None,\n",
       "  'title': 'Learning Structural Graph Layouts and 3D Shapes for Long Span Bridges 3D\\n  Reconstruction',\n",
       "  'authors': ['Fangqiao Hu', 'Jin Zhao', 'Yong Huang', 'Hui Li'],\n",
       "  'abstract': 'A learning-based 3D reconstruction method for long-span bridges is proposed in this paper. 3D reconstruction generates a 3D computer model of a real object or scene from images, it involves many stages and open problems. Existing point-based methods focus on generating 3D point clouds and their reconstructed polygonal mesh or fitting-based geometrical models in urban scenes civil structures reconstruction within Manhattan world constrains and have made great achievements. Difficulties arise when an attempt is made to transfer these systems to structures with complex topology and part relations like steel trusses and long-span bridges, this could be attributed to point clouds are often unevenly distributed with noise and suffer from occlusions and incompletion, recovering a satisfactory 3D model from these highly unstructured point clouds in a bottom-up pattern while preserving the geometrical and topological properties makes enormous challenge to existing algorithms. Considering the prior human knowledge that these structures are in conformity to regular spatial layouts in terms of components, a learning-based topology-aware 3D reconstruction method which can obtain high-level structural graph layouts and low-level 3D shapes from images is proposed in this paper. We demonstrate the feasibility of this method by testing on two real long-span steel truss cable-stayed bridges.',\n",
       "  'arxiv_id': 'abs/1907.03387v2',\n",
       "  'category': None},\n",
       " 'abs/1912.03005v1': {'id': None,\n",
       "  'title': 'Perspective-consistent multifocus multiview 3D reconstruction of small\\n  objects',\n",
       "  'authors': ['Hengjia Li', 'Chuong Nguyen'],\n",
       "  'abstract': \"Image-based 3D reconstruction or 3D photogrammetry of small-scale objects including insects and biological specimens is challenging due to the use of high magnification lens with inherent limited depth of field, and the object's fine structures and complex surface properties. Due to these challenges, traditional 3D reconstruction techniques cannot be applied without suitable image pre-processings. One such preprocessing technique is multifocus stacking that combines a set of partially focused images captured from the same viewing angle to create a single in-focus image. Traditional multifocus image capture uses a camera on a macro rail. Furthermore, the scale and shift are not properly considered by multifocus stacking techniques. As a consequence, the resulting in-focus images contain artifacts that violate perspective image formation. A 3D reconstruction using such images will fail to produce an accurate 3D model of the object. This paper shows how this problem can be solved effectively by a new multifocus stacking procedure which includes a new Fixed-Lens Multifocus Capture and camera calibration for image scale and shift. Initial experimental results are presented to confirm our expectation and show that the camera poses of fixed-lens images are at least 3-times less noisy than those of conventional moving lens images.\",\n",
       "  'arxiv_id': 'abs/1912.03005v1',\n",
       "  'category': None},\n",
       " 'abs/2007.01562v1': {'id': None,\n",
       "  'title': 'An Edge Computing-based Photo Crowdsourcing Framework for Real-time 3D\\n  Reconstruction',\n",
       "  'authors': ['Shuai Yu', 'Xu Chen', 'Shuai Wang', 'Lingjun Pu', 'Di Wu'],\n",
       "  'abstract': 'Image-based three-dimensional (3D) reconstruction utilizes a set of photos to build 3D model and can be widely used in many emerging applications such as augmented reality (AR) and disaster recovery. Most of existing 3D reconstruction methods require a mobile user to walk around the target area and reconstruct objectives with a hand-held camera, which is inefficient and time-consuming. To meet the requirements of delay intensive and resource hungry applications in 5G, we propose an edge computing-based photo crowdsourcing (EC-PCS) framework in this paper. The main objective is to collect a set of representative photos from ubiquitous mobile and Internet of Things (IoT) devices at the network edge for real-time 3D model reconstruction, with network resource and monetary cost considerations. Specifically, we first propose a photo pricing mechanism by jointly considering their freshness, resolution and data size. Then, we design a novel photo selection scheme to dynamically select a set of photos with the required target coverage and the minimum monetary cost. We prove the NP-hardness of such problem, and develop an efficient greedy-based approximation algorithm to obtain a near-optimal solution. Moreover, an optimal network resource allocation scheme is presented, in order to minimize the maximum uploading delay of the selected photos to the edge server. Finally, a 3D reconstruction algorithm and a 3D model caching scheme are performed by the edge server in real time. Extensive experimental results based on real-world datasets demonstrate the superior performance of our EC-PCS system over the existing mechanisms.',\n",
       "  'arxiv_id': 'abs/2007.01562v1',\n",
       "  'category': None},\n",
       " 'abs/2009.05596v1': {'id': None,\n",
       "  'title': '3D Reconstruction and Segmentation of Dissection Photographs for\\n  MRI-free Neuropathology',\n",
       "  'authors': ['Henry Tregidgo',\n",
       "   'Adria Casamitjana',\n",
       "   'Caitlin Latimer',\n",
       "   'Mitchell Kilgore',\n",
       "   'Eleanor Robinson',\n",
       "   'Emily Blackburn',\n",
       "   'Koen Van Leemput',\n",
       "   'Bruce Fischl',\n",
       "   'Adrian Dalca',\n",
       "   'Christine Mac Donald',\n",
       "   'Dirk Keene',\n",
       "   'Juan Eugenio Iglesias'],\n",
       "  'abstract': 'Neuroimaging to neuropathology correlation (NTNC) promises to enable the transfer of microscopic signatures of pathology to in vivo imaging with MRI, ultimately enhancing clinical care. NTNC traditionally requires a volumetric MRI scan, acquired either ex vivo or a short time prior to death. Unfortunately, ex vivo MRI is difficult and costly, and recent premortem scans of sufficient quality are seldom available. To bridge this gap, we present methodology to 3D reconstruct and segment full brain image volumes from brain dissection photographs, which are routinely acquired at many brain banks and neuropathology departments. The 3D reconstruction is achieved via a joint registration framework, which uses a reference volume other than MRI. This volume may represent either the sample at hand (e.g., a surface 3D scan) or the general population (a probabilistic atlas). In addition, we present a Bayesian method to segment the 3D reconstructed photographic volumes into 36 neuroanatomical structures, which is robust to nonuniform brightness within and across photographs. We evaluate our methods on a dataset with 24 brains, using Dice scores and volume correlations. The results show that dissection photography is a valid replacement for ex vivo MRI in many volumetric analyses, opening an avenue for MRI-free NTNC, including retrospective data. The code is available at https://github.com/htregidgo/DissectionPhotoVolumes.',\n",
       "  'arxiv_id': 'abs/2009.05596v1',\n",
       "  'category': None},\n",
       " 'abs/1706.03762v5': {'id': None,\n",
       "  'title': 'Attention Is All You Need',\n",
       "  'authors': ['Ashish Vaswani',\n",
       "   'Noam Shazeer',\n",
       "   'Niki Parmar',\n",
       "   'Jakob Uszkoreit',\n",
       "   'Llion Jones',\n",
       "   'Aidan N. Gomez',\n",
       "   'Lukasz Kaiser',\n",
       "   'Illia Polosukhin'],\n",
       "  'abstract': 'The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.',\n",
       "  'arxiv_id': 'abs/1706.03762v5',\n",
       "  'category': None},\n",
       " 'abs/2104.04692v3': {'id': None,\n",
       "  'title': 'Not All Attention Is All You Need',\n",
       "  'authors': ['Hongqiu Wu', 'Hai Zhao', 'Min Zhang'],\n",
       "  'abstract': 'Beyond the success story of pre-trained language models (PrLMs) in recent natural language processing, they are susceptible to over-fitting due to unusual large model size. To this end, dropout serves as a therapy. However, existing methods like random-based, knowledge-based and search-based dropout are more general but less effective onto self-attention based models, which are broadly chosen as the fundamental architecture of PrLMs. In this paper, we propose a novel dropout method named AttendOut to let self-attention empowered PrLMs capable of more robust task-specific tuning. We demonstrate that state-of-the-art models with elaborate training design may achieve much stronger results. We verify the universality of our approach on extensive natural language processing tasks.',\n",
       "  'arxiv_id': 'abs/2104.04692v3',\n",
       "  'category': None},\n",
       " 'abs/1910.14537v3': {'id': None,\n",
       "  'title': 'Attention Is All You Need for Chinese Word Segmentation',\n",
       "  'authors': ['Sufeng Duan', 'Hai Zhao'],\n",
       "  'abstract': 'Taking greedy decoding algorithm as it should be, this work focuses on further strengthening the model itself for Chinese word segmentation (CWS), which results in an even more fast and more accurate CWS model. Our model consists of an attention only stacked encoder and a light enough decoder for the greedy segmentation plus two highway connections for smoother training, in which the encoder is composed of a newly proposed Transformer variant, Gaussian-masked Directional (GD) Transformer, and a biaffine attention scorer. With the effective encoder design, our model only needs to take unigram features for scoring. Our model is evaluated on SIGHAN Bakeoff benchmark datasets. The experimental results show that with the highest segmentation speed, the proposed model achieves new state-of-the-art or comparable performance against strong baselines in terms of strict closed test setting.',\n",
       "  'arxiv_id': 'abs/1910.14537v3',\n",
       "  'category': None},\n",
       " 'abs/2010.13154v2': {'id': None,\n",
       "  'title': 'Attention is All You Need in Speech Separation',\n",
       "  'authors': ['Cem Subakan',\n",
       "   'Mirco Ravanelli',\n",
       "   'Samuele Cornell',\n",
       "   'Mirko Bronzi',\n",
       "   'Jianyuan Zhong'],\n",
       "  'abstract': 'Recurrent Neural Networks (RNNs) have long been the dominant architecture in sequence-to-sequence learning. RNNs, however, are inherently sequential models that do not allow parallelization of their computations. Transformers are emerging as a natural alternative to standard RNNs, replacing recurrent computations with a multi-head attention mechanism. In this paper, we propose the SepFormer, a novel RNN-free Transformer-based neural network for speech separation. The SepFormer learns short and long-term dependencies with a multi-scale approach that employs transformers. The proposed model achieves state-of-the-art (SOTA) performance on the standard WSJ0-2/3mix datasets. It reaches an SI-SNRi of 22.3 dB on WSJ0-2mix and an SI-SNRi of 19.5 dB on WSJ0-3mix. The SepFormer inherits the parallelization advantages of Transformers and achieves a competitive performance even when downsampling the encoded representation by a factor of 8. It is thus significantly faster and it is less memory-demanding than the latest speech separation systems with comparable performance.',\n",
       "  'arxiv_id': 'abs/2010.13154v2',\n",
       "  'category': None},\n",
       " 'abs/1906.02792v1': {'id': None,\n",
       "  'title': 'Attention is all you need for Videos: Self-attention based Video\\n  Summarization using Universal Transformers',\n",
       "  'authors': ['Manjot Bilkhu', 'Siyang Wang', 'Tushar Dobhal'],\n",
       "  'abstract': 'Video Captioning and Summarization have become very popular in the recent years due to advancements in Sequence Modelling, with the resurgence of Long-Short Term Memory networks (LSTMs) and introduction of Gated Recurrent Units (GRUs). Existing architectures extract spatio-temporal features using CNNs and utilize either GRUs or LSTMs to model dependencies with soft attention layers. These attention layers do help in attending to the most prominent features and improve upon the recurrent units, however, these models suffer from the inherent drawbacks of the recurrent units themselves. The introduction of the Transformer model has driven the Sequence Modelling field into a new direction. In this project, we implement a Transformer-based model for Video captioning, utilizing 3D CNN architectures like C3D and Two-stream I3D for video extraction. We also apply certain dimensionality reduction techniques so as to keep the overall size of the model within limits. We finally present our results on the MSVD and ActivityNet datasets for Single and Dense video captioning tasks respectively.',\n",
       "  'arxiv_id': 'abs/1906.02792v1',\n",
       "  'category': None},\n",
       " 'abs/2104.08771v2': {'id': None,\n",
       "  'title': 'Cross-Attention is All You Need: Adapting Pretrained Transformers for\\n  Machine Translation',\n",
       "  'authors': ['Mozhdeh Gheini', 'Xiang Ren', 'Jonathan May'],\n",
       "  'abstract': 'We study the power of cross-attention in the Transformer architecture within the context of transfer learning for machine translation, and extend the findings of studies into cross-attention when training from scratch. We conduct a series of experiments through fine-tuning a translation model on data where either the source or target language has changed. These experiments reveal that fine-tuning only the cross-attention parameters is nearly as effective as fine-tuning all parameters (i.e., the entire translation model). We provide insights into why this is the case and observe that limiting fine-tuning in this manner yields cross-lingually aligned embeddings. The implications of this finding for researchers and practitioners include a mitigation of catastrophic forgetting, the potential for zero-shot translation, and the ability to extend machine translation models to several new language pairs with reduced parameter storage overhead.',\n",
       "  'arxiv_id': 'abs/2104.08771v2',\n",
       "  'category': None},\n",
       " 'abs/2207.04213v1': {'id': None,\n",
       "  'title': 'Dual-path Attention is All You Need for Audio-Visual Speech Extraction',\n",
       "  'authors': ['Zhongweiyang Xu', 'Xulin Fan', 'Mark Hasegawa-Johnson'],\n",
       "  'abstract': \"Audio-visual target speech extraction, which aims to extract a certain speaker's speech from the noisy mixture by looking at lip movements, has made significant progress combining time-domain speech separation models and visual feature extractors (CNN). One problem of fusing audio and video information is that they have different time resolutions. Most current research upsamples the visual features along the time dimension so that audio and video features are able to align in time. However, we believe that lip movement should mostly contain long-term, or phone-level information. Based on this assumption, we propose a new way to fuse audio-visual features. We observe that for DPRNN \\\\cite{dprnn}, the interchunk dimension's time resolution could be very close to the time resolution of video frames. Like \\\\cite{sepformer}, the LSTM in DPRNN is replaced by intra-chunk and inter-chunk self-attention, but in the proposed algorithm, inter-chunk attention incorporates the visual features as an additional feature stream. This prevents the upsampling of visual cues, resulting in more efficient audio-visual fusion. The result shows we achieve superior results compared with other time-domain based audio-visual fusion models.\",\n",
       "  'arxiv_id': 'abs/2207.04213v1',\n",
       "  'category': None},\n",
       " 'abs/2211.04346v1': {'id': None,\n",
       "  'title': 'Cross-Attention is all you need: Real-Time Streaming Transformers for\\n  Personalised Speech Enhancement',\n",
       "  'authors': ['Shucong Zhang',\n",
       "   'Malcolm Chadwick',\n",
       "   'Alberto Gil C. P. Ramos',\n",
       "   'Sourav Bhattacharya'],\n",
       "  'abstract': \"Personalised speech enhancement (PSE), which extracts only the speech of a target user and removes everything else from a recorded audio clip, can potentially improve users' experiences of audio AI modules deployed in the wild. To support a large variety of downstream audio tasks, such as real-time ASR and audio-call enhancement, a PSE solution should operate in a streaming mode, i.e., input audio cleaning should happen in real-time with a small latency and real-time factor. Personalisation is typically achieved by extracting a target speaker's voice profile from an enrolment audio, in the form of a static embedding vector, and then using it to condition the output of a PSE model. However, a fixed target speaker embedding may not be optimal under all conditions. In this work, we present a streaming Transformer-based PSE model and propose a novel cross-attention approach that gives adaptive target speaker representations. We present extensive experiments and show that our proposed cross-attention approach outperforms competitive baselines consistently, even when our model is only approximately half the size.\",\n",
       "  'arxiv_id': 'abs/2211.04346v1',\n",
       "  'category': None},\n",
       " 'abs/2110.03183v5': {'id': None,\n",
       "  'title': 'Attention is All You Need? Good Embeddings with Statistics are\\n  enough:Large Scale Audio Understanding without Transformers/ Convolutions/\\n  BERTs/ Mixers/ Attention/ RNNs or ....',\n",
       "  'authors': ['Prateek Verma'],\n",
       "  'abstract': 'This paper presents a way of doing large scale audio understanding without traditional state of the art neural architectures. Ever since the introduction of deep learning for understanding audio signals in the past decade, convolutional architectures have been able to achieve state of the art results surpassing traditional hand-crafted features. In the recent past, there has been a similar shift away from traditional convolutional and recurrent neural networks towards purely end-to-end Transformer architectures. We, in this work, explore an approach, based on Bag-of-Words model. Our approach does not have any convolutions, recurrence, attention, transformers or other approaches such as BERT. We utilize micro and macro level clustered vanilla embeddings, and use a MLP head for classification. We only use feed-forward encoder-decoder models to get the bottlenecks of spectral envelops, spectral patches and slices as well as multi-resolution spectra. A classification head (a feed-forward layer), similar to the approach in SimCLR is trained on a learned representation. Using simple codes learned on latent representations, we show how we surpass traditional convolutional neural network architectures, and come strikingly close to outperforming powerful Transformer architectures. This work hopefully would pave way for exciting advancements in the field of representation learning without massive, end-to-end neural architectures.',\n",
       "  'arxiv_id': 'abs/2110.03183v5',\n",
       "  'category': None},\n",
       " 'abs/2102.11870v1': {'id': None,\n",
       "  'title': 'UnsupervisedR&R: Unsupervised Point Cloud Registration via\\n  Differentiable Rendering',\n",
       "  'authors': ['Mohamed El Banani', 'Luya Gao', 'Justin Johnson'],\n",
       "  'abstract': \"Aligning partial views of a scene into a single whole is essential to understanding one's environment and is a key component of numerous robotics tasks such as SLAM and SfM. Recent approaches have proposed end-to-end systems that can outperform traditional methods by leveraging pose supervision. However, with the rising prevalence of cameras with depth sensors, we can expect a new stream of raw RGB-D data without the annotations needed for supervision. We propose UnsupervisedR&R: an end-to-end unsupervised approach to learning point cloud registration from raw RGB-D video. The key idea is to leverage differentiable alignment and rendering to enforce photometric and geometric consistency between frames. We evaluate our approach on indoor scene datasets and find that we outperform existing traditional approaches with classic and learned descriptors while being competitive with supervised geometric point cloud registration approaches.\",\n",
       "  'arxiv_id': 'abs/2102.11870v1',\n",
       "  'category': None},\n",
       " 'abs/1611.07853v2': {'id': None,\n",
       "  'title': 'Vector-valued multibang control of differential equations',\n",
       "  'authors': ['Christian Clason', 'Carla Tameling', 'Benedikt Wirth'],\n",
       "  'abstract': 'We consider a class of (ill-posed) optimal control problems in which a distributed vector-valued control is enforced to pointwise take values in a finite set $\\\\mathcal{M}\\\\subset\\\\mathbb{R}^m$. After convex relaxation, one obtains a well-posed optimization problem, which still promotes control values in $\\\\mathcal{M}$. We state the corresponding well-posedness and stability analysis and exemplify the results for two specific cases of quite general interest, optimal control of the Bloch equation and optimal control of an elastic deformation. We finally formulate a semismooth Newton method to numerically solve a regularized version of the optimal control problem and illustrate the behavior of the approach for our example cases.',\n",
       "  'arxiv_id': 'abs/1611.07853v2',\n",
       "  'category': None},\n",
       " 'abs/1710.07400v1': {'id': None,\n",
       "  'title': 'Ligand Pose Optimization with Atomic Grid-Based Convolutional Neural\\n  Networks',\n",
       "  'authors': ['Matthew Ragoza', 'Lillian Turner', 'David Ryan Koes'],\n",
       "  'abstract': 'Docking is an important tool in computational drug discovery that aims to predict the binding pose of a ligand to a target protein through a combination of pose scoring and optimization. A scoring function that is differentiable with respect to atom positions can be used for both scoring and gradient-based optimization of poses for docking. Using a differentiable grid-based atomic representation as input, we demonstrate that a scoring function learned by training a convolutional neural network (CNN) to identify binding poses can also be applied to pose optimization. We also show that an iteratively-trained CNN that includes poses optimized by the first CNN in its training set performs even better at optimizing randomly initialized poses than either the first CNN scoring function or AutoDock Vina.',\n",
       "  'arxiv_id': 'abs/1710.07400v1',\n",
       "  'category': None},\n",
       " 'abs/1905.07807v1': {'id': None,\n",
       "  'title': 'Good Feature Selection for Least Squares Pose Optimization in VO/VSLAM',\n",
       "  'authors': ['Yipu Zhao', 'Patricio A. Vela'],\n",
       "  'abstract': 'This paper aims to select features that contribute most to the pose estimation in VO/VSLAM. Unlike existing feature selection works that are focused on efficiency only, our method significantly improves the accuracy of pose tracking, while introducing little overhead. By studying the impact of feature selection towards least squares pose optimization, we demonstrate the applicability of improving accuracy via good feature selection. To that end, we introduce the Max-logDet metric to guide the feature selection, which is connected to the conditioning of least squares pose optimization problem. We then describe an efficient algorithm for approximately solving the NP-hard Max-logDet problem. Integrating Max-logDet feature selection into a state-of-the-art visual SLAM system leads to accuracy improvements with low overhead, as demonstrated via evaluation on a public benchmark.',\n",
       "  'arxiv_id': 'abs/1905.07807v1',\n",
       "  'category': None},\n",
       " 'abs/2205.01694v2': {'id': None,\n",
       "  'title': 'End2End Multi-View Feature Matching using Differentiable Pose\\n  Optimization',\n",
       "  'authors': ['Barbara Roessle', 'Matthias Nießner'],\n",
       "  'abstract': 'Erroneous feature matches have severe impact on subsequent camera pose estimation and often require additional, time-costly measures, like RANSAC, for outlier rejection. Our method tackles this challenge by addressing feature matching and pose optimization jointly. To this end, we propose a graph attention network to predict image correspondences along with confidence weights. The resulting matches serve as weighted constraints in a differentiable pose estimation. Training feature matching with gradients from pose optimization naturally learns to down-weight outliers and boosts pose estimation on image pairs compared to SuperGlue by 6.7% on ScanNet. At the same time, it reduces the pose estimation time by over 50% and renders RANSAC iterations unnecessary. Moreover, we integrate information from multiple views by spanning the graph across multiple frames to predict the matches all at once. Multi-view matching combined with end-to-end training improves the pose estimation metrics on Matterport3D by 18.8% compared to SuperGlue.',\n",
       "  'arxiv_id': 'abs/2205.01694v2',\n",
       "  'category': None},\n",
       " 'abs/2109.09934v2': {'id': None,\n",
       "  'title': 'Balancing Control and Pose Optimization for Wheel-legged Robots\\n  Navigating High Obstacles',\n",
       "  'authors': ['Junheng Li', 'Junchao Ma', 'Quan Nguyen'],\n",
       "  'abstract': \"In this paper, we propose a novel approach on controlling wheel-legged quadrupedal robots using pose optimization and force control via quadratic programming (QP). Our method allows the robot to leverage the whole-body motion and the wheel actuation to roll over high obstacles while keeping the wheel torques to navigate the terrain while keeping the wheel traction and balancing the robot body. In detail, we first present a linear rigid body dynamics with wheels that can be used for real-time balancing control of wheel-legged robots. We then introduce an effective pose optimization method for wheel-legged robot's locomotion over steep ramp and stair terrains. The pose optimization solves for optimal poses to enhance stability and enforce collision-fee constraints for the rolling motion over stair terrain. Experimental validation on the real robot demonstrated the capability of rolling up on a 0.36 m obstacle. The robot can also successfully roll up and down multiple stairs without lifting its legs or having collision with the terrain.\",\n",
       "  'arxiv_id': 'abs/2109.09934v2',\n",
       "  'category': None},\n",
       " 'abs/2002.12324v4': {'id': None,\n",
       "  'title': 'Visual Camera Re-Localization from RGB and RGB-D Images Using DSAC',\n",
       "  'authors': ['Eric Brachmann', 'Carsten Rother'],\n",
       "  'abstract': 'We describe a learning-based system that estimates the camera position and orientation from a single input image relative to a known environment. The system is flexible w.r.t. the amount of information available at test and at training time, catering to different applications. Input images can be RGB-D or RGB, and a 3D model of the environment can be utilized for training but is not necessary. In the minimal case, our system requires only RGB images and ground truth poses at training time, and it requires only a single RGB image at test time. The framework consists of a deep neural network and fully differentiable pose optimization. The neural network predicts so called scene coordinates, i.e. dense correspondences between the input image and 3D scene space of the environment. The pose optimization implements robust fitting of pose parameters using differentiable RANSAC (DSAC) to facilitate end-to-end training. The system, an extension of DSAC++ and referred to as DSAC*, achieves state-of-the-art accuracy an various public datasets for RGB-based re-localization, and competitive accuracy for RGB-D-based re-localization.',\n",
       "  'arxiv_id': 'abs/2002.12324v4',\n",
       "  'category': None},\n",
       " 'abs/2110.03940v2': {'id': None,\n",
       "  'title': 'Pose Refinement with Joint Optimization of Visual Points and Lines',\n",
       "  'authors': ['Shuang Gao',\n",
       "   'Jixiang Wan',\n",
       "   'Yishan Ping',\n",
       "   'Xudong Zhang',\n",
       "   'Shuzhou Dong',\n",
       "   'Yuchen Yang',\n",
       "   'Haikuan Ning',\n",
       "   'Jijunnan Li',\n",
       "   'Yandong Guo'],\n",
       "  'abstract': 'High-precision camera re-localization technology in a pre-established 3D environment map is the basis for many tasks, such as Augmented Reality, Robotics and Autonomous Driving. The point-based visual re-localization approaches are well-developed in recent decades, but are insufficient in some feature-less cases. In this paper, we design a complete pipeline for camera pose refinement with points and lines, which contains the innovatively designed line extracting CNN named VLSE, the line matching and the pose optimization approaches. We adopt a novel line representation and customize a hybrid convolution block based on the Stacked Hourglass network, to detect accurate and stable line features on images. Then we apply a geometric-based strategy to obtain precise 2D-3D line correspondences using epipolar constraint and reprojection filtering. A following point-line joint cost function is constructed to optimize the camera pose with the initial coarse pose from the pure point-based localization. Sufficient experiments are conducted on open datasets, i.e, line extractor on Wireframe and YorkUrban, localization performance on InLoc duc1 and duc2, to confirm the effectiveness of our point-line joint pose optimization method.',\n",
       "  'arxiv_id': 'abs/2110.03940v2',\n",
       "  'category': None},\n",
       " 'abs/2206.13345v1': {'id': None,\n",
       "  'title': 'A fully differentiable ligand pose optimization framework guided by deep\\n  learning and traditional scoring functions',\n",
       "  'authors': ['Zechen Wang',\n",
       "   'Liangzhen Zheng',\n",
       "   'Sheng Wang',\n",
       "   'Mingzhi Lin',\n",
       "   'Zhihao Wang',\n",
       "   'Adams Wai-Kin Kong',\n",
       "   'Yuguang Mu',\n",
       "   'Yanjie Wei',\n",
       "   'Weifeng Li'],\n",
       "  'abstract': 'The machine learning (ML) and deep learning (DL) techniques are widely recognized to be powerful tools for virtual drug screening. The recently reported ML- or DL-based scoring functions have shown exciting performance in predicting protein-ligand binding affinities with fruitful application prospects. However, the differentiation between highly similar ligand conformations, including the native binding pose (the global energy minimum state), remains challenging which could greatly enhance the docking. In this work, we propose a fully differentiable framework for ligand pose optimization based on a hybrid scoring function (SF) combined with a multi-layer perceptron (DeepRMSD) and the traditional AutoDock Vina SF. The DeepRMSD+Vina, which combines (1) the root mean square deviation (RMSD) of the docking pose with respect to the native pose and (2) the AutoDock Vina score, is fully differentiable thus is capable of optimizing the ligand binding pose to the energy-lowest conformation. Evaluated by the CASF-2016 docking power dataset, the DeepRMSD+Vina reaches a success rate of 95.4%, which is by far the best reported SF to date. Based on this SF, an end-to-end ligand pose optimization framework was implemented to improve the docking pose quality. We demonstrated that this method significantly improves the docking success rate (by 15%) in redocking and crossdocking tasks, revealing the high potentialities of this framework in drug design and discovery.',\n",
       "  'arxiv_id': 'abs/2206.13345v1',\n",
       "  'category': None},\n",
       " 'abs/cs/0611135v1': {'id': None,\n",
       "  'title': 'Genetic Programming for Kernel-based Learning with Co-evolving Subsets\\n  Selection',\n",
       "  'authors': ['Christian Gagné',\n",
       "   'Marc Schoenauer',\n",
       "   'Michèle Sebag',\n",
       "   'Marco Tomassini'],\n",
       "  'abstract': 'Support Vector Machines (SVMs) are well-established Machine Learning (ML) algorithms. They rely on the fact that i) linear learning can be formalized as a well-posed optimization problem; ii) non-linear learning can be brought into linear learning thanks to the kernel trick and the mapping of the initial search space onto a high dimensional feature space. The kernel is designed by the ML expert and it governs the efficiency of the SVM approach. In this paper, a new approach for the automatic design of kernels by Genetic Programming, called the Evolutionary Kernel Machine (EKM), is presented. EKM combines a well-founded fitness function inspired from the margin criterion, and a co-evolution framework ensuring the computational scalability of the approach. Empirical validation on standard ML benchmark demonstrates that EKM is competitive using state-of-the-art SVMs with tuned hyper-parameters.',\n",
       "  'arxiv_id': 'abs/cs/0611135v1',\n",
       "  'category': None},\n",
       " 'abs/1409.6045v1': {'id': None,\n",
       "  'title': 'Analyzing sparse dictionaries for online learning with kernels',\n",
       "  'authors': ['Paul Honeine'],\n",
       "  'abstract': \"Many signal processing and machine learning methods share essentially the same linear-in-the-parameter model, with as many parameters as available samples as in kernel-based machines. Sparse approximation is essential in many disciplines, with new challenges emerging in online learning with kernels. To this end, several sparsity measures have been proposed in the literature to quantify sparse dictionaries and constructing relevant ones, the most prolific ones being the distance, the approximation, the coherence and the Babel measures. In this paper, we analyze sparse dictionaries based on these measures. By conducting an eigenvalue analysis, we show that these sparsity measures share many properties, including the linear independence condition and inducing a well-posed optimization problem. Furthermore, we prove that there exists a quasi-isometry between the parameter (i.e., dual) space and the dictionary's induced feature space.\",\n",
       "  'arxiv_id': 'abs/1409.6045v1',\n",
       "  'category': None},\n",
       " 'abs/1601.03329v1': {'id': None,\n",
       "  'title': 'Fixed-Endpoint Optimal Control of Bilinear Ensemble Systems',\n",
       "  'authors': ['Shuo Wang', 'Jr-Shin Li'],\n",
       "  'abstract': 'Optimal control of bilinear systems has been a well-studied subject in the area of mathematical control. However, techniques for solving emerging optimal control problems involving an ensemble of structurally identical bilinear systems are underdeveloped. In this work, we develop an iterative method to effectively and systematically solve these challenging optimal ensemble control problems, in which the bilinear ensemble system is represented as a time-varying linear ensemble system at each iteration and the optimal ensemble control law is then obtained by the singular value expansion of the input-to-state operator that describes the dynamics of the linear ensemble system. We examine the convergence of the developed iterative procedure and pose optimality conditions for the convergent solution. We also provide examples of practical control designs in magnetic resonance to demonstrate the applicability and robustness of the developed iterative method.',\n",
       "  'arxiv_id': 'abs/1601.03329v1',\n",
       "  'category': None},\n",
       " 'abs/1602.03860v1': {'id': None,\n",
       "  'title': 'Real-Time Hand Tracking Using a Sum of Anisotropic Gaussians Model',\n",
       "  'authors': ['Srinath Sridhar',\n",
       "   'Helge Rhodin',\n",
       "   'Hans-Peter Seidel',\n",
       "   'Antti Oulasvirta',\n",
       "   'Christian Theobalt'],\n",
       "  'abstract': 'Real-time marker-less hand tracking is of increasing importance in human-computer interaction. Robust and accurate tracking of arbitrary hand motion is a challenging problem due to the many degrees of freedom, frequent self-occlusions, fast motions, and uniform skin color. In this paper, we propose a new approach that tracks the full skeleton motion of the hand from multiple RGB cameras in real-time. The main contributions include a new generative tracking method which employs an implicit hand shape representation based on Sum of Anisotropic Gaussians (SAG), and a pose fitting energy that is smooth and analytically differentiable making fast gradient based pose optimization possible. This shape representation, together with a full perspective projection model, enables more accurate hand modeling than a related baseline method from literature. Our method achieves better accuracy than previous methods and runs at 25 fps. We show these improvements both qualitatively and quantitatively on publicly available datasets.',\n",
       "  'arxiv_id': 'abs/1602.03860v1',\n",
       "  'category': None},\n",
       " 'abs/1612.00040v1': {'id': None,\n",
       "  'title': 'Principal component analysis of periodically correlated functional time\\n  series',\n",
       "  'authors': ['Łukasz Kidziński', 'Piotr Kokoszka', 'Neda Mohammadi Jouzdani'],\n",
       "  'abstract': 'Within the framework of functional data analysis, we develop principal component analysis for periodically correlated time series of functions. We define the components of the above analysis including periodic, operator-valued filters, score processes and the inversion formulas. We show that these objects are defined via convergent series under a simple condition requiring summability of the Hilbert-Schmidt norms of the filter coefficients, and that they poses optimality properties.   We explain how the Hilbert space theory reduces to an approximate finite-dimensional setting which is implemented in a custom build R package. A data example and a simulation study show that the new methodology is superior to existing tools if the functional time series exhibit periodic characteristics.',\n",
       "  'arxiv_id': 'abs/1612.00040v1',\n",
       "  'category': None},\n",
       " 'abs/1710.10519v3': {'id': None,\n",
       "  'title': 'Exploiting Points and Lines in Regression Forests for RGB-D Camera\\n  Relocalization',\n",
       "  'authors': ['Lili Meng',\n",
       "   'Frederick Tung',\n",
       "   'James J. Little',\n",
       "   'Julien Valentin',\n",
       "   'Clarence de Silva'],\n",
       "  'abstract': 'Camera relocalization plays a vital role in many robotics and computer vision tasks, such as global localization, recovery from tracking failure and loop closure detection. Recent random forests based methods exploit randomly sampled pixel comparison features to predict 3D world locations for 2D image locations to guide the camera pose optimization. However, these image features are only sampled randomly in the images, without considering the spatial structures or geometric information, leading to large errors or failure cases with the existence of poorly textured areas or in motion blur. Line segment features are more robust in these environments. In this work, we propose to jointly exploit points and lines within the framework of uncertainty driven regression forests. The proposed approach is thoroughly evaluated on three publicly available datasets against several strong state-of-the-art baselines in terms of several different error metrics. Experimental results prove the efficacy of our method, showing superior or on-par state-of-the-art performance.',\n",
       "  'arxiv_id': 'abs/1710.10519v3',\n",
       "  'category': None},\n",
       " 'abs/1803.02380v3': {'id': None,\n",
       "  'title': 'Fast Cylinder and Plane Extraction from Depth Cameras for Visual\\n  Odometry',\n",
       "  'authors': ['Pedro F. Proença', 'Yang Gao'],\n",
       "  'abstract': 'This paper presents CAPE, a method to extract planes and cylinder segments from organized point clouds, which processes 640x480 depth images on a single CPU core at an average of 300 Hz, by operating on a grid of planar cells. While, compared to state-of-the-art plane extraction, the latency of CAPE is more consistent and 4-10 times faster, depending on the scene, we also demonstrate empirically that applying CAPE to visual odometry can improve trajectory estimation on scenes made of cylindrical surfaces (e.g. tunnels), whereas using a plane extraction approach that is not curve-aware deteriorates performance on these scenes. To use these geometric primitives in visual odometry, we propose extending a probabilistic RGB-D odometry framework based on points, lines and planes to cylinder primitives. Following this framework, CAPE runs on fused depth maps and the parameters of cylinders are modelled probabilistically to account for uncertainty and weight accordingly the pose optimization residuals.',\n",
       "  'arxiv_id': 'abs/1803.02380v3',\n",
       "  'category': None},\n",
       " 'abs/1804.00050v2': {'id': None,\n",
       "  'title': 'Real-Time Grasp Planning for Multi-Fingered Hands by Finger Splitting',\n",
       "  'authors': ['Yongxiang Fan',\n",
       "   'Te Tang',\n",
       "   'Hsien-Chung Lin',\n",
       "   'Masayoshi Tomizuka'],\n",
       "  'abstract': 'Grasp planning for multi-fingered hands is computationally expensive due to the joint-contact coupling, surface nonlinearities and high dimensionality, thus is generally not affordable for real-time implementations. Traditional planning methods by optimization, sampling or learning work well in planning for parallel grippers but remain challenging for multi-fingered hands. This paper proposes a strategy called finger splitting, to plan precision grasps for multi-fingered hands starting from optimal parallel grasps. The finger splitting is optimized by a dual-stage iterative optimization including a contact point optimization (CPO) and a palm pose optimization (PPO), to gradually split fingers and adjust both the contact points and the palm pose. The dual-stage optimization is able to consider both the object grasp quality and hand manipulability, address the nonlinearities and coupling, and achieve efficient convergence within one second. Simulation results demonstrate the effectiveness of the proposed approach. The simulation video is available at: http://me.berkeley.edu/\\\\%7Eyongxiangfan/IROS2018/fingersplit.html',\n",
       "  'arxiv_id': 'abs/1804.00050v2',\n",
       "  'category': None},\n",
       " 'abs/1909.06293v4': {'id': None,\n",
       "  'title': 'ISL: A novel approach for deep exploration',\n",
       "  'authors': ['Lucas Cassano', 'Ali H. Sayed'],\n",
       "  'abstract': 'In this article we explore an alternative approach to address deep exploration and we introduce the ISL algorithm, which is efficient at performing deep exploration. Similarly to maximum entropy RL, we derive the algorithm by augmenting the traditional RL objective with a novel regularization term. A distinctive feature of our approach is that, as opposed to other works that tackle the problem of deep exploration, in our derivation both the learning equations and the exploration-exploitation strategy are derived in tandem as the solution to a well-posed optimization problem whose minimization leads to the optimal value function. Empirically we show that our method exhibits state of the art performance on a range of challenging deep-exploration benchmarks.',\n",
       "  'arxiv_id': 'abs/1909.06293v4',\n",
       "  'category': None},\n",
       " 'abs/1805.04496v1': {'id': None,\n",
       "  'title': 'Cell-free Massive MIMO Networks: Optimal Power Control against Active\\n  Eavesdropping',\n",
       "  'authors': ['Tiep M. Hoang',\n",
       "   'Hien Quoc Ngo',\n",
       "   'Trung Q. Duong',\n",
       "   'Hoang D. Tuan',\n",
       "   'Alan Marshall'],\n",
       "  'abstract': 'This paper studies the security aspect of a recently introduced network (\"cell-free massive MIMO\") under a pilot spoofing attack. Firstly, a simple method to recognize the presence of this type of an active eavesdropping attack to a particular user is shown. In order to deal with this attack, we consider the problem of maximizing the achievable data rate of the attacked user or its achievable secrecy rate. The corresponding problems of minimizing the consumption power subject to security constraints are also considered in parallel. Path-following algorithms are developed to solve the posed optimization problems under different power allocation to access points (APs). Under equip-power allocation to APs, these optimization problems admit closed-form solutions. Numerical results show their efficiencies.',\n",
       "  'arxiv_id': 'abs/1805.04496v1',\n",
       "  'category': None},\n",
       " 'abs/1908.08891v1': {'id': None,\n",
       "  'title': 'Flexible Trinocular: Non-rigid Multi-Camera-IMU Dense Reconstruction for\\n  UAV Navigation and Mapping',\n",
       "  'authors': ['Timo Hinzmann',\n",
       "   'Cesar Cadena',\n",
       "   'Juan Nieto',\n",
       "   'Roland Siegwart'],\n",
       "  'abstract': 'In this paper, we propose a visual-inertial framework able to efficiently estimate the camera poses of a non-rigid trinocular baseline for long-range depth estimation on-board a fast moving aerial platform. The estimation of the time-varying baseline is based on relative inertial measurements, a photometric relative pose optimizer, and a probabilistic wing model fused in an efficient Extended Kalman Filter (EKF) formulation. The estimated depth measurements can be integrated into a geo-referenced global map to render a reconstruction of the environment useful for local replanning algorithms. Based on extensive real-world experiments we describe the challenges and solutions for obtaining the probabilistic wing model, reliable relative inertial measurements, and vision-based relative pose updates and demonstrate the computational efficiency and robustness of the overall system under challenging conditions.',\n",
       "  'arxiv_id': 'abs/1908.08891v1',\n",
       "  'category': None},\n",
       " 'abs/1910.13102v1': {'id': None,\n",
       "  'title': 'A Robust Pavement Mapping System Based on Normal-Constrained Stereo\\n  Visual Odometry',\n",
       "  'authors': ['Huaiyang Huang',\n",
       "   'Rui Fan',\n",
       "   'Yilong Zhu',\n",
       "   'Ming Liu',\n",
       "   'Ioannis Pitas'],\n",
       "  'abstract': 'Pavement condition is crucial for civil infrastructure maintenance. This task usually requires efficient road damage localization, which can be accomplished by the visual odometry system embedded in unmanned aerial vehicles (UAVs). However, the state-of-the-art visual odometry and mapping methods suffer from large drift under the degeneration of the scene structure. To alleviate this issue, we integrate normal constraints into the visual odometry process, which greatly helps to avoid large drift. By parameterizing the normal vector on the tangential plane, the normal factors are coupled with traditional reprojection factors in the pose optimization procedure. The experimental results demonstrate the effectiveness of the proposed system. The overall absolute trajectory error is improved by approximately 20%, which indicates that the estimated trajectory is much more accurate than that obtained using other state-of-the-art methods.',\n",
       "  'arxiv_id': 'abs/1910.13102v1',\n",
       "  'category': None},\n",
       " 'abs/1912.12726v1': {'id': None,\n",
       "  'title': 'SLOAM: Semantic Lidar Odometry and Mapping for Forest Inventory',\n",
       "  'authors': ['Steven W. Chen',\n",
       "   'Guilherme V. Nardari',\n",
       "   'Elijah S. Lee',\n",
       "   'Chao Qu',\n",
       "   'Xu Liu',\n",
       "   'Roseli A. F. Romero',\n",
       "   'Vijay Kumar'],\n",
       "  'abstract': 'This paper describes an end-to-end pipeline for tree diameter estimation based on semantic segmentation and lidar odometry and mapping. Accurate mapping of this type of environment is challenging since the ground and the trees are surrounded by leaves, thorns and vines, and the sensor typically experiences extreme motion. We propose a semantic feature based pose optimization that simultaneously refines the tree models while estimating the robot pose. The pipeline utilizes a custom virtual reality tool for labeling 3D scans that is used to train a semantic segmentation network. The masked point cloud is used to compute a trellis graph that identifies individual instances and extracts relevant features that are used by the SLAM module. We show that traditional lidar and image based methods fail in the forest environment on both Unmanned Aerial Vehicle (UAV) and hand-carry systems, while our method is more robust, scalable, and automatically generates tree diameter estimations.',\n",
       "  'arxiv_id': 'abs/1912.12726v1',\n",
       "  'category': None},\n",
       " 'abs/2009.12662v1': {'id': None,\n",
       "  'title': 'Co-Planar Parametrization for Stereo-SLAM and Visual-Inertial Odometry',\n",
       "  'authors': ['Xin Li',\n",
       "   'Yanyan Li',\n",
       "   'Evin Pınar Örnek',\n",
       "   'Jinlong Lin',\n",
       "   'Federico Tombari'],\n",
       "  'abstract': 'This work proposes a novel SLAM framework for stereo and visual inertial odometry estimation. It builds an efficient and robust parametrization of co-planar points and lines which leverages specific geometric constraints to improve camera pose optimization in terms of both efficiency and accuracy. %reduce the size of the Hessian matrix in the optimization. The pipeline consists of extracting 2D points and lines, predicting planar regions and filtering the outliers via RANSAC. Our parametrization scheme then represents co-planar points and lines as their 2D image coordinates and parameters of planes. We demonstrate the effectiveness of the proposed method by comparing it to traditional parametrizations in a novel Monte-Carlo simulation set. Further, the whole stereo SLAM and VIO system is compared with state-of-the-art methods on the public real-world dataset EuRoC. Our method shows better results in terms of accuracy and efficiency than the state-of-the-art. The code is released at https://github.com/LiXin97/Co-Planar-Parametrization.',\n",
       "  'arxiv_id': 'abs/2009.12662v1',\n",
       "  'category': None},\n",
       " 'abs/2102.13187v1': {'id': None,\n",
       "  'title': 'CollisionIK: A Per-Instant Pose Optimization Method for Generating Robot\\n  Motions with Environment Collision Avoidance',\n",
       "  'authors': ['Daniel Rakita',\n",
       "   'Haochen Shi',\n",
       "   'Bilge Mutlu',\n",
       "   'Michael Gleicher'],\n",
       "  'abstract': 'In this work, we present a per-instant pose optimization method that can generate configurations that achieve specified pose or motion objectives as best as possible over a sequence of solutions, while also simultaneously avoiding collisions with static or dynamic obstacles in the environment. We cast our method as a multi-objective, non-linear constrained optimization-based IK problem where each term in the objective function encodes a particular pose objective. We demonstrate how to effectively incorporate environment collision avoidance as a single term in this multi-objective, optimization-based IK structure, and provide solutions for how to spatially represent and organize external environments such that data can be efficiently passed to a real-time, performance-critical optimization loop. We demonstrate the effectiveness of our method by comparing it to various state-of-the-art methods in a testbed of simulation experiments and discuss the implications of our work based on our results.',\n",
       "  'arxiv_id': 'abs/2102.13187v1',\n",
       "  'category': None},\n",
       " 'abs/2103.15279v1': {'id': None,\n",
       "  'title': 'Generalizing to the Open World: Deep Visual Odometry with Online\\n  Adaptation',\n",
       "  'authors': ['Shunkai Li', 'Xin Wu', 'Yingdian Cao', 'Hongbin Zha'],\n",
       "  'abstract': 'Despite learning-based visual odometry (VO) has shown impressive results in recent years, the pretrained networks may easily collapse in unseen environments. The large domain gap between training and testing data makes them difficult to generalize to new scenes. In this paper, we propose an online adaptation framework for deep VO with the assistance of scene-agnostic geometric computations and Bayesian inference. In contrast to learning-based pose estimation, our method solves pose from optical flow and depth while the single-view depth estimation is continuously improved with new observations by online learned uncertainties. Meanwhile, an online learned photometric uncertainty is used for further depth and pose optimization by a differentiable Gauss-Newton layer. Our method enables fast adaptation of deep VO networks to unseen environments in a self-supervised manner. Extensive experiments including Cityscapes to KITTI and outdoor KITTI to indoor TUM demonstrate that our method achieves state-of-the-art generalization ability among self-supervised VO methods.',\n",
       "  'arxiv_id': 'abs/2103.15279v1',\n",
       "  'category': None},\n",
       " 'abs/2104.05252v1': {'id': None,\n",
       "  'title': 'Boltzmann Tuning of Generative Models',\n",
       "  'authors': ['Victor Berger', 'Michele Sebag'],\n",
       "  'abstract': 'The paper focuses on the a posteriori tuning of a generative model in order to favor the generation of good instances in the sense of some external differentiable criterion. The proposed approach, called Boltzmann Tuning of Generative Models (BTGM), applies to a wide range of applications. It covers conditional generative modelling as a particular case, and offers an affordable alternative to rejection sampling. The contribution of the paper is twofold. Firstly, the objective is formalized and tackled as a well-posed optimization problem; a practical methodology is proposed to choose among the candidate criteria representing the same goal, the one best suited to efficiently learn a tuned generative model. Secondly, the merits of the approach are demonstrated on a real-world application, in the context of robust design for energy policies, showing the ability of BTGM to sample the extreme regions of the considered criteria.',\n",
       "  'arxiv_id': 'abs/2104.05252v1',\n",
       "  'category': None},\n",
       " 'abs/2109.12030v1': {'id': None,\n",
       "  'title': 'Toward Efficient and Robust Multiple Camera Visual-inertial Odometry',\n",
       "  'authors': ['Yao He', 'Huai Yu', 'Wen Yang', 'Sebastian Scherer'],\n",
       "  'abstract': \"Efficiency and robustness are the essential criteria for the visual-inertial odometry (VIO) system. To process massive visual data, the high cost on CPU resources and computation latency limits VIO's possibility in integration with other applications. Recently, the powerful embedded GPUs have great potentials to improve the front-end image processing capability. Meanwhile, multi-camera systems can increase the visual constraints for back-end optimization. Inspired by these insights, we incorporate the GPU-enhanced algorithms in the field of VIO and thus propose a new front-end with NVIDIA Vision Programming Interface (VPI). This new front-end then enables multi-camera VIO feature association and provides more stable back-end pose optimization. Experiments with our new front-end on monocular datasets show the CPU resource occupation rate and computational latency are reduced by 40.4% and 50.6% without losing accuracy compared with the original VIO. The multi-camera system shows a higher VIO initialization success rate and better robustness overall state estimation.\",\n",
       "  'arxiv_id': 'abs/2109.12030v1',\n",
       "  'category': None},\n",
       " 'abs/2110.08977v1': {'id': None,\n",
       "  'title': 'Accurate and Robust Object-oriented SLAM with 3D Quadric Landmark\\n  Construction in Outdoor Environment',\n",
       "  'authors': ['Rui Tian',\n",
       "   'Yunzhou Zhang',\n",
       "   'Yonghui Feng',\n",
       "   'Linghao Yang',\n",
       "   'Zhenzhong Cao',\n",
       "   'Sonya Coleman',\n",
       "   'Dermot Kerr'],\n",
       "  'abstract': 'Object-oriented SLAM is a popular technology in autonomous driving and robotics. In this paper, we propose a stereo visual SLAM with a robust quadric landmark representation method. The system consists of four components, including deep learning detection, object-oriented data association, dual quadric landmark initialization and object-based pose optimization. State-of-the-art quadric-based SLAM algorithms always face observation related problems and are sensitive to observation noise, which limits their application in outdoor scenes. To solve this problem, we propose a quadric initialization method based on the decoupling of the quadric parameters method, which improves the robustness to observation noise. The sufficient object data association algorithm and object-oriented optimization with multiple cues enables a highly accurate object pose estimation that is robust to local observations. Experimental results show that the proposed system is more robust to observation noise and significantly outperforms current state-of-the-art methods in outdoor environments. In addition, the proposed system demonstrates real-time performance.',\n",
       "  'arxiv_id': 'abs/2110.08977v1',\n",
       "  'category': None},\n",
       " 'abs/2206.01916v1': {'id': None,\n",
       "  'title': 'Nerfels: Renderable Neural Codes for Improved Camera Pose Estimation',\n",
       "  'authors': ['Gil Avraham',\n",
       "   'Julian Straub',\n",
       "   'Tianwei Shen',\n",
       "   'Tsun-Yi Yang',\n",
       "   'Hugo Germain',\n",
       "   'Chris Sweeney',\n",
       "   'Vasileios Balntas',\n",
       "   'David Novotny',\n",
       "   'Daniel DeTone',\n",
       "   'Richard Newcombe'],\n",
       "  'abstract': 'This paper presents a framework that combines traditional keypoint-based camera pose optimization with an invertible neural rendering mechanism. Our proposed 3D scene representation, Nerfels, is locally dense yet globally sparse. As opposed to existing invertible neural rendering systems which overfit a model to the entire scene, we adopt a feature-driven approach for representing scene-agnostic, local 3D patches with renderable codes. By modelling a scene only where local features are detected, our framework effectively generalizes to unseen local regions in the scene via an optimizable code conditioning mechanism in the neural renderer, all while maintaining the low memory footprint of a sparse 3D map representation. Our model can be incorporated to existing state-of-the-art hand-crafted and learned local feature pose estimators, yielding improved performance when evaluating on ScanNet for wide camera baseline scenarios.',\n",
       "  'arxiv_id': 'abs/2206.01916v1',\n",
       "  'category': None},\n",
       " 'abs/2211.06624v1': {'id': None,\n",
       "  'title': 'Regularized Barzilai-Borwein method',\n",
       "  'authors': ['Congpei An', 'Xin Xu'],\n",
       "  'abstract': 'This paper is concerned with the introduction of regularization into RBB method for solving some ill-posed optimization problems. The step size generated by the BB method is relatively too long, resulting in a very unstable sequence of iterations and non-monotonicity of the objective function value in some difficult problems. A new step size generated by the RBB method is relatively short and is more effective in its ability to overcome the instability of the BB method in such ill-posed problems. At the same time, fast convergence of the RBB method is maintained by the adjustment of regularization parameters. Numerical examples show the benefits of the RBB method for solving some difficult problems.',\n",
       "  'arxiv_id': 'abs/2211.06624v1',\n",
       "  'category': None},\n",
       " 'abs/2105.05600v1': {'id': None,\n",
       "  'title': 'ROSEFusion: Random Optimization for Online Dense Reconstruction under\\n  Fast Camera Motion',\n",
       "  'authors': ['Jiazhao Zhang', 'Chenyang Zhu', 'Lintao Zheng', 'Kai Xu'],\n",
       "  'abstract': 'Online reconstruction based on RGB-D sequences has thus far been restrained to relatively slow camera motions (<1m/s). Under very fast camera motion (e.g., 3m/s), the reconstruction can easily crumble even for the state-of-the-art methods. Fast motion brings two challenges to depth fusion: 1) the high nonlinearity of camera pose optimization due to large inter-frame rotations and 2) the lack of reliably trackable features due to motion blur. We propose to tackle the difficulties of fast-motion camera tracking in the absence of inertial measurements using random optimization, in particular, the Particle Filter Optimization (PFO). To surmount the computation-intensive particle sampling and update in standard PFO, we propose to accelerate the randomized search via updating a particle swarm template (PST). PST is a set of particles pre-sampled uniformly within the unit sphere in the 6D space of camera pose. Through moving and rescaling the pre-sampled PST guided by swarm intelligence, our method is able to drive tens of thousands of particles to locate and cover a good local optimum extremely fast and robustly. The particles, representing candidate poses, are evaluated with a fitness function defined based on depth-model conformance. Therefore, our method, being depth-only and correspondence-free, mitigates the motion blur impediment as ToF-based depths are often resilient to motion blur. Thanks to the efficient template-based particle set evolution and the effective fitness function, our method attains good quality pose tracking under fast camera motion (up to 4m/s) in a realtime framerate without including loop closure or global pose optimization. Through extensive evaluations on public datasets of RGB-D sequences, especially on a newly proposed benchmark of fast camera motion, we demonstrate the significant advantage of our method over the state of the arts.',\n",
       "  'arxiv_id': 'abs/2105.05600v1',\n",
       "  'category': None},\n",
       " 'abs/1310.4638v2': {'id': None,\n",
       "  'title': 'Using multiobjective optimization to map the entropy region of four\\n  random variables',\n",
       "  'authors': ['Laszlo Csirmaz'],\n",
       "  'abstract': \"Presently the only available method of exploring the 15-dimensional entropy region formed by the entropies of four random variables is the one of Zhang and Yeung from 1998. It is argued that their method is equivalent to solving linear multiobjective optimization problems. Benson's outer approximation algorithm is a fundamental tool for solving these optimization problems. An improved version of Benson's algorithm is described which requires solving one scalar linear program in each iteration rather than two or three as in previous versions. During the algorithm design special care was taken for numerical stability. The implemented algorithm was used to check previous statements about the entropy region, and to gain new information on that region. The experimental results demonstrate the viability of the method for determining the extremal set of medium size, numerically ill-posed optimization problems. With growing problem size two limitations of Benson's algorithm have been observed: the inefficiency of the scalar LP solver on one hand and the unexpectedly large number of intermediate vertices on the other.\",\n",
       "  'arxiv_id': 'abs/1310.4638v2',\n",
       "  'category': None},\n",
       " 'abs/1312.7482v2': {'id': None,\n",
       "  'title': 'Element-wise uniqueness, prior knowledge, and data-dependent resolution',\n",
       "  'authors': ['Keith Dillon', 'Yeshaiahu Fainman'],\n",
       "  'abstract': 'Techniques for finding regularized solutions to underdetermined linear systems can be viewed as imposing prior knowledge on the unknown vector. The success of modern techniques, which can impose priors such as sparsity and non-negativity, is the result of advances in optimization algorithms to solve problems which lack closed-form solutions. Techniques for characterization and analysis of the system to determined when information is recoverable, however, still typically rely on closed-form solution techniques such as singular value decomposition or a filter cutoff, for example. In this letter we pose optimization approaches to broaden the approach to system characterization. We start by deriving conditions for when each unknown element of a system admits a unique solution, subject to a broad class of types of prior knowledge. With this approach we can pose a convex optimization problem to find \"how unique\" each element of the solution is, which may be viewed as a generalization of resolution to incorporate prior knowledge. We find that the result varies with the unknown vector itself, i.e. is data-dependent, such as when the sparsity of the solution improves the chance it can be uniquely reconstructed. The approach can be used to analyze systems on a case-by-case basis, estimate the amount of important information present in the data, and quantitatively understand the degree to which the regularized solution may be trusted.',\n",
       "  'arxiv_id': 'abs/1312.7482v2',\n",
       "  'category': None},\n",
       " 'abs/1410.7566v1': {'id': None,\n",
       "  'title': 'Parametric Estimation of Ordinary Differential Equations with\\n  Orthogonality Conditions',\n",
       "  'authors': ['Nicolas J-B Brunel', 'Quentin Clairon', \"Florence d'Alche-Buc\"],\n",
       "  'abstract': 'Differential equations are commonly used to model dynamical deterministic systems in applications. When statistical parameter estimation is required to calibrate theoretical models to data, classical statistical estimators are often confronted to complex and potentially ill-posed optimization problem. As a consequence, alternative estimators to classical parametric estimators are needed for obtaining reliable estimates. We propose a gradient matching approach for the estimation of parametric Ordinary Differential Equations observed with noise. Starting from a nonparametric proxy of a true solution of the ODE, we build a parametric estimator based on a variational characterization of the solution. As a Generalized Moment Estimator, our estimator must satisfy a set of orthogonal conditions that are solved in the least squares sense. Despite the use of a nonparametric estimator, we prove the root-$n$ consistency and asymptotic normality of the Orthogonal Conditions estimator. We can derive confidence sets thanks to a closed-form expression for the asymptotic variance. Finally, the OC estimator is compared to classical estimators in several (simulated and real) experiments and ODE models in order to show its versatility and relevance with respect to classical Gradient Matching and Nonlinear Least Squares estimators. In particular, we show on a real dataset of influenza infection that the approach gives reliable estimates. Moreover, we show that our approach can deal directly with more elaborated models such as Delay Differential Equation (DDE).',\n",
       "  'arxiv_id': 'abs/1410.7566v1',\n",
       "  'category': None},\n",
       " 'abs/1606.00151v2': {'id': None,\n",
       "  'title': 'Mapping and Localization from Planar Markers',\n",
       "  'authors': ['Rafael Muñoz-Salinas',\n",
       "   'Manuel J. Marín-Jimenez',\n",
       "   'Enrique Yeguas-Bolivar',\n",
       "   'Rafael Medina-Carnicer'],\n",
       "  'abstract': 'Squared planar markers are a popular tool for fast, accurate and robust camera localization, but its use is frequently limited to a single marker, or at most, to a small set of them for which their relative pose is known beforehand. Mapping and localization from a large set of planar markers is yet a scarcely treated problem in favour of keypoint-based approaches. However, while keypoint detectors are not robust to rapid motion, large changes in viewpoint, or significant changes in appearance, fiducial markers can be robustly detected under a wider range of conditions. This paper proposes a novel method to simultaneously solve the problems of mapping and localization from a set of squared planar markers. First, a quiver of pairwise relative marker poses is created, from which an initial pose graph is obtained. The pose graph may contain small pairwise pose errors, that when propagated, leads to large errors. Thus, we distribute the rotational and translational error along the basis cycles of the graph so as to obtain a corrected pose graph. Finally, we perform a global pose optimization by minimizing the reprojection errors of the planar markers in all observed frames. The experiments conducted show that our method performs better than Structure from Motion and visual SLAM techniques.',\n",
       "  'arxiv_id': 'abs/1606.00151v2',\n",
       "  'category': None},\n",
       " 'abs/1610.04889v1': {'id': None,\n",
       "  'title': 'Real-time Joint Tracking of a Hand Manipulating an Object from RGB-D\\n  Input',\n",
       "  'authors': ['Srinath Sridhar',\n",
       "   'Franziska Mueller',\n",
       "   'Michael Zollhöfer',\n",
       "   'Dan Casas',\n",
       "   'Antti Oulasvirta',\n",
       "   'Christian Theobalt'],\n",
       "  'abstract': 'Real-time simultaneous tracking of hands manipulating and interacting with external objects has many potential applications in augmented reality, tangible computing, and wearable computing. However, due to difficult occlusions, fast motions, and uniform hand appearance, jointly tracking hand and object pose is more challenging than tracking either of the two separately. Many previous approaches resort to complex multi-camera setups to remedy the occlusion problem and often employ expensive segmentation and optimization steps which makes real-time tracking impossible. In this paper, we propose a real-time solution that uses a single commodity RGB-D camera. The core of our approach is a 3D articulated Gaussian mixture alignment strategy tailored to hand-object tracking that allows fast pose optimization. The alignment energy uses novel regularizers to address occlusions and hand-object contacts. For added robustness, we guide the optimization with discriminative part classification of the hand and segmentation of the object. We conducted extensive experiments on several existing datasets and introduce a new annotated hand-object dataset. Quantitative and qualitative results show the key advantages of our method: speed, accuracy, and robustness.',\n",
       "  'arxiv_id': 'abs/1610.04889v1',\n",
       "  'category': None},\n",
       " 'abs/1702.04423v3': {'id': None,\n",
       "  'title': 'Efficient Multitask Feature and Relationship Learning',\n",
       "  'authors': ['Han Zhao', 'Otilia Stretcu', 'Alex Smola', 'Geoff Gordon'],\n",
       "  'abstract': 'We consider a multitask learning problem, in which several predictors are learned jointly. Prior research has shown that learning the relations between tasks, and between the input features, together with the predictor, can lead to better generalization and interpretability, which proved to be useful for applications in many domains. In this paper, we consider a formulation of multitask learning that learns the relationships both between tasks and between features, represented through a task covariance and a feature covariance matrix, respectively. First, we demonstrate that existing methods proposed for this problem present an issue that may lead to ill-posed optimization. We then propose an alternative formulation, as well as an efficient algorithm to optimize it. Using ideas from optimization and graph theory, we propose an efficient coordinate-wise minimization algorithm that has a closed form solution for each block subproblem. Our experiments show that the proposed optimization method is orders of magnitude faster than its competitors. We also provide a nonlinear extension that is able to achieve better generalization than existing methods.',\n",
       "  'arxiv_id': 'abs/1702.04423v3',\n",
       "  'category': None},\n",
       " 'abs/1710.07965v1': {'id': None,\n",
       "  'title': 'Backtracking Regression Forests for Accurate Camera Relocalization',\n",
       "  'authors': ['Lili Meng',\n",
       "   'Jianhui Chen',\n",
       "   'Frederick Tung',\n",
       "   'James J. Little',\n",
       "   'Julien Valentin',\n",
       "   'Clarence W. de Silva'],\n",
       "  'abstract': 'Camera relocalization plays a vital role in many robotics and computer vision tasks, such as global localization, recovery from tracking failure, and loop closure detection. Recent random forests based methods directly predict 3D world locations for 2D image locations to guide the camera pose optimization. During training, each tree greedily splits the samples to minimize the spatial variance. However, these greedy splits often produce uneven sub-trees in training or incorrect 2D-3D correspondences in testing. To address these problems, we propose a sample-balanced objective to encourage equal numbers of samples in the left and right sub-trees, and a novel backtracking scheme to remedy the incorrect 2D-3D correspondence predictions. Furthermore, we extend the regression forests based methods to use local features in both training and testing stages for outdoor RGB-only applications. Experimental results on publicly available indoor and outdoor datasets demonstrate the efficacy of our approach, which shows superior or on-par accuracy with several state-of-the-art methods.',\n",
       "  'arxiv_id': 'abs/1710.07965v1',\n",
       "  'category': None},\n",
       " 'abs/1809.10035v1': {'id': None,\n",
       "  'title': 'A Randomized Block Coordinate Iterative Regularized Gradient Method for\\n  High-dimensional Ill-posed Convex Optimization',\n",
       "  'authors': ['Harshal Kaushik', 'Farzad Yousefian'],\n",
       "  'abstract': 'Motivated by high-dimensional nonlinear optimization problems as well as ill-posed optimization problems arising in image processing, we consider a bilevel optimization model where we seek among the optimal solutions of the inner level problem, a solution that minimizes a secondary metric. Our goal is to address the high-dimensionality of the bilevel problem, and the nondifferentiability of the objective function. Minimal norm gradient, sequential averaging, and iterative regularization are some of the recent schemes developed for addressing the bilevel problem. But none of them address the high-dimensional structure and nondifferentiability. With this gap in the literature, we develop a randomized block coordinate iterative regularized gradient descent scheme (RB-IRG). We establish the convergence of the sequence generated by RB-IRG to the unique solution of the bilevel problem of interest. Furthermore, we derive a rate of convergence $\\\\mathcal{O} \\\\left(\\\\frac{1}{{k}^{0.5-\\\\delta}}\\\\right)$, with respect to the inner level objective function. We demonstrate the performance of RB-IRG in solving the ill-posed problems arising in image processing.',\n",
       "  'arxiv_id': 'abs/1809.10035v1',\n",
       "  'category': None},\n",
       " 'abs/1909.10270v1': {'id': None,\n",
       "  'title': 'Pose Estimation for Texture-less Shiny Objects in a Single RGB Image\\n  Using Synthetic Training Data',\n",
       "  'authors': ['Chen Chen', 'Xin Jiang', 'Weiguo Zhou', 'Yun-Hui Liu'],\n",
       "  'abstract': 'In the industrial domain, the pose estimation of multiple texture-less shiny parts is a valuable but challenging task. In this particular scenario, it is impractical to utilize keypoints or other texture information because most of them are not actual features of the target but the reflections of surroundings. Moreover, the similarity of color also poses a challenge in segmentation. In this article, we propose to divide the pose estimation process into three stages: object detection, features detection and pose optimization. A convolutional neural network was utilized to perform object detection. Concerning the reliability of surface texture, we leveraged the contour information for estimating pose. Since conventional contour-based methods are inapplicable to clustered metal parts due to the difficulties in segmentation, we use the dense discrete points along the metal part edges as semantic keypoints for contour detection. Afterward, we exploit both keypoint information and CAD model to calculate the 6D pose of each object in view. A typical implementation of deep learning methods not only requires a large amount of training data, but also relies on intensive human labor for labeling the datasets. Therefore, we propose an approach to generate datasets and label them automatically. Despite not using any real-world photos for training, a series of experiments showed that the algorithm built on synthetic data perform well in the real environment.',\n",
       "  'arxiv_id': 'abs/1909.10270v1',\n",
       "  'category': None},\n",
       " 'abs/1704.04826v1': {'id': None,\n",
       "  'title': 'Optical Tomographic Imaging for Breast Cancer Detection',\n",
       "  'authors': ['Wenxiang Cong', 'Xavier Intes', 'Ge Wang'],\n",
       "  'abstract': 'Diffuse optical breast imaging utilizes near-infrared (NIR) light propagation through tissues to assess the optical properties of tissue for the identification of abnormal tissue. This optical imaging approach is sensitive, cost-effective, and does not involve any ionizing radiation. However, the image reconstruction of diffuse optical tomography (DOT) is a nonlinear inverse problem and suffers from severe ill-posedness, especially in the cases of strong noise and incomplete data. In this paper, a novel image reconstruction method is proposed for the detection of breast cancer. This method split the image reconstruction problem into the localization of abnormal tissues and quantification of absorption variations. The localization of abnormal tissues is performed based on a new well-posed optimization model, which can be solved via differential evolution optimization method to achieve a stable image reconstruction. The quantification of abnormal absorption variations is then determined in localized regions of relatively small extents, which are potentially tumors. Consequently, the number of unknown absorption variables can be greatly reduced to overcome the underdetermined nature of diffuse optical tomography (DOT), allowing for accurate and stable reconstruction of the abnormal absorption variations in the breast. Numerical simulation experiments show that the image reconstruction method is stable and accurate for the identification of abnormal tissues, and robust against measurement noise of data.',\n",
       "  'arxiv_id': 'abs/1704.04826v1',\n",
       "  'category': None},\n",
       " 'abs/1901.00463v1': {'id': None,\n",
       "  'title': 'Improved Hyperspectral Unmixing With Endmember Variability Parametrized\\n  Using an Interpolated Scaling Tensor',\n",
       "  'authors': ['Ricardo Augusto Borsoi',\n",
       "   'Tales Imbiriba',\n",
       "   'José Carlos Moreira Bermudez'],\n",
       "  'abstract': 'Endmember (EM) variability has an important impact on the performance of hyperspectral image (HI) analysis algorithms. Recently, extended linear mixing models have been proposed to account for EM variability in the spectral unmixing (SU) problem. The direct use of these models has led to severely ill-posed optimization problems. Different regularization strategies have been considered to deal with this issue, but none so far has consistently exploited the information provided by the existence of multiple pure pixels often present in HIs. In this work, we propose to break the SU problem into a sequence of two problems. First, we use pure pixel information to estimate an interpolated tensor of scaling factors representing spectral variability. This is done by considering the spectral variability to be a smooth function over the HI and confining the energy of the scaling tensor to a low-rank structure. Afterwards, we solve a matrix-factorization problem to estimate the fractional abundances using the variability scaling factors estimated in the previous step, what leads to a significantly more well-posed problem. Simulation swith synthetic and real data attest the effectiveness of the proposed strategy.',\n",
       "  'arxiv_id': 'abs/1901.00463v1',\n",
       "  'category': None},\n",
       " 'abs/1908.06109v1': {'id': None,\n",
       "  'title': 'RIO: 3D Object Instance Re-Localization in Changing Indoor Environments',\n",
       "  'authors': ['Johanna Wald',\n",
       "   'Armen Avetisyan',\n",
       "   'Nassir Navab',\n",
       "   'Federico Tombari',\n",
       "   'Matthias Nießner'],\n",
       "  'abstract': 'In this work, we introduce the task of 3D object instance re-localization (RIO): given one or multiple objects in an RGB-D scan, we want to estimate their corresponding 6DoF poses in another 3D scan of the same environment taken at a later point in time. We consider RIO a particularly important task in 3D vision since it enables a wide range of practical applications, including AI-assistants or robots that are asked to find a specific object in a 3D scene. To address this problem, we first introduce 3RScan, a novel dataset and benchmark, which features 1482 RGB-D scans of 478 environments across multiple time steps. Each scene includes several objects whose positions change over time, together with ground truth annotations of object instances and their respective 6DoF mappings among re-scans. Automatically finding 6DoF object poses leads to a particular challenging feature matching task due to varying partial observations and changes in the surrounding context. To this end, we introduce a new data-driven approach that efficiently finds matching features using a fully-convolutional 3D correspondence network operating on multiple spatial scales. Combined with a 6DoF pose optimization, our method outperforms state-of-the-art baselines on our newly-established benchmark, achieving an accuracy of 30.58%.',\n",
       "  'arxiv_id': 'abs/1908.06109v1',\n",
       "  'category': None},\n",
       " 'abs/1912.08601v1': {'id': None,\n",
       "  'title': 'Kalman Filter Tuning with Bayesian Optimization',\n",
       "  'authors': ['Zhaozhong Chen',\n",
       "   'Nisar Ahmed',\n",
       "   'Simon Julier',\n",
       "   'Christoffer Heckman'],\n",
       "  'abstract': \"Many state estimation algorithms must be tuned given the state space process and observation models, the process and observation noise parameters must be chosen. Conventional tuning approaches rely on heuristic hand-tuning or gradient-based optimization techniques to minimize a performance cost function. However, the relationship between tuned noise values and estimator performance is highly nonlinear and stochastic. Therefore, the tuning solutions can easily get trapped in local minima, which can lead to poor choices of noise parameters and suboptimal estimator performance. This paper describes how Bayesian Optimization (BO) can overcome these issues. BO poses optimization as a Bayesian search problem for a stochastic ``black box'' cost function, where the goal is to search the solution space to maximize the probability of improving the current best solution. As such, BO offers a principled approach to optimization-based estimator tuning in the presence of local minima and performance stochasticity. While extended Kalman filters (EKFs) are the main focus of this work, BO can be similarly used to tune other related state space filters. The method presented here uses performance metrics derived from normalized innovation squared (NIS) filter residuals obtained via sensor data, which renders knowledge of ground-truth states unnecessary. The robustness, accuracy, and reliability of BO-based tuning is illustrated on practical nonlinear state estimation problems,losed-loop aero-robotic control.\",\n",
       "  'arxiv_id': 'abs/1912.08601v1',\n",
       "  'category': None},\n",
       " 'abs/2006.04847v1': {'id': None,\n",
       "  'title': 'Procrustean Orthogonal Sparse Hashing',\n",
       "  'authors': ['Mariano Tepper', 'Dipanjan Sengupta', 'Ted Willke'],\n",
       "  'abstract': 'Hashing is one of the most popular methods for similarity search because of its speed and efficiency. Dense binary hashing is prevalent in the literature. Recently, insect olfaction was shown to be structurally and functionally analogous to sparse hashing [6]. Here, we prove that this biological mechanism is the solution to a well-posed optimization problem. Furthermore, we show that orthogonality increases the accuracy of sparse hashing. Next, we present a novel method, Procrustean Orthogonal Sparse Hashing (POSH), that unifies these findings, learning an orthogonal transform from training data compatible with the sparse hashing mechanism. We provide theoretical evidence of the shortcomings of Optimal Sparse Lifting (OSL) [22] and BioHash [30], two related olfaction-inspired methods, and propose two new methods, Binary OSL and SphericalHash, to address these deficiencies. We compare POSH, Binary OSL, and SphericalHash to several state-of-the-art hashing methods and provide empirical results for the superiority of the proposed methods across a wide range of standard benchmarks and parameter settings.',\n",
       "  'arxiv_id': 'abs/2006.04847v1',\n",
       "  'category': None},\n",
       " 'abs/2007.00258v3': {'id': None,\n",
       "  'title': 'LIO-SAM: Tightly-coupled Lidar Inertial Odometry via Smoothing and\\n  Mapping',\n",
       "  'authors': ['Tixiao Shan',\n",
       "   'Brendan Englot',\n",
       "   'Drew Meyers',\n",
       "   'Wei Wang',\n",
       "   'Carlo Ratti',\n",
       "   'Daniela Rus'],\n",
       "  'abstract': \"We propose a framework for tightly-coupled lidar inertial odometry via smoothing and mapping, LIO-SAM, that achieves highly accurate, real-time mobile robot trajectory estimation and map-building. LIO-SAM formulates lidar-inertial odometry atop a factor graph, allowing a multitude of relative and absolute measurements, including loop closures, to be incorporated from different sources as factors into the system. The estimated motion from inertial measurement unit (IMU) pre-integration de-skews point clouds and produces an initial guess for lidar odometry optimization. The obtained lidar odometry solution is used to estimate the bias of the IMU. To ensure high performance in real-time, we marginalize old lidar scans for pose optimization, rather than matching lidar scans to a global map. Scan-matching at a local scale instead of a global scale significantly improves the real-time performance of the system, as does the selective introduction of keyframes, and an efficient sliding window approach that registers a new keyframe to a fixed-size set of prior ``sub-keyframes.'' The proposed method is extensively evaluated on datasets gathered from three platforms over various scales and environments.\",\n",
       "  'arxiv_id': 'abs/2007.00258v3',\n",
       "  'category': None},\n",
       " 'abs/2008.09042v1': {'id': None,\n",
       "  'title': 'Maxwell Parallel Imaging',\n",
       "  'authors': ['Matteo Alessandro Francavilla',\n",
       "   'Stamatios Lefkimmiatis',\n",
       "   'Jorge F. Villena',\n",
       "   'Athanasios G. Polimeridis'],\n",
       "  'abstract': 'Purpose: To develop a general framework for Parallel Imaging (PI) with the use of Maxwell regularization for the estimation of the sensitivity maps (SMs) and constrained optimization for the parameter-free image reconstruction.   Theory and Methods: Certain characteristics of both the SMs and the images are routinely used to regularize the otherwise ill-posed optimization-based joint reconstruction from highly accelerated PI data. In this paper we rely on a fundamental property of SMs--they are solutions of Maxwell equations-- we construct the subspace of all possible SM distributions supported in a given field-of-view, and we promote solutions of SMs that belong in this subspace. In addition, we propose a constrained optimization scheme for the image reconstruction, as a second step, once an accurate estimation of the SMs is available. The resulting method, dubbed Maxwell Parallel Imaging (MPI), works seamlessly for arbitrary sequences (both 2D and 3D) with any trajectory and minimal calibration signals.   Results: The effectiveness of MPI is illustrated for a wide range of datasets with various undersampling schemes, including radial, variable-density Poisson-disc, and Cartesian, and is compared against the state-of-the-art PI methods. Finally, we include some numerical experiments that demonstrate the memory footprint reduction of the constructed Maxwell basis with the help of tensor decomposition, thus allowing the use of MPI for full 3D image reconstructions.   Conclusions: The MPI framework provides a physics-inspired optimization method for the accurate and efficient image reconstruction from arbitrary accelerated scans.',\n",
       "  'arxiv_id': 'abs/2008.09042v1',\n",
       "  'category': None},\n",
       " 'abs/2009.04710v2': {'id': None,\n",
       "  'title': 'Robust Clustering with Normal Mixture Models: A Pseudo\\n  $β$-Likelihood Approach',\n",
       "  'authors': ['Soumya Chakraborty', 'Ayanendranath Basu', 'Abhik Ghosh'],\n",
       "  'abstract': 'As in other estimation scenarios, likelihood based estimation in the normal mixture set-up is highly non-robust against model misspecification and presence of outliers (apart from being an ill-posed optimization problem). We propose a robust alternative to the ordinary likelihood approach for this estimation problem which performs simultaneous estimation and data clustering and leads to subsequent anomaly detection. To invoke robustness, we follow, in spirit, the methodology based on the minimization of the density power divergence (or alternatively, the maximization of the $\\\\beta$-likelihood) under suitable constraints. An iteratively reweighted least squares approach has been followed in order to compute our estimators for the component means (or equivalently cluster centers) and component dispersion matrices which leads to simultaneous data clustering. Some exploratory techniques are also suggested for anomaly detection, a problem of great importance in the domain of statistics and machine learning. Existence and consistency of the estimators are established under the aforesaid constraints. We validate our method with simulation studies under different set-ups; it is seen to perform competitively or better compared to the popular existing methods like K-means and TCLUST, especially when the mixture components (i.e., the clusters) share regions with significant overlap or outlying clusters exist with small but non-negligible weights. Two real datasets are also used to illustrate the performance of our method in comparison with others along with an application in image processing. It is observed that our method detects the clusters with lower misclassification rates and successfully points out the outlying (anomalous) observations from these datasets.',\n",
       "  'arxiv_id': 'abs/2009.04710v2',\n",
       "  'category': None},\n",
       " 'abs/2104.07639v4': {'id': None,\n",
       "  'title': 'Robust Optimization for Multilingual Translation with Imbalanced Data',\n",
       "  'authors': ['Xian Li', 'Hongyu Gong'],\n",
       "  'abstract': 'Multilingual models are parameter-efficient and especially effective in improving low-resource languages by leveraging crosslingual transfer. Despite recent advance in massive multilingual translation with ever-growing model and data, how to effectively train multilingual models has not been well understood. In this paper, we show that a common situation in multilingual training, data imbalance among languages, poses optimization tension between high resource and low resource languages where the found multilingual solution is often sub-optimal for low resources. We show that common training method which upsamples low resources can not robustly optimize population loss with risks of either underfitting high resource languages or overfitting low resource ones. Drawing on recent findings on the geometry of loss landscape and its effect on generalization, we propose a principled optimization algorithm, Curvature Aware Task Scaling (CATS), which adaptively rescales gradients from different tasks with a meta objective of guiding multilingual training to low-curvature neighborhoods with uniformly low loss for all languages. We ran experiments on common benchmarks (TED, WMT and OPUS-100) with varying degrees of data imbalance. CATS effectively improved multilingual optimization and as a result demonstrated consistent gains on low resources ($+0.8$ to $+2.2$ BLEU) without hurting high resources. In addition, CATS is robust to overparameterization and large batch size training, making it a promising training method for massive multilingual models that truly improve low resource languages.',\n",
       "  'arxiv_id': 'abs/2104.07639v4',\n",
       "  'category': None},\n",
       " 'abs/2107.00822v1': {'id': None,\n",
       "  'title': 'F-LOAM: Fast LiDAR Odometry And Mapping',\n",
       "  'authors': ['Han Wang', 'Chen Wang', 'Chun-Lin Chen', 'Lihua Xie'],\n",
       "  'abstract': 'Simultaneous Localization and Mapping (SLAM) has wide robotic applications such as autonomous driving and unmanned aerial vehicles. Both computational efficiency and localization accuracy are of great importance towards a good SLAM system. Existing works on LiDAR based SLAM often formulate the problem as two modules: scan-to-scan match and scan-to-map refinement. Both modules are solved by iterative calculation which are computationally expensive. In this paper, we propose a general solution that aims to provide a computationally efficient and accurate framework for LiDAR based SLAM. Specifically, we adopt a non-iterative two-stage distortion compensation method to reduce the computational cost. For each scan input, the edge and planar features are extracted and matched to a local edge map and a local plane map separately, where the local smoothness is also considered for iterative pose optimization. Thorough experiments are performed to evaluate its performance in challenging scenarios, including localization for a warehouse Automated Guided Vehicle (AGV) and a public dataset on autonomous driving. The proposed method achieves a competitive localization accuracy with a processing rate of more than 10 Hz in the public dataset evaluation, which provides a good trade-off between performance and computational cost for practical applications.',\n",
       "  'arxiv_id': 'abs/2107.00822v1',\n",
       "  'category': None},\n",
       " 'abs/2109.05483v1': {'id': None,\n",
       "  'title': 'ART-SLAM: Accurate Real-Time 6DoF LiDAR SLAM',\n",
       "  'authors': ['Matteo Frosi', 'Matteo Matteucci'],\n",
       "  'abstract': 'Real-time six degree-of-freedom pose estimation with ground vehicles represents a relevant and well studied topic in robotics, due to its many applications, such as autonomous driving and 3D mapping. Although some systems exist already, they are either not accurate or they struggle in real-time setting. In this paper, we propose a fast, accurate and modular LiDAR SLAM system for both batch and online estimation. We first apply downsampling and outlier removal, to filter out noise and reduce the size of the input point clouds. Filtered clouds are then used for pose tracking and floor detection, to ground-optimize the estimated trajectory. The availability of a pre-tracker, working in parallel with the filtering process, allows to obtain pre-computed odometries, to be used as aids when performing tracking. Efficient loop closure and pose optimization, achieved through a g2o pose graph, are the last steps of the proposed SLAM pipeline. We compare the performance of our system with state-of-the-art point cloud based methods, LOAM, LeGO-LOAM, A-LOAM, LeGO-LOAM-BOR and HDL, and show that the proposed system achieves equal or better accuracy and can easily handle even cases without loops. The comparison is done evaluating the estimated trajectory displacement using the KITTI and RADIATE datasets.',\n",
       "  'arxiv_id': 'abs/2109.05483v1',\n",
       "  'category': None},\n",
       " 'abs/2109.10115v3': {'id': None,\n",
       "  'title': 'StereOBJ-1M: Large-scale Stereo Image Dataset for 6D Object Pose\\n  Estimation',\n",
       "  'authors': ['Xingyu Liu', 'Shun Iwase', 'Kris M. Kitani'],\n",
       "  'abstract': 'We present a large-scale stereo RGB image object pose estimation dataset named the $\\\\textbf{StereOBJ-1M}$ dataset. The dataset is designed to address challenging cases such as object transparency, translucency, and specular reflection, in addition to the common challenges of occlusion, symmetry, and variations in illumination and environments. In order to collect data of sufficient scale for modern deep learning models, we propose a novel method for efficiently annotating pose data in a multi-view fashion that allows data capturing in complex and flexible environments. Fully annotated with 6D object poses, our dataset contains over 393K frames and over 1.5M annotations of 18 objects recorded in 182 scenes constructed in 11 different environments. The 18 objects include 8 symmetric objects, 7 transparent objects, and 8 reflective objects. We benchmark two state-of-the-art pose estimation frameworks on StereOBJ-1M as baselines for future work. We also propose a novel object-level pose optimization method for computing 6D pose from keypoint predictions in multiple images. Project website: https://sites.google.com/view/stereobj-1m.',\n",
       "  'arxiv_id': 'abs/2109.10115v3',\n",
       "  'category': None},\n",
       " 'abs/2109.11282v1': {'id': None,\n",
       "  'title': 'Unbiased Loss Functions for Multilabel Classification with Missing\\n  Labels',\n",
       "  'authors': ['Erik Schultheis', 'Rohit Babbar'],\n",
       "  'abstract': 'This paper considers binary and multilabel classification problems in a setting where labels are missing independently and with a known rate. Missing labels are a ubiquitous phenomenon in extreme multi-label classification (XMC) tasks, such as matching Wikipedia articles to a small subset out of the hundreds of thousands of possible tags, where no human annotator can possibly check the validity of all the negative samples. For this reason, propensity-scored precision -- an unbiased estimate for precision-at-k under a known noise model -- has become one of the standard metrics in XMC. Few methods take this problem into account already during the training phase, and all are limited to loss functions that can be decomposed into a sum of contributions from each individual label. A typical approach to training is to reduce the multilabel problem into a series of binary or multiclass problems, and it has been shown that if the surrogate task should be consistent for optimizing recall, the resulting loss function is not decomposable over labels. Therefore, this paper derives the unique unbiased estimators for the different multilabel reductions, including the non-decomposable ones. These estimators suffer from increased variance and may lead to ill-posed optimization problems, which we address by switching to convex upper-bounds. The theoretical considerations are further supplemented by an experimental study showing that the switch to unbiased estimators significantly alters the bias-variance trade-off and may thus require stronger regularization, which in some cases can negate the benefits of unbiased estimation.',\n",
       "  'arxiv_id': 'abs/2109.11282v1',\n",
       "  'category': None},\n",
       " 'abs/2203.12870v3': {'id': None,\n",
       "  'title': 'RNNPose: Recurrent 6-DoF Object Pose Refinement with Robust\\n  Correspondence Field Estimation and Pose Optimization',\n",
       "  'authors': ['Yan Xu',\n",
       "   'Kwan-Yee Lin',\n",
       "   'Guofeng Zhang',\n",
       "   'Xiaogang Wang',\n",
       "   'Hongsheng Li'],\n",
       "  'abstract': '6-DoF object pose estimation from a monocular image is challenging, and a post-refinement procedure is generally needed for high-precision estimation. In this paper, we propose a framework based on a recurrent neural network (RNN) for object pose refinement, which is robust to erroneous initial poses and occlusions. During the recurrent iterations, object pose refinement is formulated as a non-linear least squares problem based on the estimated correspondence field (between a rendered image and the observed image). The problem is then solved by a differentiable Levenberg-Marquardt (LM) algorithm enabling end-to-end training. The correspondence field estimation and pose refinement are conducted alternatively in each iteration to recover the object poses. Furthermore, to improve the robustness to occlusion, we introduce a consistency-check mechanism based on the learned descriptors of the 3D model and observed 2D images, which downweights the unreliable correspondences during pose optimization. Extensive experiments on LINEMOD, Occlusion-LINEMOD, and YCB-Video datasets validate the effectiveness of our method and demonstrate state-of-the-art performance.',\n",
       "  'arxiv_id': 'abs/2203.12870v3',\n",
       "  'category': None},\n",
       " 'abs/2205.14313v3': {'id': None,\n",
       "  'title': 'Learning to Use Chopsticks in Diverse Gripping Styles',\n",
       "  'authors': ['Zeshi Yang', 'KangKang Yin', 'Libin Liu'],\n",
       "  'abstract': 'Learning dexterous manipulation skills is a long-standing challenge in computer graphics and robotics, especially when the task involves complex and delicate interactions between the hands, tools and objects. In this paper, we focus on chopsticks-based object relocation tasks, which are common yet demanding. The key to successful chopsticks skills is steady gripping of the sticks that also supports delicate maneuvers. We automatically discover physically valid chopsticks holding poses by Bayesian Optimization (BO) and Deep Reinforcement Learning (DRL), which works for multiple gripping styles and hand morphologies without the need of example data. Given as input the discovered gripping poses and desired objects to be moved, we build physics-based hand controllers to accomplish relocation tasks in two stages. First, kinematic trajectories are synthesized for the chopsticks and hand in a motion planning stage. The key components of our motion planner include a grasping model to select suitable chopsticks configurations for grasping the object, and a trajectory optimization module to generate collision-free chopsticks trajectories. Then we train physics-based hand controllers through DRL again to track the desired kinematic trajectories produced by the motion planner. We demonstrate the capabilities of our framework by relocating objects of various shapes and sizes, in diverse gripping styles and holding positions for multiple hand morphologies. Our system achieves faster learning speed and better control robustness, when compared to vanilla systems that attempt to learn chopstick-based skills without a gripping pose optimization module and/or without a kinematic motion planner.',\n",
       "  'arxiv_id': 'abs/2205.14313v3',\n",
       "  'category': None},\n",
       " 'abs/2207.12620v1': {'id': None,\n",
       "  'title': 'Large-displacement 3D Object Tracking with Hybrid Non-local Optimization',\n",
       "  'authors': ['Xuhui Tian', 'Xinran Lin', 'Fan Zhong', 'Xueying Qin'],\n",
       "  'abstract': 'Optimization-based 3D object tracking is known to be precise and fast, but sensitive to large inter-frame displacements. In this paper we propose a fast and effective non-local 3D tracking method. Based on the observation that erroneous local minimum are mostly due to the out-of-plane rotation, we propose a hybrid approach combining non-local and local optimizations for different parameters, resulting in efficient non-local search in the 6D pose space. In addition, a precomputed robust contour-based tracking method is proposed for the pose optimization. By using long search lines with multiple candidate correspondences, it can adapt to different frame displacements without the need of coarse-to-fine search. After the pre-computation, pose updates can be conducted very fast, enabling the non-local optimization to run in real time. Our method outperforms all previous methods for both small and large displacements. For large displacements, the accuracy is greatly improved ($81.7\\\\% \\\\;\\\\text{v.s.}\\\\; 19.4\\\\%$). At the same time, real-time speed ($>$50fps) can be achieved with only CPU. The source code is available at \\\\url{https://github.com/cvbubbles/nonlocal-3dtracking}.',\n",
       "  'arxiv_id': 'abs/2207.12620v1',\n",
       "  'category': None},\n",
       " 'abs/2208.06933v1': {'id': None,\n",
       "  'title': 'Visual Localization via Few-Shot Scene Region Classification',\n",
       "  'authors': ['Siyan Dong',\n",
       "   'Shuzhe Wang',\n",
       "   'Yixin Zhuang',\n",
       "   'Juho Kannala',\n",
       "   'Marc Pollefeys',\n",
       "   'Baoquan Chen'],\n",
       "  'abstract': 'Visual (re)localization addresses the problem of estimating the 6-DoF (Degree of Freedom) camera pose of a query image captured in a known scene, which is a key building block of many computer vision and robotics applications. Recent advances in structure-based localization solve this problem by memorizing the mapping from image pixels to scene coordinates with neural networks to build 2D-3D correspondences for camera pose optimization. However, such memorization requires training by amounts of posed images in each scene, which is heavy and inefficient. On the contrary, few-shot images are usually sufficient to cover the main regions of a scene for a human operator to perform visual localization. In this paper, we propose a scene region classification approach to achieve fast and effective scene memorization with few-shot images. Our insight is leveraging a) pre-learned feature extractor, b) scene region classifier, and c) meta-learning strategy to accelerate training while mitigating overfitting. We evaluate our method on both indoor and outdoor benchmarks. The experiments validate the effectiveness of our method in the few-shot setting, and the training time is significantly reduced to only a few minutes. Code available at: \\\\url{https://github.com/siyandong/SRC}',\n",
       "  'arxiv_id': 'abs/2208.06933v1',\n",
       "  'category': None},\n",
       " 'abs/2209.08498v1': {'id': None,\n",
       "  'title': 'LATITUDE: Robotic Global Localization with Truncated Dynamic Low-pass\\n  Filter in City-scale NeRF',\n",
       "  'authors': ['Zhenxin Zhu',\n",
       "   'Yuantao Chen',\n",
       "   'Zirui Wu',\n",
       "   'Chao Hou',\n",
       "   'Yongliang Shi',\n",
       "   'Chuxuan Li',\n",
       "   'Pengfei Li',\n",
       "   'Hao Zhao',\n",
       "   'Guyue Zhou'],\n",
       "  'abstract': 'Neural Radiance Fields (NeRFs) have made great success in representing complex 3D scenes with high-resolution details and efficient memory. Nevertheless, current NeRF-based pose estimators have no initial pose prediction and are prone to local optima during optimization. In this paper, we present LATITUDE: Global Localization with Truncated Dynamic Low-pass Filter, which introduces a two-stage localization mechanism in city-scale NeRF. In place recognition stage, we train a regressor through images generated from trained NeRFs, which provides an initial value for global localization. In pose optimization stage, we minimize the residual between the observed image and rendered image by directly optimizing the pose on tangent plane. To avoid convergence to local optimum, we introduce a Truncated Dynamic Low-pass Filter (TDLF) for coarse-to-fine pose registration. We evaluate our method on both synthetic and real-world data and show its potential applications for high-precision navigation in large-scale city scenes. Codes and data will be publicly available at https://github.com/jike5/LATITUDE.',\n",
       "  'arxiv_id': 'abs/2209.08498v1',\n",
       "  'category': None},\n",
       " 'abs/1903.08789v1': {'id': None,\n",
       "  'title': 'Interpreting Neural Networks Using Flip Points',\n",
       "  'authors': ['Roozbeh Yousefzadeh', \"Dianne P. O'Leary\"],\n",
       "  'abstract': 'Neural networks have been criticized for their lack of easy interpretation, which undermines confidence in their use for important applications. Here, we introduce a novel technique, interpreting a trained neural network by investigating its flip points. A flip point is any point that lies on the boundary between two output classes: e.g. for a neural network with a binary yes/no output, a flip point is any input that generates equal scores for \"yes\" and \"no\". The flip point closest to a given input is of particular importance, and this point is the solution to a well-posed optimization problem. This paper gives an overview of the uses of flip points and how they are computed. Through results on standard datasets, we demonstrate how flip points can be used to provide detailed interpretation of the output produced by a neural network. Moreover, for a given input, flip points enable us to measure confidence in the correctness of outputs much more effectively than softmax score. They also identify influential features of the inputs, identify bias, and find changes in the input that change the output of the model. We show that distance between an input and the closest flip point identifies the most influential points in the training data. Using principal component analysis (PCA) and rank-revealing QR factorization (RR-QR), the set of directions from each training input to its closest flip point provides explanations of how a trained neural network processes an entire dataset: what features are most important for classification into a given class, which features are most responsible for particular misclassifications, how an adversary might fool the network, etc. Although we investigate flip points for neural networks, their usefulness is actually model-agnostic.',\n",
       "  'arxiv_id': 'abs/1903.08789v1',\n",
       "  'category': None},\n",
       " 'abs/2001.00682v1': {'id': None,\n",
       "  'title': 'Auditing and Debugging Deep Learning Models via Decision Boundaries:\\n  Individual-level and Group-level Analysis',\n",
       "  'authors': ['Roozbeh Yousefzadeh', \"Dianne P. O'Leary\"],\n",
       "  'abstract': 'Deep learning models have been criticized for their lack of easy interpretation, which undermines confidence in their use for important applications. Nevertheless, they are consistently utilized in many applications, consequential to humans\\' lives, mostly because of their better performance. Therefore, there is a great need for computational methods that can explain, audit, and debug such models. Here, we use flip points to accomplish these goals for deep learning models with continuous output scores (e.g., computed by softmax), used in social applications. A flip point is any point that lies on the boundary between two output classes: e.g. for a model with a binary yes/no output, a flip point is any input that generates equal scores for \"yes\" and \"no\". The flip point closest to a given input is of particular importance because it reveals the least changes in the input that would change a model\\'s classification, and we show that it is the solution to a well-posed optimization problem. Flip points also enable us to systematically study the decision boundaries of a deep learning classifier. The resulting insight into the decision boundaries of a deep model can clearly explain the model\\'s output on the individual-level, via an explanation report that is understandable by non-experts. We also develop a procedure to understand and audit model behavior towards groups of people. Flip points can also be used to alter the decision boundaries in order to improve undesirable behaviors. We demonstrate our methods by investigating several models trained on standard datasets used in social applications of machine learning. We also identify the features that are most responsible for particular classifications and misclassifications.',\n",
       "  'arxiv_id': 'abs/2001.00682v1',\n",
       "  'category': None},\n",
       " 'abs/2102.07560v3': {'id': None,\n",
       "  'title': 'Bounds for the extremal eigenvalues of gain Laplacian matrices',\n",
       "  'authors': ['M. Rajesh Kannan', 'Navish Kumar', 'Shivaramakrishna Pragada'],\n",
       "  'abstract': \"A complex unit gain graph ($\\\\mathbb{T}$-gain graph), $\\\\Phi = (G, \\\\varphi)$ is a graph where the function $\\\\varphi$ assigns a unit complex number to each orientation of an edge of $G$, and its inverse is assigned to the opposite orientation. A $ \\\\mathbb{T} $-gain graph $\\\\Phi$ is balanced if the product of the edge gains of each cycle (with a fixed orientation) is $1$. Signed graphs are special cases of $\\\\mathbb{T}$-gain graphs.   The adjacency matrix of $\\\\Phi$, denoted by $ \\\\mathbf{A}(\\\\Phi)$ is defined canonically. The gain Laplacian for $\\\\Phi$ is defined as $\\\\mathbf{L}(\\\\Phi) = \\\\mathbf{D}(\\\\Phi) - \\\\mathbf{A}(\\\\Phi)$, where $\\\\mathbf{D}(\\\\Phi)$ is the diagonal matrix with diagonal entries are the degrees of the vertices of $G$. The minimum number of vertices (resp., edges) to be deleted from $\\\\Phi$ in order to get a balanced gain graph the frustration number (resp, frustration index). We show that frustration number and frustration index are bounded below by the smallest eigenvalue of $\\\\mathbf{L}(\\\\Phi)$. We provide several lower and upper bounds for extremal eigenvalues of $\\\\mathbf{L}(\\\\Phi)$ in terms of different graph parameters such as the number of edges, vertex degrees, and average $2$-degrees. The signed graphs are particular cases of the $\\\\mathbb{T}$-gain graphs, all the bounds established in paper hold for signed graphs. Most of the bounds established here are new for signed graphs. Finally, we perform comparative analysis for all the obtained bounds in the paper with the state-of-the-art bounds available in the literature for randomly generated Erd\\\\H{o}s-Re\\\\'yni graphs.   Some of the major highlights of our paper are the gain-dependent bounds, limit convergence of the bounds to the extremal eigenvalues, and optimal extremal bounds obtained by posing optimization problems to achieve the best possible bounds.\",\n",
       "  'arxiv_id': 'abs/2102.07560v3',\n",
       "  'category': None},\n",
       " 'abs/2104.03176v2': {'id': None,\n",
       "  'title': 'On Self-Contact and Human Pose',\n",
       "  'authors': ['Lea Müller',\n",
       "   'Ahmed A. A. Osman',\n",
       "   'Siyu Tang',\n",
       "   'Chun-Hao P. Huang',\n",
       "   'Michael J. Black'],\n",
       "  'abstract': 'People touch their face 23 times an hour, they cross their arms and legs, put their hands on their hips, etc. While many images of people contain some form of self-contact, current 3D human pose and shape (HPS) regression methods typically fail to estimate this contact. To address this, we develop new datasets and methods that significantly improve human pose estimation with self-contact. First, we create a dataset of 3D Contact Poses (3DCP) containing SMPL-X bodies fit to 3D scans as well as poses from AMASS, which we refine to ensure good contact. Second, we leverage this to create the Mimic-The-Pose (MTP) dataset of images, collected via Amazon Mechanical Turk, containing people mimicking the 3DCP poses with selfcontact. Third, we develop a novel HPS optimization method, SMPLify-XMC, that includes contact constraints and uses the known 3DCP body pose during fitting to create near ground-truth poses for MTP images. Fourth, for more image variety, we label a dataset of in-the-wild images with Discrete Self-Contact (DSC) information and use another new optimization method, SMPLify-DC, that exploits discrete contacts during pose optimization. Finally, we use our datasets during SPIN training to learn a new 3D human pose regressor, called TUCH (Towards Understanding Contact in Humans). We show that the new self-contact training data significantly improves 3D human pose estimates on withheld test data and existing datasets like 3DPW. Not only does our method improve results for self-contact poses, but it also improves accuracy for non-contact poses. The code and data are available for research purposes at https://tuch.is.tue.mpg.de.',\n",
       "  'arxiv_id': 'abs/2104.03176v2',\n",
       "  'category': None},\n",
       " 'abs/2006.06666v3': {'id': None,\n",
       "  'title': 'VirTex: Learning Visual Representations from Textual Annotations',\n",
       "  'authors': ['Karan Desai', 'Justin Johnson'],\n",
       "  'abstract': 'The de-facto approach to many vision tasks is to start from pretrained visual representations, typically learned via supervised training on ImageNet. Recent methods have explored unsupervised pretraining to scale to vast quantities of unlabeled images. In contrast, we aim to learn high-quality visual representations from fewer images. To this end, we revisit supervised pretraining, and seek data-efficient alternatives to classification-based pretraining. We propose VirTex -- a pretraining approach using semantically dense captions to learn visual representations. We train convolutional networks from scratch on COCO Captions, and transfer them to downstream recognition tasks including image classification, object detection, and instance segmentation. On all tasks, VirTex yields features that match or exceed those learned on ImageNet -- supervised or unsupervised -- despite using up to ten times fewer images.',\n",
       "  'arxiv_id': 'abs/2006.06666v3',\n",
       "  'category': None},\n",
       " 'abs/2105.07576v1': {'id': None,\n",
       "  'title': 'Rethinking \"Batch\" in BatchNorm',\n",
       "  'authors': ['Yuxin Wu', 'Justin Johnson'],\n",
       "  'abstract': 'BatchNorm is a critical building block in modern convolutional neural networks. Its unique property of operating on \"batches\" instead of individual samples introduces significantly different behaviors from most other operations in deep learning. As a result, it leads to many hidden caveats that can negatively impact model\\'s performance in subtle ways. This paper thoroughly reviews such problems in visual recognition tasks, and shows that a key to address them is to rethink different choices in the concept of \"batch\" in BatchNorm. By presenting these caveats and their mitigations, we hope this review can help researchers use BatchNorm more effectively.',\n",
       "  'arxiv_id': 'abs/2105.07576v1',\n",
       "  'category': None},\n",
       " 'abs/2106.13933v1': {'id': None,\n",
       "  'title': 'Inverting and Understanding Object Detectors',\n",
       "  'authors': ['Ang Cao', 'Justin Johnson'],\n",
       "  'abstract': 'As a core problem in computer vision, the performance of object detection has improved drastically in the past few years. Despite their impressive performance, object detectors suffer from a lack of interpretability. Visualization techniques have been developed and widely applied to introspect the decisions made by other kinds of deep learning models; however, visualizing object detectors has been underexplored. In this paper, we propose using inversion as a primary tool to understand modern object detectors and develop an optimization-based approach to layout inversion, allowing us to generate synthetic images recognized by trained detectors as containing a desired configuration of objects. We reveal intriguing properties of detectors by applying our layout inversion technique to a variety of modern object detectors, and further investigate them via validation experiments: they rely on qualitatively different features for classification and regression; they learn canonical motifs of commonly co-occurring objects; they use diff erent visual cues to recognize objects of varying sizes. We hope our insights can help practitioners improve object detectors.',\n",
       "  'arxiv_id': 'abs/2106.13933v1',\n",
       "  'category': None},\n",
       " 'abs/2211.16421v1': {'id': None,\n",
       "  'title': 'RGB no more: Minimally-decoded JPEG Vision Transformers',\n",
       "  'authors': ['Jeongsoo Park', 'Justin Johnson'],\n",
       "  'abstract': 'Most neural networks for computer vision are designed to infer using RGB images. However, these RGB images are commonly encoded in JPEG before saving to disk; decoding them imposes an unavoidable overhead for RGB networks. Instead, our work focuses on training Vision Transformers (ViT) directly from the encoded features of JPEG. This way, we can avoid most of the decoding overhead, accelerating data load. Existing works have studied this aspect but they focus on CNNs. Due to how these encoded features are structured, CNNs require heavy modification to their architecture to accept such data. Here, we show that this is not the case for ViTs. In addition, we tackle data augmentation directly on these encoded features, which to our knowledge, has not been explored in-depth for training in this setting. With these two improvements -- ViT and data augmentation -- we show that our ViT-Ti model achieves up to 39.2% faster training and 17.9% faster inference with no accuracy loss compared to the RGB counterpart.',\n",
       "  'arxiv_id': 'abs/2211.16421v1',\n",
       "  'category': None},\n",
       " 'abs/1506.02078v2': {'id': None,\n",
       "  'title': 'Visualizing and Understanding Recurrent Networks',\n",
       "  'authors': ['Andrej Karpathy', 'Justin Johnson', 'Li Fei-Fei'],\n",
       "  'abstract': 'Recurrent Neural Networks (RNNs), and specifically a variant with Long Short-Term Memory (LSTM), are enjoying renewed interest as a result of successful applications in a wide range of machine learning problems that involve sequential data. However, while LSTMs provide exceptional results in practice, the source of their performance and their limitations remain rather poorly understood. Using character-level language models as an interpretable testbed, we aim to bridge this gap by providing an analysis of their representations, predictions and error types. In particular, our experiments reveal the existence of interpretable cells that keep track of long-range dependencies such as line lengths, quotes and brackets. Moreover, our comparative analysis with finite horizon n-gram models traces the source of the LSTM improvements to long-range structural dependencies. Finally, we provide analysis of the remaining errors and suggests areas for further study.',\n",
       "  'arxiv_id': 'abs/1506.02078v2',\n",
       "  'category': None},\n",
       " 'abs/1511.07571v1': {'id': None,\n",
       "  'title': 'DenseCap: Fully Convolutional Localization Networks for Dense Captioning',\n",
       "  'authors': ['Justin Johnson', 'Andrej Karpathy', 'Li Fei-Fei'],\n",
       "  'abstract': 'We introduce the dense captioning task, which requires a computer vision system to both localize and describe salient regions in images in natural language. The dense captioning task generalizes object detection when the descriptions consist of a single word, and Image Captioning when one predicted region covers the full image. To address the localization and description task jointly we propose a Fully Convolutional Localization Network (FCLN) architecture that processes an image with a single, efficient forward pass, requires no external regions proposals, and can be trained end-to-end with a single round of optimization. The architecture is composed of a Convolutional Network, a novel dense localization layer, and Recurrent Neural Network language model that generates the label sequences. We evaluate our network on the Visual Genome dataset, which comprises 94,000 images and 4,100,000 region-grounded captions. We observe both speed and accuracy improvements over baselines based on current state of the art approaches in both generation and retrieval settings.',\n",
       "  'arxiv_id': 'abs/1511.07571v1',\n",
       "  'category': None},\n",
       " 'abs/1603.08155v1': {'id': None,\n",
       "  'title': 'Perceptual Losses for Real-Time Style Transfer and Super-Resolution',\n",
       "  'authors': ['Justin Johnson', 'Alexandre Alahi', 'Li Fei-Fei'],\n",
       "  'abstract': 'We consider image transformation problems, where an input image is transformed into an output image. Recent methods for such problems typically train feed-forward convolutional neural networks using a \\\\emph{per-pixel} loss between the output and ground-truth images. Parallel work has shown that high-quality images can be generated by defining and optimizing \\\\emph{perceptual} loss functions based on high-level features extracted from pretrained networks. We combine the benefits of both approaches, and propose the use of perceptual loss functions for training feed-forward networks for image transformation tasks. We show results on image style transfer, where a feed-forward network is trained to solve the optimization problem proposed by Gatys et al in real-time. Compared to the optimization-based method, our network gives similar qualitative results but is three orders of magnitude faster. We also experiment with single-image super-resolution, where replacing a per-pixel loss with a perceptual loss gives visually pleasing results.',\n",
       "  'arxiv_id': 'abs/1603.08155v1',\n",
       "  'category': None},\n",
       " 'abs/1803.11361v1': {'id': None,\n",
       "  'title': 'DDRprog: A CLEVR Differentiable Dynamic Reasoning Programmer',\n",
       "  'authors': ['Joseph Suarez', 'Justin Johnson', 'Fei-Fei Li'],\n",
       "  'abstract': 'We present a novel Dynamic Differentiable Reasoning (DDR) framework for jointly learning branching programs and the functions composing them; this resolves a significant nondifferentiability inhibiting recent dynamic architectures. We apply our framework to two settings in two highly compact and data efficient architectures: DDRprog for CLEVR Visual Question Answering and DDRstack for reverse Polish notation expression evaluation. DDRprog uses a recurrent controller to jointly predict and execute modular neural programs that directly correspond to the underlying question logic; it explicitly forks subprocesses to handle logical branching. By effectively leveraging additional structural supervision, we achieve a large improvement over previous approaches in subtask consistency and a small improvement in overall accuracy. We further demonstrate the benefits of structural supervision in the RPN setting: the inclusion of a stack assumption in DDRstack allows our approach to generalize to long expressions where an LSTM fails the task.',\n",
       "  'arxiv_id': 'abs/1803.11361v1',\n",
       "  'category': None},\n",
       " 'abs/1804.01622v1': {'id': None,\n",
       "  'title': 'Image Generation from Scene Graphs',\n",
       "  'authors': ['Justin Johnson', 'Agrim Gupta', 'Li Fei-Fei'],\n",
       "  'abstract': \"To truly understand the visual world our models should be able not only to recognize images but also generate them. To this end, there has been exciting recent progress on generating images from natural language descriptions. These methods give stunning results on limited domains such as descriptions of birds or flowers, but struggle to faithfully reproduce complex sentences with many objects and relationships. To overcome this limitation we propose a method for generating images from scene graphs, enabling explicitly reasoning about objects and their relationships. Our model uses graph convolution to process input graphs, computes a scene layout by predicting bounding boxes and segmentation masks for objects, and converts the layout to an image with a cascaded refinement network. The network is trained adversarially against a pair of discriminators to ensure realistic outputs. We validate our approach on Visual Genome and COCO-Stuff, where qualitative results, ablations, and user studies demonstrate our method's ability to generate complex images with multiple objects.\",\n",
       "  'arxiv_id': 'abs/1804.01622v1',\n",
       "  'category': None},\n",
       " 'abs/1611.06607v2': {'id': None,\n",
       "  'title': 'A Hierarchical Approach for Generating Descriptive Image Paragraphs',\n",
       "  'authors': ['Jonathan Krause',\n",
       "   'Justin Johnson',\n",
       "   'Ranjay Krishna',\n",
       "   'Li Fei-Fei'],\n",
       "  'abstract': 'Recent progress on image captioning has made it possible to generate novel sentences describing images in natural language, but compressing an image into a single sentence can describe visual content in only coarse detail. While one new captioning approach, dense captioning, can potentially describe images in finer levels of detail by captioning many regions within an image, it in turn is unable to produce a coherent story for an image. In this paper we overcome these limitations by generating entire paragraphs for describing images, which can tell detailed, unified stories. We develop a model that decomposes both images and paragraphs into their constituent parts, detecting semantic regions in images and using a hierarchical recurrent neural network to reason about language. Linguistic analysis confirms the complexity of the paragraph generation task, and thorough experiments on a new dataset of image and paragraph pairs demonstrate the effectiveness of our approach.',\n",
       "  'arxiv_id': 'abs/1611.06607v2',\n",
       "  'category': None},\n",
       " 'abs/1906.02739v2': {'id': None,\n",
       "  'title': 'Mesh R-CNN',\n",
       "  'authors': ['Georgia Gkioxari', 'Jitendra Malik', 'Justin Johnson'],\n",
       "  'abstract': \"Rapid advances in 2D perception have led to systems that accurately detect objects in real-world images. However, these systems make predictions in 2D, ignoring the 3D structure of the world. Concurrently, advances in 3D shape prediction have mostly focused on synthetic benchmarks and isolated objects. We unify advances in these two areas. We propose a system that detects objects in real-world images and produces a triangle mesh giving the full 3D shape of each detected object. Our system, called Mesh R-CNN, augments Mask R-CNN with a mesh prediction branch that outputs meshes with varying topological structure by first predicting coarse voxel representations which are converted to meshes and refined with a graph convolution network operating over the mesh's vertices and edges. We validate our mesh prediction branch on ShapeNet, where we outperform prior work on single-image shape prediction. We then deploy our full Mesh R-CNN system on Pix3D, where we jointly detect objects and predict their 3D shapes.\",\n",
       "  'arxiv_id': 'abs/1906.02739v2',\n",
       "  'category': None},\n",
       " 'abs/1911.09655v1': {'id': None,\n",
       "  'title': 'Temporal Reasoning via Audio Question Answering',\n",
       "  'authors': ['Haytham M. Fayek', 'Justin Johnson'],\n",
       "  'abstract': 'Multimodal question answering tasks can be used as proxy tasks to study systems that can perceive and reason about the world. Answering questions about different types of input modalities stresses different aspects of reasoning such as visual reasoning, reading comprehension, story understanding, or navigation. In this paper, we use the task of Audio Question Answering (AQA) to study the temporal reasoning abilities of machine learning models. To this end, we introduce the Diagnostic Audio Question Answering (DAQA) dataset comprising audio sequences of natural sound events and programmatically generated questions and answers that probe various aspects of temporal reasoning. We adapt several recent state-of-the-art methods for visual question answering to the AQA task, and use DAQA to demonstrate that they perform poorly on questions that require in-depth temporal reasoning. Finally, we propose a new model, Multiple Auxiliary Controllers for Linear Modulation (MALiMo) that extends the recent Feature-wise Linear Modulation (FiLM) model and significantly improves its temporal reasoning capabilities. We envisage DAQA to foster research on AQA and temporal reasoning and MALiMo a step towards models for AQA.',\n",
       "  'arxiv_id': 'abs/1911.09655v1',\n",
       "  'category': None},\n",
       " 'abs/1912.08804v2': {'id': None,\n",
       "  'title': 'SynSin: End-to-end View Synthesis from a Single Image',\n",
       "  'authors': ['Olivia Wiles',\n",
       "   'Georgia Gkioxari',\n",
       "   'Richard Szeliski',\n",
       "   'Justin Johnson'],\n",
       "  'abstract': 'Single image view synthesis allows for the generation of new views of a scene given a single input image. This is challenging, as it requires comprehensively understanding the 3D scene from a single image. As a result, current methods typically use multiple images, train on ground-truth depth, or are limited to synthetic data. We propose a novel end-to-end model for this task; it is trained on real images without any ground-truth 3D information. To this end, we introduce a novel differentiable point cloud renderer that is used to transform a latent 3D point cloud of features into the target view. The projected features are decoded by our refinement network to inpaint missing regions and generate a realistic output image. The 3D component inside of our generative model allows for interpretable manipulation of the latent feature space at test time, e.g. we can animate trajectories from a single image. Unlike prior work, we can generate high resolution images and generalise to other input resolutions. We outperform baselines and prior work on the Matterport, Replica, and RealEstate10K datasets.',\n",
       "  'arxiv_id': 'abs/1912.08804v2',\n",
       "  'category': None},\n",
       " 'abs/2106.00677v1': {'id': None,\n",
       "  'title': 'Bootstrap Your Own Correspondences',\n",
       "  'authors': ['Mohamed El Banani', 'Justin Johnson'],\n",
       "  'abstract': \"Geometric feature extraction is a crucial component of point cloud registration pipelines. Recent work has demonstrated how supervised learning can be leveraged to learn better and more compact 3D features. However, those approaches' reliance on ground-truth annotation limits their scalability. We propose BYOC: a self-supervised approach that learns visual and geometric features from RGB-D video without relying on ground-truth pose or correspondence. Our key observation is that randomly-initialized CNNs readily provide us with good correspondences; allowing us to bootstrap the learning of both visual and geometric features. Our approach combines classic ideas from point cloud registration with more recent representation learning approaches. We evaluate our approach on indoor scene datasets and find that our method outperforms traditional and learned descriptors, while being competitive with current state-of-the-art supervised approaches.\",\n",
       "  'arxiv_id': 'abs/2106.00677v1',\n",
       "  'category': None},\n",
       " 'abs/2108.05892v1': {'id': None,\n",
       "  'title': 'PixelSynth: Generating a 3D-Consistent Experience from a Single Image',\n",
       "  'authors': ['Chris Rockwell', 'David F. Fouhey', 'Justin Johnson'],\n",
       "  'abstract': 'Recent advancements in differentiable rendering and 3D reasoning have driven exciting results in novel view synthesis from a single image. Despite realistic results, methods are limited to relatively small view change. In order to synthesize immersive scenes, models must also be able to extrapolate. We present an approach that fuses 3D reasoning with autoregressive modeling to outpaint large view changes in a 3D-consistent manner, enabling scene synthesis. We demonstrate considerable improvement in single image large-angle view synthesis results compared to a variety of methods and possible variants across simulated and real datasets. In addition, we show increased 3D consistency compared to alternative accumulation methods. Project website: https://crockwell.github.io/pixelsynth/',\n",
       "  'arxiv_id': 'abs/2108.05892v1',\n",
       "  'category': None},\n",
       " 'abs/2111.11431v1': {'id': None,\n",
       "  'title': 'RedCaps: web-curated image-text data created by the people, for the\\n  people',\n",
       "  'authors': ['Karan Desai', 'Gaurav Kaul', 'Zubin Aysola', 'Justin Johnson'],\n",
       "  'abstract': 'Large datasets of paired images and text have become increasingly popular for learning generic representations for vision and vision-and-language tasks. Such datasets have been built by querying search engines or collecting HTML alt-text -- since web data is noisy, they require complex filtering pipelines to maintain quality. We explore alternate data sources to collect high quality data with minimal filtering. We introduce RedCaps -- a large-scale dataset of 12M image-text pairs collected from Reddit. Images and captions from Reddit depict and describe a wide variety of objects and scenes. We collect data from a manually curated set of subreddits, which give coarse image labels and allow us to steer the dataset composition without labeling individual instances. We show that captioning models trained on RedCaps produce rich and varied captions preferred by humans, and learn visual representations that transfer to many downstream tasks.',\n",
       "  'arxiv_id': 'abs/2111.11431v1',\n",
       "  'category': None},\n",
       " 'abs/2112.01530v2': {'id': None,\n",
       "  'title': 'StyleMesh: Style Transfer for Indoor 3D Scene Reconstructions',\n",
       "  'authors': ['Lukas Höllein', 'Justin Johnson', 'Matthias Nießner'],\n",
       "  'abstract': 'We apply style transfer on mesh reconstructions of indoor scenes. This enables VR applications like experiencing 3D environments painted in the style of a favorite artist. Style transfer typically operates on 2D images, making stylization of a mesh challenging. When optimized over a variety of poses, stylization patterns become stretched out and inconsistent in size. On the other hand, model-based 3D style transfer methods exist that allow stylization from a sparse set of images, but they require a network at inference time. To this end, we optimize an explicit texture for the reconstructed mesh of a scene and stylize it jointly from all available input images. Our depth- and angle-aware optimization leverages surface normal and depth data of the underlying mesh to create a uniform and consistent stylization for the whole scene. Our experiments show that our method creates sharp and detailed results for the complete scene without view-dependent artifacts. Through extensive ablation studies, we show that the proposed 3D awareness enables style transfer to be applied to the 3D domain of a mesh. Our method can be used to render a stylized mesh in real-time with traditional rendering pipelines.',\n",
       "  'arxiv_id': 'abs/2112.01530v2',\n",
       "  'category': None},\n",
       " 'abs/2112.04481v2': {'id': None,\n",
       "  'title': \"What's Behind the Couch? Directed Ray Distance Functions (DRDF) for 3D\\n  Scene Reconstruction\",\n",
       "  'authors': ['Nilesh Kulkarni', 'Justin Johnson', 'David F. Fouhey'],\n",
       "  'abstract': 'We present an approach for full 3D scene reconstruction from a single unseen image. We train on dataset of realistic non-watertight scans of scenes. Our approach predicts a distance function, since these have shown promise in handling complex topologies and large spaces. We identify and analyze two key challenges for predicting such image conditioned distance functions that have prevented their success on real 3D scene data. First, we show that predicting a conventional scene distance from an image requires reasoning over a large receptive field. Second, we analytically show that the optimal output of the network trained to predict these distance functions does not obey all the distance function properties. We propose an alternate distance function, the Directed Ray Distance Function (DRDF), that tackles both challenges. We show that a deep network trained to predict DRDFs outperforms all other methods quantitatively and qualitatively on 3D reconstruction from single image on Matterport3D, 3DFront, and ScanNet.',\n",
       "  'arxiv_id': 'abs/2112.04481v2',\n",
       "  'category': None},\n",
       " 'abs/2206.07028v1': {'id': None,\n",
       "  'title': 'Learning 3D Object Shape and Layout without 3D Supervision',\n",
       "  'authors': ['Georgia Gkioxari', 'Nikhila Ravi', 'Justin Johnson'],\n",
       "  'abstract': 'A 3D scene consists of a set of objects, each with a shape and a layout giving their position in space. Understanding 3D scenes from 2D images is an important goal, with applications in robotics and graphics. While there have been recent advances in predicting 3D shape and layout from a single image, most approaches rely on 3D ground truth for training which is expensive to collect at scale. We overcome these limitations and propose a method that learns to predict 3D shape and layout for objects without any ground truth shape or layout information: instead we rely on multi-view images with 2D supervision which can more easily be collected at scale. Through extensive experiments on 3D Warehouse, Hypersim, and ScanNet we demonstrate that our approach scales to large datasets of realistic images, and compares favorably to methods relying on 3D ground truth. On Hypersim and ScanNet where reliable 3D ground truth is not available, our approach outperforms supervised approaches trained on smaller and less diverse datasets.',\n",
       "  'arxiv_id': 'abs/2206.07028v1',\n",
       "  'category': None},\n",
       " 'abs/2206.08355v3': {'id': None,\n",
       "  'title': 'FWD: Real-time Novel View Synthesis with Forward Warping and Depth',\n",
       "  'authors': ['Ang Cao', 'Chris Rockwell', 'Justin Johnson'],\n",
       "  'abstract': 'Novel view synthesis (NVS) is a challenging task requiring systems to generate photorealistic images of scenes from new viewpoints, where both quality and speed are important for applications. Previous image-based rendering (IBR) methods are fast, but have poor quality when input views are sparse. Recent Neural Radiance Fields (NeRF) and generalizable variants give impressive results but are not real-time. In our paper, we propose a generalizable NVS method with sparse inputs, called FWD, which gives high-quality synthesis in real-time. With explicit depth and differentiable rendering, it achieves competitive results to the SOTA methods with 130-1000x speedup and better perceptual quality. If available, we can seamlessly integrate sensor depth during either training or inference to improve image quality while retaining real-time speed. With the growing prevalence of depths sensors, we hope that methods making use of depth will become increasingly useful.',\n",
       "  'arxiv_id': 'abs/2206.08355v3',\n",
       "  'category': None},\n",
       " 'abs/2208.08988v1': {'id': None,\n",
       "  'title': 'The 8-Point Algorithm as an Inductive Bias for Relative Pose Prediction\\n  by ViTs',\n",
       "  'authors': ['Chris Rockwell', 'Justin Johnson', 'David F. Fouhey'],\n",
       "  'abstract': 'We present a simple baseline for directly estimating the relative pose (rotation and translation, including scale) between two images. Deep methods have recently shown strong progress but often require complex or multi-stage architectures. We show that a handful of modifications can be applied to a Vision Transformer (ViT) to bring its computations close to the Eight-Point Algorithm. This inductive bias enables a simple method to be competitive in multiple settings, often substantially improving over the state of the art with strong performance gains in limited data regimes.',\n",
       "  'arxiv_id': 'abs/2208.08988v1',\n",
       "  'category': None},\n",
       " 'abs/1508.07647v2': {'id': None,\n",
       "  'title': 'Love Thy Neighbors: Image Annotation by Exploiting Image Metadata',\n",
       "  'authors': ['Justin Johnson', 'Lamberto Ballan', 'Fei-Fei Li'],\n",
       "  'abstract': 'Some images that are difficult to recognize on their own may become more clear in the context of a neighborhood of related images with similar social-network metadata. We build on this intuition to improve multilabel image annotation. Our model uses image metadata nonparametrically to generate neighborhoods of related images using Jaccard similarities, then uses a deep neural network to blend visual information from the image and its neighbors. Prior work typically models image metadata parametrically, in contrast, our nonparametric treatment allows our model to perform well even when the vocabulary of metadata changes between training and testing. We perform comprehensive experiments on the NUS-WIDE dataset, where we show that our model outperforms state-of-the-art methods for multilabel image annotation even when our model is forced to generalize to new types of metadata.',\n",
       "  'arxiv_id': 'abs/1508.07647v2',\n",
       "  'category': None},\n",
       " 'abs/1705.02092v1': {'id': None,\n",
       "  'title': 'Characterizing and Improving Stability in Neural Style Transfer',\n",
       "  'authors': ['Agrim Gupta',\n",
       "   'Justin Johnson',\n",
       "   'Alexandre Alahi',\n",
       "   'Li Fei-Fei'],\n",
       "  'abstract': 'Recent progress in style transfer on images has focused on improving the quality of stylized images and speed of methods. However, real-time methods are highly unstable resulting in visible flickering when applied to videos. In this work we characterize the instability of these methods by examining the solution set of the style transfer objective. We show that the trace of the Gram matrix representing style is inversely related to the stability of the method. Then, we present a recurrent convolutional network for real-time video style transfer which incorporates a temporal consistency loss and overcomes the instability of prior methods. Our networks can be applied at any resolution, do not re- quire optical flow at test time, and produce high quality, temporally consistent stylized videos in real-time.',\n",
       "  'arxiv_id': 'abs/1705.02092v1',\n",
       "  'category': None},\n",
       " 'abs/1803.10892v1': {'id': None,\n",
       "  'title': 'Social GAN: Socially Acceptable Trajectories with Generative Adversarial\\n  Networks',\n",
       "  'authors': ['Agrim Gupta',\n",
       "   'Justin Johnson',\n",
       "   'Li Fei-Fei',\n",
       "   'Silvio Savarese',\n",
       "   'Alexandre Alahi'],\n",
       "  'abstract': 'Understanding human motion behavior is critical for autonomous moving platforms (like self-driving cars and social robots) if they are to navigate human-centric environments. This is challenging because human motion is inherently multimodal: given a history of human motion paths, there are many socially plausible ways that people could move in the future. We tackle this problem by combining tools from sequence prediction and generative adversarial networks: a recurrent sequence-to-sequence model observes motion histories and predicts future behavior, using a novel pooling mechanism to aggregate information across people. We predict socially plausible futures by training adversarially against a recurrent discriminator, and encourage diverse predictions with a novel variety loss. Through experiments on several datasets we demonstrate that our approach outperforms prior work in terms of accuracy, variety, collision avoidance, and computational complexity.',\n",
       "  'arxiv_id': 'abs/1803.10892v1',\n",
       "  'category': None},\n",
       " 'abs/1807.09937v1': {'id': None,\n",
       "  'title': 'HiDDeN: Hiding Data With Deep Networks',\n",
       "  'authors': ['Jiren Zhu', 'Russell Kaplan', 'Justin Johnson', 'Li Fei-Fei'],\n",
       "  'abstract': 'Recent work has shown that deep neural networks are highly sensitive to tiny perturbations of input images, giving rise to adversarial examples. Though this property is usually considered a weakness of learned models, we explore whether it can be beneficial. We find that neural networks can learn to use invisible perturbations to encode a rich amount of useful information. In fact, one can exploit this capability for the task of data hiding. We jointly train encoder and decoder networks, where given an input message and cover image, the encoder produces a visually indistinguishable encoded image, from which the decoder can recover the original message. We show that these encodings are competitive with existing data hiding algorithms, and further that they can be made robust to noise: our models learn to reconstruct hidden information in an encoded image despite the presence of Gaussian blurring, pixel-wise dropout, cropping, and JPEG compression. Even though JPEG is non-differentiable, we show that a robust model can be trained using differentiable approximations. Finally, we demonstrate that adversarial training improves the visual quality of encoded images.',\n",
       "  'arxiv_id': 'abs/1807.09937v1',\n",
       "  'category': None},\n",
       " 'abs/2012.04630v1': {'id': None,\n",
       "  'title': 'CASTing Your Model: Learning to Localize Improves Self-Supervised\\n  Representations',\n",
       "  'authors': ['Ramprasaath R. Selvaraju',\n",
       "   'Karan Desai',\n",
       "   'Justin Johnson',\n",
       "   'Nikhil Naik'],\n",
       "  'abstract': 'Recent advances in self-supervised learning (SSL) have largely closed the gap with supervised ImageNet pretraining. Despite their success these methods have been primarily applied to unlabeled ImageNet images, and show marginal gains when trained on larger sets of uncurated images. We hypothesize that current SSL methods perform best on iconic images, and struggle on complex scene images with many objects. Analyzing contrastive SSL methods shows that they have poor visual grounding and receive poor supervisory signal when trained on scene images. We propose Contrastive Attention-Supervised Tuning(CAST) to overcome these limitations. CAST uses unsupervised saliency maps to intelligently sample crops, and to provide grounding supervision via a Grad-CAM attention loss. Experiments on COCO show that CAST significantly improves the features learned by SSL methods on scene images, and further experiments show that CAST-trained models are more robust to changes in backgrounds.',\n",
       "  'arxiv_id': 'abs/2012.04630v1',\n",
       "  'category': None},\n",
       " 'abs/2207.10660v1': {'id': None,\n",
       "  'title': 'Omni3D: A Large Benchmark and Model for 3D Object Detection in the Wild',\n",
       "  'authors': ['Garrick Brazil',\n",
       "   'Julian Straub',\n",
       "   'Nikhila Ravi',\n",
       "   'Justin Johnson',\n",
       "   'Georgia Gkioxari'],\n",
       "  'abstract': 'Recognizing scenes and objects in 3D from a single image is a longstanding goal of computer vision with applications in robotics and AR/VR. For 2D recognition, large datasets and scalable solutions have led to unprecedented advances. In 3D, existing benchmarks are small in size and approaches specialize in few object categories and specific domains, e.g. urban driving scenes. Motivated by the success of 2D recognition, we revisit the task of 3D object detection by introducing a large benchmark, called Omni3D. Omni3D re-purposes and combines existing datasets resulting in 234k images annotated with more than 3 million instances and 97 categories.3D detection at such scale is challenging due to variations in camera intrinsics and the rich diversity of scene and object types. We propose a model, called Cube R-CNN, designed to generalize across camera and scene types with a unified approach. We show that Cube R-CNN outperforms prior works on the larger Omni3D and existing benchmarks. Finally, we prove that Omni3D is a powerful dataset for 3D object recognition, show that it improves single-dataset performance and can accelerate learning on new smaller datasets via pre-training.',\n",
       "  'arxiv_id': 'abs/2207.10660v1',\n",
       "  'category': None},\n",
       " 'abs/1905.13214v1': {'id': None,\n",
       "  'title': 'On Network Design Spaces for Visual Recognition',\n",
       "  'authors': ['Ilija Radosavovic',\n",
       "   'Justin Johnson',\n",
       "   'Saining Xie',\n",
       "   'Wan-Yen Lo',\n",
       "   'Piotr Dollár'],\n",
       "  'abstract': 'Over the past several years progress in designing better neural network architectures for visual recognition has been substantial. To help sustain this rate of progress, in this work we propose to reexamine the methodology for comparing network architectures. In particular, we introduce a new comparison paradigm of distribution estimates, in which network design spaces are compared by applying statistical techniques to populations of sampled models, while controlling for confounding factors like network complexity. Compared to current methodologies of comparing point and curve estimates of model families, distribution estimates paint a more complete picture of the entire design landscape. As a case study, we examine design spaces used in neural architecture search (NAS). We find significant statistical differences between recent NAS design space variants that have been largely overlooked. Furthermore, our analysis reveals that the design spaces for standard model families like ResNeXt can be comparable to the more complex ones used in recent NAS work. We hope these insights into distribution analysis will enable more robust progress toward discovering better networks for visual recognition.',\n",
       "  'arxiv_id': 'abs/1905.13214v1',\n",
       "  'category': None},\n",
       " 'abs/1908.05656v1': {'id': None,\n",
       "  'title': 'PHYRE: A New Benchmark for Physical Reasoning',\n",
       "  'authors': ['Anton Bakhtin',\n",
       "   'Laurens van der Maaten',\n",
       "   'Justin Johnson',\n",
       "   'Laura Gustafson',\n",
       "   'Ross Girshick'],\n",
       "  'abstract': 'Understanding and reasoning about physics is an important ability of intelligent agents. We develop the PHYRE benchmark for physical reasoning that contains a set of simple classical mechanics puzzles in a 2D physical environment. The benchmark is designed to encourage the development of learning algorithms that are sample-efficient and generalize well across puzzles. We test several modern learning algorithms on PHYRE and find that these algorithms fall short in solving the puzzles efficiently. We expect that PHYRE will encourage the development of novel sample-efficient agents that learn efficient but useful models of physics. For code and to play PHYRE for yourself, please visit https://player.phyre.ai.',\n",
       "  'arxiv_id': 'abs/1908.05656v1',\n",
       "  'category': None},\n",
       " 'abs/1612.06890v1': {'id': None,\n",
       "  'title': 'CLEVR: A Diagnostic Dataset for Compositional Language and Elementary\\n  Visual Reasoning',\n",
       "  'authors': ['Justin Johnson',\n",
       "   'Bharath Hariharan',\n",
       "   'Laurens van der Maaten',\n",
       "   'Li Fei-Fei',\n",
       "   'C. Lawrence Zitnick',\n",
       "   'Ross Girshick'],\n",
       "  'abstract': 'When building artificial intelligence systems that can reason and answer questions about visual data, we need diagnostic tests to analyze our progress and discover shortcomings. Existing benchmarks for visual question answering can help, but have strong biases that models can exploit to correctly answer questions without reasoning. They also conflate multiple sources of error, making it hard to pinpoint model weaknesses. We present a diagnostic dataset that tests a range of visual reasoning abilities. It contains minimal biases and has detailed annotations describing the kind of reasoning each question requires. We use this dataset to analyze a variety of modern visual reasoning systems, providing novel insights into their abilities and limitations.',\n",
       "  'arxiv_id': 'abs/1612.06890v1',\n",
       "  'category': None},\n",
       " 'abs/1705.03633v1': {'id': None,\n",
       "  'title': 'Inferring and Executing Programs for Visual Reasoning',\n",
       "  'authors': ['Justin Johnson',\n",
       "   'Bharath Hariharan',\n",
       "   'Laurens van der Maaten',\n",
       "   'Judy Hoffman',\n",
       "   'Li Fei-Fei',\n",
       "   'C. Lawrence Zitnick',\n",
       "   'Ross Girshick'],\n",
       "  'abstract': 'Existing methods for visual reasoning attempt to directly map inputs to outputs using black-box architectures without explicitly modeling the underlying reasoning processes. As a result, these black-box models often learn to exploit biases in the data rather than learning to perform visual reasoning. Inspired by module networks, this paper proposes a model for visual reasoning that consists of a program generator that constructs an explicit representation of the reasoning process to be performed, and an execution engine that executes the resulting program to produce an answer. Both the program generator and the execution engine are implemented by neural networks, and are trained using a combination of backpropagation and REINFORCE. Using the CLEVR benchmark for visual reasoning, we show that our model significantly outperforms strong baselines and generalizes better in a variety of settings.',\n",
       "  'arxiv_id': 'abs/1705.03633v1',\n",
       "  'category': None},\n",
       " 'abs/2007.08501v1': {'id': None,\n",
       "  'title': 'Accelerating 3D Deep Learning with PyTorch3D',\n",
       "  'authors': ['Nikhila Ravi',\n",
       "   'Jeremy Reizenstein',\n",
       "   'David Novotny',\n",
       "   'Taylor Gordon',\n",
       "   'Wan-Yen Lo',\n",
       "   'Justin Johnson',\n",
       "   'Georgia Gkioxari'],\n",
       "  'abstract': 'Deep learning has significantly improved 2D image recognition. Extending into 3D may advance many new applications including autonomous vehicles, virtual and augmented reality, authoring 3D content, and even improving 2D recognition. However despite growing interest, 3D deep learning remains relatively underexplored. We believe that some of this disparity is due to the engineering challenges involved in 3D deep learning, such as efficiently processing heterogeneous data and reframing graphics operations to be differentiable. We address these challenges by introducing PyTorch3D, a library of modular, efficient, and differentiable operators for 3D deep learning. It includes a fast, modular differentiable renderer for meshes and point clouds, enabling analysis-by-synthesis approaches. Compared with other differentiable renderers, PyTorch3D is more modular and efficient, allowing users to more easily extend it while also gracefully scaling to large meshes and images. We compare the PyTorch3D operators and renderer with other implementations and demonstrate significant speed and memory improvements. We also use PyTorch3D to improve the state-of-the-art for unsupervised 3D mesh and point cloud prediction from 2D images on ShapeNet. PyTorch3D is open-source and we hope it will help accelerate research in 3D deep learning.',\n",
       "  'arxiv_id': 'abs/2007.08501v1',\n",
       "  'category': None},\n",
       " 'abs/2112.01520v1': {'id': None,\n",
       "  'title': 'Recognizing Scenes from Novel Viewpoints',\n",
       "  'authors': ['Shengyi Qian',\n",
       "   'Alexander Kirillov',\n",
       "   'Nikhila Ravi',\n",
       "   'Devendra Singh Chaplot',\n",
       "   'Justin Johnson',\n",
       "   'David F. Fouhey',\n",
       "   'Georgia Gkioxari'],\n",
       "  'abstract': \"Humans can perceive scenes in 3D from a handful of 2D views. For AI agents, the ability to recognize a scene from any viewpoint given only a few images enables them to efficiently interact with the scene and its objects. In this work, we attempt to endow machines with this ability. We propose a model which takes as input a few RGB images of a new scene and recognizes the scene from novel viewpoints by segmenting it into semantic categories. All this without access to the RGB images from those views. We pair 2D scene recognition with an implicit 3D representation and learn from multi-view 2D annotations of hundreds of scenes without any 3D supervision beyond camera poses. We experiment on challenging datasets and demonstrate our model's ability to jointly capture semantics and geometry of novel scenes with diverse layouts, object types and shapes.\",\n",
       "  'arxiv_id': 'abs/2112.01520v1',\n",
       "  'category': None},\n",
       " 'abs/2212.03236v1': {'id': None,\n",
       "  'title': 'Self-Supervised Correspondence Estimation via Multiview Registration',\n",
       "  'authors': ['Mohamed El Banani',\n",
       "   'Ignacio Rocco',\n",
       "   'David Novotny',\n",
       "   'Andrea Vedaldi',\n",
       "   'Natalia Neverova',\n",
       "   'Justin Johnson',\n",
       "   'Benjamin Graham'],\n",
       "  'abstract': 'Video provides us with the spatio-temporal consistency needed for visual learning. Recent approaches have utilized this signal to learn correspondence estimation from close-by frame pairs. However, by only relying on close-by frame pairs, those approaches miss out on the richer long-range consistency between distant overlapping frames. To address this, we propose a self-supervised approach for correspondence estimation that learns from multiview consistency in short RGB-D video sequences. Our approach combines pairwise correspondence estimation and registration with a novel SE(3) transformation synchronization algorithm. Our key insight is that self-supervised multiview registration allows us to obtain correspondences over longer time frames; increasing both the diversity and difficulty of sampled pairs. We evaluate our approach on indoor scenes for correspondence estimation and RGB-D pointcloud registration and find that we perform on-par with supervised approaches.',\n",
       "  'arxiv_id': 'abs/2212.03236v1',\n",
       "  'category': None},\n",
       " 'abs/1602.07332v1': {'id': None,\n",
       "  'title': 'Visual Genome: Connecting Language and Vision Using Crowdsourced Dense\\n  Image Annotations',\n",
       "  'authors': ['Ranjay Krishna',\n",
       "   'Yuke Zhu',\n",
       "   'Oliver Groth',\n",
       "   'Justin Johnson',\n",
       "   'Kenji Hata',\n",
       "   'Joshua Kravitz',\n",
       "   'Stephanie Chen',\n",
       "   'Yannis Kalantidis',\n",
       "   'Li-Jia Li',\n",
       "   'David A. Shamma',\n",
       "   'Michael S. Bernstein',\n",
       "   'Fei-Fei Li'],\n",
       "  'abstract': 'Despite progress in perceptual tasks such as image classification, computers still perform poorly on cognitive tasks such as image description and question answering. Cognition is core to tasks that involve not just recognizing, but reasoning about our visual world. However, models used to tackle the rich content in images for cognitive tasks are still being trained using the same datasets designed for perceptual tasks. To achieve success at cognitive tasks, models need to understand the interactions and relationships between objects in an image. When asked \"What vehicle is the person riding?\", computers will need to identify the objects in an image as well as the relationships riding(man, carriage) and pulling(horse, carriage) in order to answer correctly that \"the person is riding a horse-drawn carriage\".   In this paper, we present the Visual Genome dataset to enable the modeling of such relationships. We collect dense annotations of objects, attributes, and relationships within each image to learn these models. Specifically, our dataset contains over 100K images where each image has an average of 21 objects, 18 attributes, and 18 pairwise relationships between objects. We canonicalize the objects, attributes, relationships, and noun phrases in region descriptions and questions answer pairs to WordNet synsets. Together, these annotations represent the densest and largest dataset of image descriptions, objects, attributes, relationships, and question answers.',\n",
       "  'arxiv_id': 'abs/1602.07332v1',\n",
       "  'category': None}}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('dlsys')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b3055c5785132655756dbddcc495a6d3f8d1c3ceb13ef316dc6e2968612d0aa0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
